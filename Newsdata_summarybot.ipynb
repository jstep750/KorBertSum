{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZ1lg-rIZJak"
   },
   "source": [
    "#0. Default Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJkdy_8RZGjc"
   },
   "source": [
    "Executed in Colab environment.\n",
    "\n",
    "* ML Framework\n",
    "   - Python 3.7.10\n",
    "   - Pytorch 1.8.1\n",
    "\n",
    "* Hardware\n",
    "   - RAM: 12.7G \n",
    "   - CPU: Intel(R) Xeon(R) CPU @ 2.20GHz (1core)\n",
    "\n",
    "Assumed that data exists like below.\n",
    "you also need a etri openapi key.\n",
    "\n",
    "```\n",
    "../gdrive/My Drive/data\n",
    "├── 1_bert_download_001_bert_morp_pytorch.zip\n",
    "```\n",
    "\n",
    "Project Tree (directory only)\n",
    "```\n",
    "../KoBertSum\n",
    "├── bert_data\n",
    "├── json_data\n",
    "├── logs\n",
    "├── models\n",
    "├── raw_data\n",
    "├── results\n",
    "├── src\n",
    "│   ├── models\n",
    "│   ├── others\n",
    "│   └── prepro\n",
    "└── urls\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bYOoWzjYSip"
   },
   "source": [
    "#1. Install dependency packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7p9HcD3e8Ryo",
    "outputId": "0d132884-504f-4029-ef41-d68e812e7b27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'pyrouge'...\n",
      "remote: Enumerating objects: 551, done.\u001b[K\n",
      "remote: Total 551 (delta 0), reused 0 (delta 0), pack-reused 551\u001b[K\n",
      "Receiving objects: 100% (551/551), 123.17 KiB | 2.28 MiB/s, done.\n",
      "Resolving deltas: 100% (198/198), done.\n",
      "/content/pyrouge\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "pyrouge.__pycache__.Rouge155.cpython-37: module references __file__\n",
      "pyrouge.tests.__pycache__.Rouge155_test.cpython-37: module references __file__\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following additional packages will be installed:\n",
      "  libauthen-sasl-perl libdata-dump-perl libencode-locale-perl\n",
      "  libfile-listing-perl libfont-afm-perl libhtml-form-perl libhtml-format-perl\n",
      "  libhtml-parser-perl libhtml-tagset-perl libhtml-tree-perl\n",
      "  libhttp-cookies-perl libhttp-daemon-perl libhttp-date-perl\n",
      "  libhttp-message-perl libhttp-negotiate-perl libio-html-perl\n",
      "  libio-socket-ssl-perl liblwp-mediatypes-perl liblwp-protocol-https-perl\n",
      "  libmailtools-perl libnet-http-perl libnet-smtp-ssl-perl libnet-ssleay-perl\n",
      "  libtimedate-perl libtry-tiny-perl liburi-perl libwww-perl\n",
      "  libwww-robotrules-perl netbase perl-openssl-defaults\n",
      "Suggested packages:\n",
      "  libdigest-hmac-perl libgssapi-perl libcrypt-ssleay-perl libauthen-ntlm-perl\n",
      "The following NEW packages will be installed:\n",
      "  libauthen-sasl-perl libdata-dump-perl libencode-locale-perl\n",
      "  libfile-listing-perl libfont-afm-perl libhtml-form-perl libhtml-format-perl\n",
      "  libhtml-parser-perl libhtml-tagset-perl libhtml-tree-perl\n",
      "  libhttp-cookies-perl libhttp-daemon-perl libhttp-date-perl\n",
      "  libhttp-message-perl libhttp-negotiate-perl libio-html-perl\n",
      "  libio-socket-ssl-perl liblwp-mediatypes-perl liblwp-protocol-https-perl\n",
      "  libmailtools-perl libnet-http-perl libnet-smtp-ssl-perl libnet-ssleay-perl\n",
      "  libtimedate-perl libtry-tiny-perl liburi-perl libwww-perl\n",
      "  libwww-robotrules-perl libxml-parser-perl netbase perl-openssl-defaults\n",
      "0 upgraded, 31 newly installed, 0 to remove and 31 not upgraded.\n",
      "Need to get 1,713 kB of archives.\n",
      "After this operation, 5,581 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 netbase all 5.4 [12.7 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libdata-dump-perl all 1.23-1 [27.0 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libencode-locale-perl all 1.05-1 [12.3 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtimedate-perl all 2.3000-2 [37.5 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-date-perl all 6.02-1 [10.4 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfile-listing-perl all 6.04-1 [9,774 B]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfont-afm-perl all 1.20-2 [13.2 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-tagset-perl all 3.20-3 [12.1 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 liburi-perl all 1.73-1 [77.2 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-parser-perl amd64 3.72-3build1 [85.9 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-html-perl all 1.001-1 [14.9 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 liblwp-mediatypes-perl all 6.02-1 [21.7 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-message-perl all 6.14-1 [72.1 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-form-perl all 6.03-1 [23.5 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-tree-perl all 5.07-1 [200 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-format-perl all 2.12-1 [41.3 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-cookies-perl all 6.04-1 [17.2 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-daemon-perl all 6.01-1 [17.0 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-negotiate-perl all 6.00-2 [13.4 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 perl-openssl-defaults amd64 3build1 [7,012 B]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libnet-ssleay-perl amd64 1.84-1ubuntu0.2 [283 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libio-socket-ssl-perl all 2.060-3~ubuntu18.04.1 [173 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu bionic/main amd64 libnet-http-perl all 6.17-1 [22.7 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtry-tiny-perl all 0.30-1 [20.5 kB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu bionic/main amd64 libwww-robotrules-perl all 6.01-1 [14.1 kB]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libwww-perl all 6.31-1ubuntu0.1 [137 kB]\n",
      "Get:27 http://archive.ubuntu.com/ubuntu bionic/main amd64 liblwp-protocol-https-perl all 6.07-2 [8,284 B]\n",
      "Get:28 http://archive.ubuntu.com/ubuntu bionic/main amd64 libnet-smtp-ssl-perl all 1.04-1 [5,948 B]\n",
      "Get:29 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmailtools-perl all 2.18-1 [74.0 kB]\n",
      "Get:30 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxml-parser-perl amd64 2.44-2build3 [199 kB]\n",
      "Get:31 http://archive.ubuntu.com/ubuntu bionic/main amd64 libauthen-sasl-perl all 2.1600-1 [48.7 kB]\n",
      "Fetched 1,713 kB in 1s (1,499 kB/s)\n",
      "Extracting templates from packages: 100%\n",
      "Selecting previously unselected package netbase.\n",
      "(Reading database ... 160983 files and directories currently installed.)\n",
      "Preparing to unpack .../00-netbase_5.4_all.deb ...\n",
      "Unpacking netbase (5.4) ...\n",
      "Selecting previously unselected package libdata-dump-perl.\n",
      "Preparing to unpack .../01-libdata-dump-perl_1.23-1_all.deb ...\n",
      "Unpacking libdata-dump-perl (1.23-1) ...\n",
      "Selecting previously unselected package libencode-locale-perl.\n",
      "Preparing to unpack .../02-libencode-locale-perl_1.05-1_all.deb ...\n",
      "Unpacking libencode-locale-perl (1.05-1) ...\n",
      "Selecting previously unselected package libtimedate-perl.\n",
      "Preparing to unpack .../03-libtimedate-perl_2.3000-2_all.deb ...\n",
      "Unpacking libtimedate-perl (2.3000-2) ...\n",
      "Selecting previously unselected package libhttp-date-perl.\n",
      "Preparing to unpack .../04-libhttp-date-perl_6.02-1_all.deb ...\n",
      "Unpacking libhttp-date-perl (6.02-1) ...\n",
      "Selecting previously unselected package libfile-listing-perl.\n",
      "Preparing to unpack .../05-libfile-listing-perl_6.04-1_all.deb ...\n",
      "Unpacking libfile-listing-perl (6.04-1) ...\n",
      "Selecting previously unselected package libfont-afm-perl.\n",
      "Preparing to unpack .../06-libfont-afm-perl_1.20-2_all.deb ...\n",
      "Unpacking libfont-afm-perl (1.20-2) ...\n",
      "Selecting previously unselected package libhtml-tagset-perl.\n",
      "Preparing to unpack .../07-libhtml-tagset-perl_3.20-3_all.deb ...\n",
      "Unpacking libhtml-tagset-perl (3.20-3) ...\n",
      "Selecting previously unselected package liburi-perl.\n",
      "Preparing to unpack .../08-liburi-perl_1.73-1_all.deb ...\n",
      "Unpacking liburi-perl (1.73-1) ...\n",
      "Selecting previously unselected package libhtml-parser-perl.\n",
      "Preparing to unpack .../09-libhtml-parser-perl_3.72-3build1_amd64.deb ...\n",
      "Unpacking libhtml-parser-perl (3.72-3build1) ...\n",
      "Selecting previously unselected package libio-html-perl.\n",
      "Preparing to unpack .../10-libio-html-perl_1.001-1_all.deb ...\n",
      "Unpacking libio-html-perl (1.001-1) ...\n",
      "Selecting previously unselected package liblwp-mediatypes-perl.\n",
      "Preparing to unpack .../11-liblwp-mediatypes-perl_6.02-1_all.deb ...\n",
      "Unpacking liblwp-mediatypes-perl (6.02-1) ...\n",
      "Selecting previously unselected package libhttp-message-perl.\n",
      "Preparing to unpack .../12-libhttp-message-perl_6.14-1_all.deb ...\n",
      "Unpacking libhttp-message-perl (6.14-1) ...\n",
      "Selecting previously unselected package libhtml-form-perl.\n",
      "Preparing to unpack .../13-libhtml-form-perl_6.03-1_all.deb ...\n",
      "Unpacking libhtml-form-perl (6.03-1) ...\n",
      "Selecting previously unselected package libhtml-tree-perl.\n",
      "Preparing to unpack .../14-libhtml-tree-perl_5.07-1_all.deb ...\n",
      "Unpacking libhtml-tree-perl (5.07-1) ...\n",
      "Selecting previously unselected package libhtml-format-perl.\n",
      "Preparing to unpack .../15-libhtml-format-perl_2.12-1_all.deb ...\n",
      "Unpacking libhtml-format-perl (2.12-1) ...\n",
      "Selecting previously unselected package libhttp-cookies-perl.\n",
      "Preparing to unpack .../16-libhttp-cookies-perl_6.04-1_all.deb ...\n",
      "Unpacking libhttp-cookies-perl (6.04-1) ...\n",
      "Selecting previously unselected package libhttp-daemon-perl.\n",
      "Preparing to unpack .../17-libhttp-daemon-perl_6.01-1_all.deb ...\n",
      "Unpacking libhttp-daemon-perl (6.01-1) ...\n",
      "Selecting previously unselected package libhttp-negotiate-perl.\n",
      "Preparing to unpack .../18-libhttp-negotiate-perl_6.00-2_all.deb ...\n",
      "Unpacking libhttp-negotiate-perl (6.00-2) ...\n",
      "Selecting previously unselected package perl-openssl-defaults:amd64.\n",
      "Preparing to unpack .../19-perl-openssl-defaults_3build1_amd64.deb ...\n",
      "Unpacking perl-openssl-defaults:amd64 (3build1) ...\n",
      "Selecting previously unselected package libnet-ssleay-perl.\n",
      "Preparing to unpack .../20-libnet-ssleay-perl_1.84-1ubuntu0.2_amd64.deb ...\n",
      "Unpacking libnet-ssleay-perl (1.84-1ubuntu0.2) ...\n",
      "Selecting previously unselected package libio-socket-ssl-perl.\n",
      "Preparing to unpack .../21-libio-socket-ssl-perl_2.060-3~ubuntu18.04.1_all.deb ...\n",
      "Unpacking libio-socket-ssl-perl (2.060-3~ubuntu18.04.1) ...\n",
      "Selecting previously unselected package libnet-http-perl.\n",
      "Preparing to unpack .../22-libnet-http-perl_6.17-1_all.deb ...\n",
      "Unpacking libnet-http-perl (6.17-1) ...\n",
      "Selecting previously unselected package libtry-tiny-perl.\n",
      "Preparing to unpack .../23-libtry-tiny-perl_0.30-1_all.deb ...\n",
      "Unpacking libtry-tiny-perl (0.30-1) ...\n",
      "Selecting previously unselected package libwww-robotrules-perl.\n",
      "Preparing to unpack .../24-libwww-robotrules-perl_6.01-1_all.deb ...\n",
      "Unpacking libwww-robotrules-perl (6.01-1) ...\n",
      "Selecting previously unselected package libwww-perl.\n",
      "Preparing to unpack .../25-libwww-perl_6.31-1ubuntu0.1_all.deb ...\n",
      "Unpacking libwww-perl (6.31-1ubuntu0.1) ...\n",
      "Selecting previously unselected package liblwp-protocol-https-perl.\n",
      "Preparing to unpack .../26-liblwp-protocol-https-perl_6.07-2_all.deb ...\n",
      "Unpacking liblwp-protocol-https-perl (6.07-2) ...\n",
      "Selecting previously unselected package libnet-smtp-ssl-perl.\n",
      "Preparing to unpack .../27-libnet-smtp-ssl-perl_1.04-1_all.deb ...\n",
      "Unpacking libnet-smtp-ssl-perl (1.04-1) ...\n",
      "Selecting previously unselected package libmailtools-perl.\n",
      "Preparing to unpack .../28-libmailtools-perl_2.18-1_all.deb ...\n",
      "Unpacking libmailtools-perl (2.18-1) ...\n",
      "Selecting previously unselected package libxml-parser-perl.\n",
      "Preparing to unpack .../29-libxml-parser-perl_2.44-2build3_amd64.deb ...\n",
      "Unpacking libxml-parser-perl (2.44-2build3) ...\n",
      "Selecting previously unselected package libauthen-sasl-perl.\n",
      "Preparing to unpack .../30-libauthen-sasl-perl_2.1600-1_all.deb ...\n",
      "Unpacking libauthen-sasl-perl (2.1600-1) ...\n",
      "Setting up libhtml-tagset-perl (3.20-3) ...\n",
      "Setting up libtry-tiny-perl (0.30-1) ...\n",
      "Setting up libfont-afm-perl (1.20-2) ...\n",
      "Setting up libencode-locale-perl (1.05-1) ...\n",
      "Setting up libtimedate-perl (2.3000-2) ...\n",
      "Setting up perl-openssl-defaults:amd64 (3build1) ...\n",
      "Setting up libio-html-perl (1.001-1) ...\n",
      "Setting up liblwp-mediatypes-perl (6.02-1) ...\n",
      "Setting up liburi-perl (1.73-1) ...\n",
      "Setting up libdata-dump-perl (1.23-1) ...\n",
      "Setting up libhtml-parser-perl (3.72-3build1) ...\n",
      "Setting up libnet-http-perl (6.17-1) ...\n",
      "Setting up libwww-robotrules-perl (6.01-1) ...\n",
      "Setting up libauthen-sasl-perl (2.1600-1) ...\n",
      "Setting up netbase (5.4) ...\n",
      "Setting up libhttp-date-perl (6.02-1) ...\n",
      "Setting up libnet-ssleay-perl (1.84-1ubuntu0.2) ...\n",
      "Setting up libio-socket-ssl-perl (2.060-3~ubuntu18.04.1) ...\n",
      "Setting up libhtml-tree-perl (5.07-1) ...\n",
      "Setting up libfile-listing-perl (6.04-1) ...\n",
      "Setting up libhttp-message-perl (6.14-1) ...\n",
      "Setting up libhttp-negotiate-perl (6.00-2) ...\n",
      "Setting up libnet-smtp-ssl-perl (1.04-1) ...\n",
      "Setting up libhtml-format-perl (2.12-1) ...\n",
      "Setting up libhttp-cookies-perl (6.04-1) ...\n",
      "Setting up libhttp-daemon-perl (6.01-1) ...\n",
      "Setting up libhtml-form-perl (6.03-1) ...\n",
      "Setting up libmailtools-perl (2.18-1) ...\n",
      "Setting up liblwp-protocol-https-perl (6.07-2) ...\n",
      "Setting up libwww-perl (6.31-1ubuntu0.1) ...\n",
      "Setting up libxml-parser-perl (2.44-2build3) ...\n",
      "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
      "/content/pyrouge/pyrouge\n",
      "Cloning into 'rouge'...\n",
      "remote: Enumerating objects: 393, done.\u001b[K\n",
      "remote: Total 393 (delta 0), reused 0 (delta 0), pack-reused 393\u001b[K\n",
      "Receiving objects: 100% (393/393), 298.74 KiB | 2.87 MiB/s, done.\n",
      "Resolving deltas: 100% (109/109), done.\n",
      "2021-04-09 07:18:15,441 [MainThread  ] [INFO ]  Set ROUGE home directory to /content/pyrouge/rouge/tools/ROUGE-1.5.5.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/pyrouge_set_rouge_path\", line 4, in <module>\n",
      "    __import__('pkg_resources').run_script('pyrouge==0.1.3', 'pyrouge_set_rouge_path')\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\", line 651, in run_script\n",
      "    self.require(requires)[0].run_script(script_name, ns)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\", line 1448, in run_script\n",
      "    exec(code, namespace, namespace)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyrouge-0.1.3-py3.7.egg/EGG-INFO/scripts/pyrouge_set_rouge_path\", line 18, in <module>\n",
      "    main()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyrouge-0.1.3-py3.7.egg/EGG-INFO/scripts/pyrouge_set_rouge_path\", line 15, in main\n",
      "    Rouge155(args.home_dir)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyrouge-0.1.3-py3.7.egg/pyrouge/Rouge155.py\", line 91, in __init__\n",
      "    self.__set_rouge_dir(rouge_dir)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyrouge-0.1.3-py3.7.egg/pyrouge/Rouge155.py\", line 413, in __set_rouge_dir\n",
      "    self.data_dir = os.path.join(self._home_dir, 'data')\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyrouge-0.1.3-py3.7.egg/pyrouge/Rouge155.py\", line 549, in fset\n",
      "    verify_dir(path, dir_name)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyrouge-0.1.3-py3.7.egg/pyrouge/utils/file_utils.py\", line 87, in verify_dir\n",
      "    raise Exception(msg)\n",
      "Exception: Cannot set data directory because the path /content/pyrouge/rouge/tools/ROUGE-1.5.5/data does not exist.\n",
      "[Errno 2] No such file or directory: '/content/pyrouge/rouge/tools/ROUGE-1.5.5/data'\n",
      "/content/pyrouge/pyrouge\n",
      "mv: cannot stat 'WordNet-2.0.exc.db': No such file or directory\n",
      "Can't open perl script \"WordNet-2.0-Exceptions/buildExeptionDB.pl\": No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# install bheinzerling's pyrouge\n",
    "!git clone https://github.com/bheinzerling/pyrouge\n",
    "%cd pyrouge\n",
    "!python setup.py -q install\n",
    "# install missing dependency\n",
    "!apt install -q libxml-parser-perl\n",
    "%cd pyrouge\n",
    "!git clone https://github.com/andersjo/pyrouge.git rouge\n",
    "!pyrouge_set_rouge_path '../pyrouge/rouge/tools/ROUGE-1.5.5'\n",
    "%cd ../pyrouge/rouge/tools/ROUGE-1.5.5/data\n",
    "!mv WordNet-2.0.exc.db WordNet-2.0.exc.db.orig\n",
    "!perl WordNet-2.0-Exceptions/buildExeptionDB.pl ./WordNet-2.0-Exceptions ./smart_common_words.txt ./WordNet-2.0.exc.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fkQhs-DW_GK9",
    "outputId": "e2a96d5f-72e8-4eea-ee94-a8e0c24eff50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 133kB 6.6MB/s \n",
      "\u001b[K     |████████████████████████████████| 102kB 5.5MB/s \n",
      "\u001b[K     |████████████████████████████████| 7.4MB 15.9MB/s \n",
      "\u001b[K     |████████████████████████████████| 81kB 4.7MB/s \n",
      "\u001b[?25h  Building wheel for boto3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: botocore 1.20.48 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
      "\u001b[K     |████████████████████████████████| 122kB 5.1MB/s \n",
      "\u001b[K     |████████████████████████████████| 51kB 2.3MB/s \n",
      "\u001b[K     |████████████████████████████████| 194kB 7.8MB/s \n",
      "\u001b[K     |████████████████████████████████| 2.2MB 6.6MB/s \n",
      "\u001b[K     |████████████████████████████████| 81kB 5.3MB/s \n",
      "\u001b[K     |████████████████████████████████| 3.3MB 26.2MB/s \n",
      "\u001b[K     |████████████████████████████████| 870kB 52.6MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.0MB 52.9MB/s \n",
      "\u001b[K     |████████████████████████████████| 3.5MB 40.1MB/s \n",
      "\u001b[K     |████████████████████████████████| 92kB 5.5MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.8MB 47.0MB/s \n",
      "\u001b[K     |████████████████████████████████| 358kB 40.8MB/s \n",
      "\u001b[?25h  Building wheel for dash (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for dash-renderer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for dash-core-components (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for dash-html-components (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for dash-table (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# 기타 패키지 설치\n",
    "!pip install -q pytorch_pretrained_bert\n",
    "!pip install -q tensorboardX\n",
    "!pip install -q jupyter-dash==0.3.0rc1 dash-bootstrap-components transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Uuo_VZyYvbP"
   },
   "source": [
    "#Google Drive Mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gqy8CkL4qMNW",
    "outputId": "2591a30b-ea94-49e2-953a-871f6fa7bf3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "#gd mount\n",
    "from google.colab import drive\n",
    "drive.mount('../gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001_bert_morp_pytorch\t\tbert_data\t     results\n",
      "LICENSE\t\t\t\tetri_api_scraper.py  src\n",
      "Newsdata_extractive_summ.ipynb\tjson_data\t     summbot.png\n",
      "Newsdata_summarybot.ipynb\tlogs\t\t     temp\n",
      "README.md\t\t\tmodels\t\t     urls\n",
      "article_to_json.py\t\tpyrouge\n",
      "bert_config_uncased_base.json\traw_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNHS_RluhGcv"
   },
   "source": [
    "#2. BERT Model download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iy23A3EaQc97"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dPAwphgFm1Cc",
    "outputId": "19f9c880-84bc-4deb-91ba-4fb709e747d9"
   },
   "outputs": [],
   "source": [
    "#KoBertSum package\n",
    "!git clone -q https://github.com/raqoon886/KoBertSum.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZJSyUZLagMYV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: gdown: not found\n"
     ]
    }
   ],
   "source": [
    "#load fine-tuned bert model\n",
    "!gdown --id \"your fine-tuned bert model\" -O ./models/model_step_1000.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fnShxFgZqV_f",
    "outputId": "bea26035-7239-47e8-faad-93324c008bf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace 001_bert_morp_pytorch/bert_config.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "# data에 저장된 etri-bert 모델 가져와서 압축해제\n",
    "!cp \"../gdrive/My Drive/data/1_bert_download_001_bert_morp_pytorch.zip\" \"1_bert_download_001_bert_morp_pytorch.zip\"\n",
    "!unzip -q \"1_bert_download_001_bert_morp_pytorch.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "wv5f3XOLpZ7o"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'opt/ml/KorBertSum/src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Bertsum directory chdir\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mopt/ml/KorBertSum/src\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'opt/ml/KorBertSum/src'"
     ]
    }
   ],
   "source": [
    "# Bertsum directory chdir\n",
    "os.chdir('opt/ml/KorBertSum/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__pycache__\tmodels\tprepro\t       req.txt\t\t train.py\n",
      "distributed.py\tothers\tpreprocess.py  requirements.txt\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3Uzgqx3cLy0"
   },
   "source": [
    "#3. BERT forward propagation workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "M3m9X2IApKrt"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Main training workflow\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import signal\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertConfig\n",
    "\n",
    "\n",
    "import distributed\n",
    "from models import data_loader, model_builder\n",
    "from models.data_loader import load_dataset\n",
    "from models.model_builder import Summarizer\n",
    "from tensorboardX import SummaryWriter\n",
    "from models.reporter import ReportMgr\n",
    "from models.stats import Statistics\n",
    "from others.logging import logger\n",
    "# from models.trainer import build_trainer\n",
    "# build_trainer의 dependency package pyrouge.utils가 import되지 않아 직접 셀에 삽입\n",
    "from others.logging import logger, init_logger\n",
    "import easydict\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    \"encoder\":'classifier',\n",
    "    \"mode\":'summary',\n",
    "    \"bert_data_path\":'../bert_sample/korean',\n",
    "    \"model_path\":'../models/bert_classifier',\n",
    "    \"bert_model\":'../001_bert_morp_pytorch',\n",
    "    \"result_path\":'../results/korean',\n",
    "    \"temp_dir\":'.',\n",
    "    \"bert_config_path\":'../001_bert_morp_pytorch/bert_config.json',\n",
    "    \"batch_size\":1000,\n",
    "    \"use_interval\":True,\n",
    "    \"hidden_size\":128,\n",
    "    \"ff_size\":512,\n",
    "    \"heads\":4,\n",
    "    \"inter_layers\":2,\n",
    "    \"rnn_size\":512,\n",
    "    \"param_init\":0,\n",
    "    \"param_init_glorot\":True,\n",
    "    \"dropout\":0.1,\n",
    "    \"optim\":'adam',\n",
    "    \"lr\":2e-3,\n",
    "    \"report_every\":1,\n",
    "    \"save_checkpoint_steps\":5,\n",
    "    \"block_trigram\":True,\n",
    "    \"recall_eval\":False,\n",
    "    \n",
    "    \"accum_count\":1,\n",
    "    \"world_size\":1,\n",
    "    \"visible_gpus\":'-1',\n",
    "    \"gpu_ranks\":'0',\n",
    "    \"log_file\":'../logs/bert_classifier',\n",
    "    \"test_from\":'../models/bert_classifier/model_step_1000.pt'\n",
    "})\n",
    "\n",
    "\n",
    "def build_trainer(args, device_id, model,\n",
    "                  optim):\n",
    "    \"\"\"\n",
    "    Simplify `Trainer` creation based on user `opt`s*\n",
    "    Args:\n",
    "        opt (:obj:`Namespace`): user options (usually from argument parsing)\n",
    "        model (:obj:`onmt.models.NMTModel`): the model to train\n",
    "        fields (dict): dict of fields\n",
    "        optim (:obj:`onmt.utils.Optimizer`): optimizer used during training\n",
    "        data_type (str): string describing the type of data\n",
    "            e.g. \"text\", \"img\", \"audio\"\n",
    "        model_saver(:obj:`onmt.models.ModelSaverBase`): the utility object\n",
    "            used to save the model\n",
    "    \"\"\"\n",
    "    device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
    "\n",
    "\n",
    "    grad_accum_count = args.accum_count\n",
    "    n_gpu = args.world_size\n",
    "\n",
    "    if device_id >= 0:\n",
    "        gpu_rank = int(args.gpu_ranks[device_id])\n",
    "    else:\n",
    "        gpu_rank = 0\n",
    "        n_gpu = 0\n",
    "\n",
    "    print('gpu_rank %d' % gpu_rank)\n",
    "\n",
    "    tensorboard_log_dir = args.model_path\n",
    "\n",
    "    writer = SummaryWriter(tensorboard_log_dir, comment=\"Unmt\")\n",
    "\n",
    "    report_manager = ReportMgr(args.report_every, start_time=-1, tensorboard_writer=writer)\n",
    "\n",
    "    trainer = Trainer(args, model, optim, grad_accum_count, n_gpu, gpu_rank, report_manager)\n",
    "\n",
    "    # print(tr)\n",
    "    if (model):\n",
    "        n_params = _tally_parameters(model)\n",
    "        logger.info('* number of parameters: %d' % n_params)\n",
    "\n",
    "    return trainer\n",
    "class Trainer(object):\n",
    "    \"\"\"\n",
    "    Class that controls the training process.\n",
    "\n",
    "    Args:\n",
    "            model(:py:class:`onmt.models.model.NMTModel`): translation model\n",
    "                to train\n",
    "            train_loss(:obj:`onmt.utils.loss.LossComputeBase`):\n",
    "               training loss computation\n",
    "            valid_loss(:obj:`onmt.utils.loss.LossComputeBase`):\n",
    "               training loss computation\n",
    "            optim(:obj:`onmt.utils.optimizers.Optimizer`):\n",
    "               the optimizer responsible for update\n",
    "            trunc_size(int): length of truncated back propagation through time\n",
    "            shard_size(int): compute loss in shards of this size for efficiency\n",
    "            data_type(string): type of the source input: [text|img|audio]\n",
    "            norm_method(string): normalization methods: [sents|tokens]\n",
    "            grad_accum_count(int): accumulate gradients this many times.\n",
    "            report_manager(:obj:`onmt.utils.ReportMgrBase`):\n",
    "                the object that creates reports, or None\n",
    "            model_saver(:obj:`onmt.models.ModelSaverBase`): the saver is\n",
    "                used to save a checkpoint.\n",
    "                Thus nothing will be saved if this parameter is None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,  args, model,  optim,\n",
    "                  grad_accum_count=1, n_gpu=1, gpu_rank=1,\n",
    "                  report_manager=None):\n",
    "        # Basic attributes.\n",
    "        self.args = args\n",
    "        self.save_checkpoint_steps = args.save_checkpoint_steps\n",
    "        self.model = model\n",
    "        self.optim = optim\n",
    "        self.grad_accum_count = grad_accum_count\n",
    "        self.n_gpu = n_gpu\n",
    "        self.gpu_rank = gpu_rank\n",
    "        self.report_manager = report_manager\n",
    "\n",
    "        self.loss = torch.nn.BCELoss(reduction='none')\n",
    "        assert grad_accum_count > 0\n",
    "        # Set model in training mode.\n",
    "        if (model):\n",
    "            self.model.train()\n",
    "\n",
    "    def summary(self, test_iter, step, cal_lead=False, cal_oracle=False):\n",
    "        \"\"\" Validate model.\n",
    "            valid_iter: validate data iterator\n",
    "        Returns:\n",
    "            :obj:`nmt.Statistics`: validation loss statistics\n",
    "        \"\"\"\n",
    "        # Set model in validating mode.\n",
    "        def _get_ngrams(n, text):\n",
    "            ngram_set = set()\n",
    "            text_length = len(text)\n",
    "            max_index_ngram_start = text_length - n\n",
    "            for i in range(max_index_ngram_start + 1):\n",
    "                ngram_set.add(tuple(text[i:i + n]))\n",
    "            return ngram_set\n",
    "\n",
    "        def _block_tri(c, p):\n",
    "            tri_c = _get_ngrams(3, c.split())\n",
    "            for s in p:\n",
    "                tri_s = _get_ngrams(3, s.split())\n",
    "                if len(tri_c.intersection(tri_s))>0:\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        if (not cal_lead and not cal_oracle):\n",
    "            self.model.eval()\n",
    "        stats = Statistics()\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_iter:\n",
    "                src = batch.src\n",
    "                labels = batch.labels\n",
    "                segs = batch.segs\n",
    "                clss = batch.clss\n",
    "                mask = batch.mask\n",
    "                mask_cls = batch.mask_cls\n",
    "\n",
    "\n",
    "                gold = []\n",
    "                pred = []\n",
    "\n",
    "                if (cal_lead):\n",
    "                    selected_ids = [list(range(batch.clss.size(1)))] * batch.batch_size\n",
    "                elif (cal_oracle):\n",
    "                    selected_ids = [[j for j in range(batch.clss.size(1)) if labels[i][j] == 1] for i in\n",
    "                                    range(batch.batch_size)]\n",
    "                else:\n",
    "                    sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
    "\n",
    "                    # loss = self.loss(sent_scores, labels.float())\n",
    "                    # loss = (loss * mask.float()).sum()\n",
    "                    # batch_stats = Statistics(float(loss.cpu().data.numpy()), len(labels))\n",
    "                    # stats.update(batch_stats)\n",
    "\n",
    "                    sent_scores = sent_scores + mask.float()\n",
    "                    sent_scores = sent_scores.cpu().data.numpy()\n",
    "                    selected_ids = np.argsort(-sent_scores, 1)\n",
    "                # selected_ids = np.sort(selected_ids,1)\n",
    "                \n",
    "\n",
    "        return selected_ids\n",
    "\n",
    "\n",
    "    def _gradient_accumulation(self, true_batchs, normalization, total_stats,\n",
    "                               report_stats):\n",
    "        if self.grad_accum_count > 1:\n",
    "            self.model.zero_grad()\n",
    "\n",
    "        for batch in true_batchs:\n",
    "            if self.grad_accum_count == 1:\n",
    "                self.model.zero_grad()\n",
    "\n",
    "            src = batch.src\n",
    "            labels = batch.labels\n",
    "            segs = batch.segs\n",
    "            clss = batch.clss\n",
    "            mask = batch.mask\n",
    "            mask_cls = batch.mask_cls\n",
    "\n",
    "            sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
    "\n",
    "            loss = self.loss(sent_scores, labels.float())\n",
    "            loss = (loss*mask.float()).sum()\n",
    "            (loss/loss.numel()).backward()\n",
    "            # loss.div(float(normalization)).backward()\n",
    "\n",
    "            batch_stats = Statistics(float(loss.cpu().data.numpy()), normalization)\n",
    "\n",
    "\n",
    "            total_stats.update(batch_stats)\n",
    "            report_stats.update(batch_stats)\n",
    "\n",
    "            # 4. Update the parameters and statistics.\n",
    "            if self.grad_accum_count == 1:\n",
    "                # Multi GPU gradient gather\n",
    "                if self.n_gpu > 1:\n",
    "                    grads = [p.grad.data for p in self.model.parameters()\n",
    "                             if p.requires_grad\n",
    "                             and p.grad is not None]\n",
    "                    distributed.all_reduce_and_rescale_tensors(\n",
    "                        grads, float(1))\n",
    "                self.optim.step()\n",
    "\n",
    "        # in case of multi step gradient accumulation,\n",
    "        # update only after accum batches\n",
    "        if self.grad_accum_count > 1:\n",
    "            if self.n_gpu > 1:\n",
    "                grads = [p.grad.data for p in self.model.parameters()\n",
    "                         if p.requires_grad\n",
    "                         and p.grad is not None]\n",
    "                distributed.all_reduce_and_rescale_tensors(\n",
    "                    grads, float(1))\n",
    "            self.optim.step()\n",
    "\n",
    "    def _save(self, step):\n",
    "        real_model = self.model\n",
    "        # real_generator = (self.generator.module\n",
    "        #                   if isinstance(self.generator, torch.nn.DataParallel)\n",
    "        #                   else self.generator)\n",
    "\n",
    "        model_state_dict = real_model.state_dict()\n",
    "        # generator_state_dict = real_generator.state_dict()\n",
    "        checkpoint = {\n",
    "            'model': model_state_dict,\n",
    "            # 'generator': generator_state_dict,\n",
    "            'opt': self.args,\n",
    "            'optim': self.optim,\n",
    "        }\n",
    "        checkpoint_path = os.path.join(self.args.model_path, 'model_step_%d.pt' % step)\n",
    "        logger.info(\"Saving checkpoint %s\" % checkpoint_path)\n",
    "        # checkpoint_path = '%s_step_%d.pt' % (FLAGS.model_path, step)\n",
    "        if (not os.path.exists(checkpoint_path)):\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            return checkpoint, checkpoint_path\n",
    "\n",
    "    def _start_report_manager(self, start_time=None):\n",
    "        \"\"\"\n",
    "        Simple function to start report manager (if any)\n",
    "        \"\"\"\n",
    "        if self.report_manager is not None:\n",
    "            if start_time is None:\n",
    "                self.report_manager.start()\n",
    "            else:\n",
    "                self.report_manager.start_time = start_time\n",
    "\n",
    "    def _maybe_gather_stats(self, stat):\n",
    "        \"\"\"\n",
    "        Gather statistics in multi-processes cases\n",
    "\n",
    "        Args:\n",
    "            stat(:obj:onmt.utils.Statistics): a Statistics object to gather\n",
    "                or None (it returns None in this case)\n",
    "\n",
    "        Returns:\n",
    "            stat: the updated (or unchanged) stat object\n",
    "        \"\"\"\n",
    "        if stat is not None and self.n_gpu > 1:\n",
    "            return Statistics.all_gather_stats(stat)\n",
    "        return stat\n",
    "\n",
    "    def _maybe_report_training(self, step, num_steps, learning_rate,\n",
    "                               report_stats):\n",
    "        \"\"\"\n",
    "        Simple function to report training stats (if report_manager is set)\n",
    "        see `onmt.utils.ReportManagerBase.report_training` for doc\n",
    "        \"\"\"\n",
    "        if self.report_manager is not None:\n",
    "            return self.report_manager.report_training(\n",
    "                step, num_steps, learning_rate, report_stats,\n",
    "                multigpu=self.n_gpu > 1)\n",
    "\n",
    "    def _report_step(self, learning_rate, step, train_stats=None,\n",
    "                     valid_stats=None):\n",
    "        \"\"\"\n",
    "        Simple function to report stats (if report_manager is set)\n",
    "        see `onmt.utils.ReportManagerBase.report_step` for doc\n",
    "        \"\"\"\n",
    "        if self.report_manager is not None:\n",
    "            return self.report_manager.report_step(\n",
    "                learning_rate, step, train_stats=train_stats,\n",
    "                valid_stats=valid_stats)\n",
    "\n",
    "    def _maybe_save(self, step):\n",
    "        \"\"\"\n",
    "        Save the model if a model saver is set\n",
    "        \"\"\"\n",
    "        if self.model_saver is not None:\n",
    "            self.model_saver.maybe_save(step)\n",
    "\n",
    "def summary(args, b_list, device_id, pt, step):\n",
    "\n",
    "    device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
    "    if (pt != ''):\n",
    "        test_from = pt\n",
    "    else:\n",
    "        test_from = args.test_from\n",
    "    logger.info('Loading checkpoint from %s' % test_from)\n",
    "    checkpoint = torch.load(test_from, map_location=lambda storage, loc: storage)\n",
    "    opt = vars(checkpoint['opt'])\n",
    "    for k in opt.keys():\n",
    "        if (k in model_flags):\n",
    "            setattr(args, k, opt[k])\n",
    "    print(args)\n",
    "\n",
    "    config = BertConfig.from_json_file(args.bert_config_path)\n",
    "    model = Summarizer(args, device, load_pretrained_bert=False, bert_config = config)\n",
    "    model.load_cp(checkpoint)\n",
    "    model.eval()\n",
    "\n",
    "    test_iter =data_loader.Dataloader(args, _lazy_dataset_loader(b_list),\n",
    "                                  args.batch_size, device,\n",
    "                                  shuffle=False, is_test=True)\n",
    "    trainer = build_trainer(args, device_id, model, None)\n",
    "    result = trainer.summary(test_iter,step)\n",
    "    return result\n",
    "def _tally_parameters(model):\n",
    "    n_params = sum([p.nelement() for p in model.parameters()])\n",
    "    return n_params\n",
    "\n",
    "args.gpu_ranks = [int(i) for i in args.gpu_ranks.split(',')]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.visible_gpus\n",
    "\n",
    "init_logger(args.log_file)\n",
    "device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
    "device_id = 0 if device == \"cuda\" else -1\n",
    "model_flags = ['hidden_size', 'ff_size', 'heads', 'inter_layers','encoder','ff_actv', 'use_interval','rnn_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xji5mhCxcWcg"
   },
   "source": [
    "#4. Input data morp-tokenization workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Diau6__Vhhuo"
   },
   "source": [
    "##your openapi_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "moouXLWzclio"
   },
   "outputs": [],
   "source": [
    "openapi_key = '9318dc23-24ac-4b59-a99e-a29ec170bf02'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifDaOjcThpEr"
   },
   "source": [
    "##workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "UAnJ4R8b0hYd"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import urllib3\n",
    "from glob import glob\n",
    "import collections\n",
    "import six\n",
    "import gc\n",
    "\n",
    "def do_lang ( openapi_key, text ) :\n",
    "    openApiURL = \"http://aiopen.etri.re.kr:8000/WiseNLU\"\n",
    "    requestJson = {  \n",
    "        \"argument\": {\n",
    "            \"text\": text,\n",
    "            \"analysis_code\": analysisCode\n",
    "        }\n",
    "    }\n",
    "    http = urllib3.PoolManager()\n",
    "    response = http.request(\n",
    "        \"POST\",\n",
    "        openApiURL,\n",
    "        headers={\"Content-Type\": \"application/json; charset=UTF-8\", \"Authorization\" :  accessKey},\n",
    "        body=json.dumps(requestJson)\n",
    "    )\n",
    "\n",
    "    json_data = json.loads(response.data.decode('utf-8'))\n",
    "    json_result = json_data[\"result\"]\n",
    "    \n",
    "    if json_result == -1:\n",
    "        json_reason = json_data[\"reason\"]\n",
    "        if \"Invalid Access Key\" in json_reason:\n",
    "            logger.info(json_reason)\n",
    "            logger.info(\"Please check the openapi access key.\")\n",
    "            sys.exit()\n",
    "        return \"openapi error - \" + json_reason\n",
    "    else:\n",
    "        json_data = json.loads(response.data.decode('utf-8'))\n",
    "    \n",
    "        json_return_obj = json_data[\"return_object\"]\n",
    "        \n",
    "        return_result = \"\"\n",
    "        json_sentence = json_return_obj[\"sentence\"]\n",
    "        for json_morp in json_sentence:\n",
    "            for morp in json_morp[\"morp\"]:\n",
    "                return_result = return_result+str(morp[\"lemma\"])+\"/\"+str(morp[\"type\"])+\" \"\n",
    "\n",
    "        return return_result\n",
    "    \n",
    "class BertData():\n",
    "    def __init__(self, vocab_file_path):\n",
    "        self.tokenizer = Tokenizer(vocab_file_path)\n",
    "        self.sep_vid = self.tokenizer.vocab['[SEP]']\n",
    "        self.cls_vid = self.tokenizer.vocab['[CLS]']\n",
    "        self.pad_vid = self.tokenizer.vocab['[PAD]']\n",
    "\n",
    "    def preprocess(self, src):\n",
    "\n",
    "        if (len(src) == 0):\n",
    "            return None\n",
    "\n",
    "        original_src_txt = [''.join(s) for s in src]\n",
    "\n",
    "\n",
    "        idxs = [i for i, s in enumerate(src) if (len(s) > 0)]\n",
    "\n",
    "        src = [src[i][:20000] for i in idxs]\n",
    "        src = src[:10000]\n",
    "\n",
    "        if (len(src) < 3):\n",
    "            return None\n",
    "\n",
    "        src_txt = [''.join(sent) for sent in src]\n",
    "        text = ' [SEP] [CLS] '.join(src_txt)\n",
    "        src_subtokens = text.split(' ')\n",
    "        src_subtokens = src_subtokens[:510]\n",
    "        src_subtokens = ['[CLS]'] + src_subtokens + ['[SEP]']\n",
    "\n",
    "        src_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(src_subtokens)\n",
    "        _segs = [-1] + [i for i, t in enumerate(src_subtoken_idxs) if t == self.sep_vid]\n",
    "        segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]\n",
    "        segments_ids = []\n",
    "        for i, s in enumerate(segs):\n",
    "            if (i % 2 == 0):\n",
    "                segments_ids += s * [0]\n",
    "            else:\n",
    "                segments_ids += s * [1]\n",
    "        cls_ids = [i for i, t in enumerate(src_subtoken_idxs) if t == self.cls_vid]\n",
    "        labels = None\n",
    "        tgt_txt = None\n",
    "        src_txt = [original_src_txt[i] for i in idxs]\n",
    "        return src_subtoken_idxs, labels, segments_ids, cls_ids, src_txt, tgt_txt\n",
    "def convert_to_unicode(text):\n",
    "    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
    "    if six.PY3:\n",
    "        if isinstance(text, str):\n",
    "            return text\n",
    "        elif isinstance(text, bytes):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    elif six.PY2:\n",
    "        if isinstance(text, str):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        elif isinstance(text, unicode):\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    else:\n",
    "        raise ValueError(\"Not running on Python2 or Python 3?\")\n",
    "class Tokenizer(object):\n",
    "    def __init__(self, vocab_file_path):\n",
    "        self.vocab_file_path = vocab_file_path\n",
    "        \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "        vocab = collections.OrderedDict()\n",
    "        index = 0\n",
    "        with open(self.vocab_file_path, \"r\", encoding='utf-8') as reader:\n",
    "\n",
    "            while True:\n",
    "                token = convert_to_unicode(reader.readline())\n",
    "                if not token:\n",
    "                    break\n",
    "\n",
    "          ### joonho.lim @ 2019-03-15\n",
    "                if token.find('n_iters=') == 0 or token.find('max_length=') == 0 :\n",
    "\n",
    "                    continue\n",
    "                token = token.split('\\t')[0].strip('_')\n",
    "\n",
    "                token = token.strip()\n",
    "                vocab[token] = index\n",
    "                index += 1\n",
    "        self.vocab = vocab\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        \"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                ids.append(self.vocab[token])\n",
    "            except:\n",
    "                ids.append(1)\n",
    "        if len(ids) > 10000:\n",
    "            raise ValueError(\n",
    "                \"Token indices sequence length is longer than the specified maximum \"\n",
    "                \" sequence length for this BERT model ({} > {}). Running this\"\n",
    "                \" sequence through BERT will result in indexing errors\".format(len(ids), 10000)\n",
    "            )\n",
    "        return ids\n",
    "def _lazy_dataset_loader(pt_file):\n",
    "    \n",
    "    dataset = pt_file\n",
    "    \n",
    "    yield dataset\n",
    "    \n",
    "def News_to_input(text, openapi_key):\n",
    "    newstemp = do_lang(openapi_key, text)\n",
    "    news = newstemp.split(' ./SF ')[:-1]\n",
    "    bertdata = BertData('../001_bert_morp_pytorch/vocab.korean_morp.list')\n",
    "    tmp = bertdata.preprocess(news)\n",
    "    #print(tmp)\n",
    "    b_data_dict = {\"src\":tmp[0],\n",
    "               \"labels\":[0,1,2],\n",
    "               \"segs\":tmp[2],\n",
    "               \"clss\":tmp[3],\n",
    "               \"src_txt\":tmp[4],\n",
    "               \"tgt_txt\":'hehe'}\n",
    "    b_list = []\n",
    "    b_list.append(b_data_dict) \n",
    "    return b_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[responseCode] 200\n",
      "[responBody]\n",
      "{\"result\":0,\"return_object\":{\"doc_id\":\"\",\"DCT\":\"\",\"category\":\"\",\"category_weight\":0,\"title\":{\"text\":\"\",\"NE\":\"\"},\"metaInfo\":{},\"paragraphInfo\":[],\"sentence\":[{\"id\":0,\"reserve_str\":\"\",\"text\":\"아버지가방에들어가신다\",\"morp\":[{\"id\":0,\"lemma\":\"아버지\",\"type\":\"NNG\",\"position\":0,\"weight\":0.255817},{\"id\":1,\"lemma\":\"가방\",\"type\":\"NNG\",\"position\":9,\"weight\":0.054032},{\"id\":2,\"lemma\":\"에\",\"type\":\"JKB\",\"position\":15,\"weight\":0.0421654},{\"id\":3,\"lemma\":\"들어가\",\"type\":\"VV\",\"position\":18,\"weight\":0.0319283},{\"id\":4,\"lemma\":\"시\",\"type\":\"EP\",\"position\":27,\"weight\":0.0450641},{\"id\":5,\"lemma\":\"ㄴ다\",\"type\":\"EF\",\"position\":27,\"weight\":0.0463497}],\"morp_eval\":[{\"id\":0,\"result\":\"아버지가방/NNG+에/JKB+들어가/VV+시/EP+ㄴ다/EF\",\"target\":\"아버지가방에들어가신다\",\"word_id\":0,\"m_begin\":0,\"m_end\":5}],\"WSD\":[],\"word\":[{\"id\":0,\"text\":\"아버지가방에들어가신다\",\"type\":\"\",\"begin\":0,\"end\":5}],\"NE\":[],\"NE_Link\":[],\"chunk\":[],\"dependency\":[],\"phrase_dependency\":[],\"SRL\":[],\"relation\":[],\"SA\":[],\"ZA\":[]}],\"entity\":[]}}\n"
     ]
    }
   ],
   "source": [
    " #-*- coding:utf-8 -*-\n",
    "import urllib3\n",
    "import json\n",
    "\n",
    "# 언어 분석 기술 문어/구어 중 한가지만 선택해 사용\n",
    "# 언어 분석 기술(문어)\n",
    "openApiURL = \"http://aiopen.etri.re.kr:8000/WiseNLU\" \n",
    "# 언어 분석 기술(구어)\n",
    "#openApiURL = \"http://aiopen.etri.re.kr:8000/WiseNLU_spoken\"\n",
    "\n",
    "accessKey = openapi_key\n",
    "analysisCode = \"morp\"\n",
    "text = '''아버지가방에들어가신다'''\n",
    "\n",
    "requestJson = {  \n",
    "    \"argument\": {\n",
    "        \"text\": text,\n",
    "        \"analysis_code\": analysisCode\n",
    "    }\n",
    "}\n",
    "\n",
    "http = urllib3.PoolManager()\n",
    "response = http.request(\n",
    "    \"POST\",\n",
    "    openApiURL,\n",
    "    headers={\"Content-Type\": \"application/json; charset=UTF-8\", \"Authorization\" :  accessKey},\n",
    "    body=json.dumps(requestJson)\n",
    ")\n",
    "\n",
    "print(\"[responseCode] \" + str(response.status))\n",
    "print(\"[responBody]\")\n",
    "print(str(response.data,\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'일본/NNP 정부/NNG 가/JKS 문재인/NNP 대통령/NNG 이/JKS 도쿄/NNP 올림픽/NNG 을/JKO 계기/NNG 로/JKB 일본/NNP 을/JKO 방문/NNG 하/XSV 는/ETM 방향/NNG 으로/JKB 한일/NNP 양국/NNG 이/JKS 조율/NNG 하/XSV 고/EC 있/VX 다는/ETM 15/SN 일/NNB 요미우리신문/NNP 의/JKG 보도/NNG 를/JKO 부인/NNG 하/XSV 었/EP 다/EF ./SF 정부/NNG 대변/NNG 인/XSN 이/VCP ㄴ/ETM 가토/NNP 가쓰노부/NNP (/SS 加藤勝信/SH )/SS 관방/NNG 장관/NNG 은/JX 이날/NNG 오전/NNG 정례/NNG 기자/NNG 회견/NNG 에서/JKB 관련/NNG 질문/NNG 에/JKB “/SS 말씀/NNG 하/XSV 시/EP ㄴ/ETM 보도/NNG 와/JKB 같/VA 은/ETM 사실/NNG 이/JKS 없/VA 는/ETM 것/NNB 으로/JKB 알/VV ㄴ다/EF ”/SS 고/JKQ 밝히/VV 었/EP 다/EF ./SF 앞서/MAG 요미우리/NNP 는/JX 한국/NNP 측/NNB 이/JKS 도쿄/NNP 올림픽/NNG 을/JKO 계기/NNG 로/JKB 하/VV ㄴ/ETM 문/NNP 대통령/NNG 의/JKG 방일/NNG 을/JKO 타진/NNG 하/XSV 었/EP 고/EC ,/SP 일본/NNP 측/NNB 은/JX 수용/NNG 하/XSV 는/ETM 방향/NNG 이/VCP 라고/EC 이날/NNG 보도/NNG 하/XSV 었/EP 다/EF ./SF 한국/NNP 측/NNB 은/JX 문/NNP 대통령/NNG 의/JKG 방/NNG 이/VCP ㄹ/ETM 때/NNG 스가/NNP 요시히데/NNP (/SS 菅義偉/SH )/SS 총리/NNG 와/JC 처음/NNG 으로/JKB 정상/NNG 회담/NNG 을/JKO 하/VV 겠/EP 다는/ETM 생각/NNG 이/VCP 라고/EC 요미우리/NNP 는/JX 전하/VV 었/EP 다/EF ./SF 가토/NNP 장관/NNG 은/JX 한일/NNP 정상/NNG 회담/NNG 에/JKB 대하/VV ㄴ/ETM 일본/NNP 정부/NNG 의/JKG 자세/NNG 에/JKB 대하/VV 어/EC “/SS 그런/MM 사실/NNG 이/JKS 없/VA 기/ETN 때문/NNB 에/JKB 가정/NNG 의/JKG 질문/NNG 에/JKB 대하/VV 어/EC 답/NNG 하/XSV 는/ETM 것/NNB 을/JKO 삼가/VV 겠/EP 다/EF ”/SS 고/JKQ 말/NNG 하/XSV 었/EP 다/EF ./SF 그/NP 는/JX 한국/NNP 측/NNB 의/JKG 독도/NNP 방어/NNG 훈련/NNG 에/JKB ‘/SS 어떤/MM 대항/NNG 조치/NNG 를/JKO 생각/NNG 하/XSV 고/EC 있/VX 느냐/EF ’/SS 는/ETM 질문/NNG 에/JKB 는/JX “/SS 한국/NNP 해군/NNG 의/JKG 훈련/NNG 에/JKB 대하/VV 어/EC 정부/NNG 로서/JKB 는/JX 강하/VA ㄴ/ETM 관심/NNG 을/JKO 가지/VV 고/EC 주시/NNG 하/XSV 는/ETM 상황/NNG 이/VCP 어서/EC 지금/MAG 시점/NNG 에서/JKB ㄴ/JX 논평/NNG 을/JKO 삼가/VV 겠/EP 다/EF ”/SS 고/JKQ 말/NNG 을/JKO 아끼/VV 었/EP 다/EF ./SF 가토/NNP 장관/NNG 은/JX “/SS 다케시마/NNP (/SS 竹島/SH ·/SP 일본/NNP 이/JKS 주장/NNG 하/XSV 는/ETM 독도/NNP 의/JKG 명칭/NNG )/SS 는/JX 역사적/MM 사실/NNG 에/JKB 비추/VV 어/EC 보/VX 어도/EC ,/SP 국제/NNG 법/NNG 상/XSN 으로/JKB 도/JX 명백하/VA ㄴ/ETM 일본/NNP 고유/NNG 의/JKG 영토/NNG ”/SS 이/VCP 라며/EC 독도/NNP 영유/NNG 권/XSN 주장/NNG 을/JKO 되/XPN 풀이/NNG 하/XSV 었/EP 다/EF ./SF 그러/VV 면서/EC “/SS 다케시마/NNP 문제/NNG 에/JKB 대하/VV 어서/EC 는/JX 계속/MAG 우리/NP 나라/NNG 의/JKG 영토/NNG ,/SP 영해/NNG ,/SP 영공/NNG 을/JKO 단호히/MAG 지키/VV 겠/EP 다는/ETM 결의/NNG 로/JKB 냉정/NNG 하/XSA 고/EC 의연/NNG 하/XSA 게/EC 대응/NNG 하/XSV 어/EC 가/VX ㄹ/ETM 생각/NNG ”/SS 이라고/JKQ 밝히/VV 었/EP 다/EF ./SF '"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''\n",
    "일본 정부가 문재인 대통령이 도쿄올림픽을 계기로 일본을 방문하는 방향으로 한일 양국이 조율하고 있다는 15일 요미우리신문의 보도를 부인했다.\n",
    "\n",
    "정부 대변인인 가토 가쓰노부(加藤勝信) 관방장관은 이날 오전 정례 기자회견에서 관련 질문에 “말씀하신 보도와 같은 사실이 없는 것으로 안다”고 밝혔다.\n",
    "\n",
    "앞서 요미우리는 한국 측이 도쿄올림픽을 계기로 한 문 대통령의 방일을 타진했고, 일본 측은 수용하는 방향이라고 이날 보도했다.\n",
    "\n",
    "한국 측은 문 대통령의 방일 때 스가 요시히데(菅義偉) 총리와 처음으로 정상회담을 하겠다는 생각이라고 요미우리는 전했다.\n",
    "\n",
    "가토 장관은 한일 정상회담에 대한 일본 정부의 자세에 대해 “그런 사실이 없기 때문에 가정의 질문에 대해 답하는 것을 삼가겠다”고 말했다.\n",
    "\n",
    "그는 한국 측의 독도방어훈련에 ‘어떤 대항 조치를 생각하고 있느냐’는 질문에는 “한국 해군의 훈련에 대해 정부로서는 강한 관심을 가지고 주시하는 상황이어서 지금 시점에선 논평을 삼가겠다”고 말을 아꼈다.\n",
    "\n",
    "가토 장관은 “다케시마(竹島·일본이 주장하는 독도의 명칭)는 역사적 사실에 비춰봐도, 국제법상으로도 명백한 일본 고유의 영토”라며 독도 영유권 주장을 되풀이했다.\n",
    "\n",
    "그러면서 “다케시마 문제에 대해서는 계속 우리나라의 영토, 영해, 영공을 단호히 지키겠다는 결의로 냉정하고 의연하게 대응해갈 생각”이라고 밝혔다.\n",
    "'''\n",
    "\n",
    "do_lang(openapi_key, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-16 11:26:15,283 INFO] Loading checkpoint from ../models/bert_classifier/model_step_1000.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-16 11:26:15,746 INFO] loading archive file ../001_bert_morp_pytorch\n",
      "[2023-01-16 11:26:15,747 INFO] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30349\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoder': 'classifier', 'mode': 'summary', 'bert_data_path': '../bert_sample/korean', 'model_path': '../models/bert_classifier', 'bert_model': '../001_bert_morp_pytorch', 'result_path': '../results/korean', 'temp_dir': '.', 'bert_config_path': '../001_bert_morp_pytorch/bert_config.json', 'batch_size': 1000, 'use_interval': True, 'hidden_size': 128, 'ff_size': 512, 'heads': 4, 'inter_layers': 2, 'rnn_size': 512, 'param_init': 0, 'param_init_glorot': True, 'dropout': 0.1, 'optim': 'adam', 'lr': 0.002, 'report_every': 1, 'save_checkpoint_steps': 5, 'block_trigram': True, 'recall_eval': False, 'accum_count': 1, 'world_size': 1, 'visible_gpus': '-1', 'gpu_ranks': [0], 'log_file': '../logs/bert_classifier', 'test_from': '../models/bert_classifier/model_step_1000.pt'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-16 11:26:17,928 INFO] * number of parameters: 109350145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu_rank 0\n",
      "[[0 5 1 3 4 2 7 6]]\n",
      "------------------------------------------------------------\n",
      "[0, 5, 1]\n",
      "------------------------------------------------------------\n",
      "0 \n",
      "일본 정부가 문재인 대통령이 도쿄올림픽을 계기로 일본을 방문하는 방향으로 한일 양국이 조율하고 있다는 15일 요미우리신문의 보도를 부인했다\n",
      "1 \n",
      "\n",
      "정부 대변인인 가토 가쓰노부(加藤勝信) 관방장관은 이날 오전 정례 기자회견에서 관련 질문에 “말씀하신 보도와 같은 사실이 없는 것으로 안다”고 밝혔다\n",
      "5 \n",
      "\n",
      "그는 한국 측의 독도방어훈련에 ‘어떤 대항 조치를 생각하고 있느냐’는 질문에는 “한국 해군의 훈련에 대해 정부로서는 강한 관심을 가지고 주시하는 상황이어서 지금 시점에선 논평을 삼가겠다”고 말을 아꼈다\n",
      "------------------------------------------------------------\n",
      "\n",
      "일본 정부가 문재인 대통령이 도쿄올림픽을 계기로 일본을 방문하는 방향으로 한일 양국이 조율하고 있다는 15일 요미우리신문의 보도를 부인했다. \n",
      "\n",
      "정부 대변인인 가토 가쓰노부(加藤勝信) 관방장관은 이날 오전 정례 기자회견에서 관련 질문에 “말씀하신 보도와 같은 사실이 없는 것으로 안다”고 밝혔다. \n",
      "\n",
      "그는 한국 측의 독도방어훈련에 ‘어떤 대항 조치를 생각하고 있느냐’는 질문에는 “한국 해군의 훈련에 대해 정부로서는 강한 관심을 가지고 주시하는 상황이어서 지금 시점에선 논평을 삼가겠다”고 말을 아꼈다. \n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "chat_history = \"뉴스 \"\n",
    "user_input = '''\n",
    "일본 정부가 문재인 대통령이 도쿄올림픽을 계기로 일본을 방문하는 방향으로 한일 양국이 조율하고 있다는 15일 요미우리신문의 보도를 부인했다.\n",
    "\n",
    "정부 대변인인 가토 가쓰노부(加藤勝信) 관방장관은 이날 오전 정례 기자회견에서 관련 질문에 “말씀하신 보도와 같은 사실이 없는 것으로 안다”고 밝혔다.\n",
    "\n",
    "앞서 요미우리는 한국 측이 도쿄올림픽을 계기로 한 문 대통령의 방일을 타진했고, 일본 측은 수용하는 방향이라고 이날 보도했다.\n",
    "\n",
    "한국 측은 문 대통령의 방일 때 스가 요시히데(菅義偉) 총리와 처음으로 정상회담을 하겠다는 생각이라고 요미우리는 전했다.\n",
    "\n",
    "가토 장관은 한일 정상회담에 대한 일본 정부의 자세에 대해 “그런 사실이 없기 때문에 가정의 질문에 대해 답하는 것을 삼가겠다”고 말했다.\n",
    "\n",
    "그는 한국 측의 독도방어훈련에 ‘어떤 대항 조치를 생각하고 있느냐’는 질문에는 “한국 해군의 훈련에 대해 정부로서는 강한 관심을 가지고 주시하는 상황이어서 지금 시점에선 논평을 삼가겠다”고 말을 아꼈다.\n",
    "\n",
    "가토 장관은 “다케시마(竹島·일본이 주장하는 독도의 명칭)는 역사적 사실에 비춰봐도, 국제법상으로도 명백한 일본 고유의 영토”라며 독도 영유권 주장을 되풀이했다.\n",
    "\n",
    "그러면서 “다케시마 문제에 대해서는 계속 우리나라의 영토, 영해, 영공을 단호히 지키겠다는 결의로 냉정하고 의연하게 대응해갈 생각”이라고 밝혔다.\n",
    "'''\n",
    "\n",
    "\n",
    "bot_input_ids = News_to_input(chat_history + user_input, openapi_key)\n",
    "print('------------------------------------------------------------')\n",
    "\n",
    "chat_history_ids = summary(args, bot_input_ids, -1, '', None)\n",
    "print(chat_history_ids)\n",
    "print('------------------------------------------------------------')\n",
    "pred_lst = list(chat_history_ids[0][:3])\n",
    "print(pred_lst)\n",
    "print('------------------------------------------------------------')\n",
    "final_text = ''\n",
    "for i,a in enumerate(user_input.split('.')):\n",
    "    if i in pred_lst:\n",
    "        print(i, a)\n",
    "        final_text = final_text+a+'. '\n",
    "chat_history = user_input + '<token>' +final_text\n",
    "print('------------------------------------------------------------')\n",
    "print(final_text)\n",
    "print('------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n일본 정부가 문재인 대통령이 도쿄올림픽을 계기로 일본을 방문하는 방향으로 한일 양국이 조율하고 있다는 15일 요미우리신문의 보도를 부인했다.\\n\\n정부 대변인인 가토 가쓰노부(加藤勝信) 관방장관은 이날 오전 정례 기자회견에서 관련 질문에 “말씀하신 보도와 같은 사실이 없는 것으로 안다”고 밝혔다.\\n\\n앞서 요미우리는 한국 측이 도쿄올림픽을 계기로 한 문 대통령의 방일을 타진했고, 일본 측은 수용하는 방향이라고 이날 보도했다.\\n\\n한국 측은 문 대통령의 방일 때 스가 요시히데(菅義偉) 총리와 처음으로 정상회담을 하겠다는 생각이라고 요미우리는 전했다.\\n\\n가토 장관은 한일 정상회담에 대한 일본 정부의 자세에 대해 “그런 사실이 없기 때문에 가정의 질문에 대해 답하는 것을 삼가겠다”고 말했다.\\n\\n그는 한국 측의 독도방어훈련에 ‘어떤 대항 조치를 생각하고 있느냐’는 질문에는 “한국 해군의 훈련에 대해 정부로서는 강한 관심을 가지고 주시하는 상황이어서 지금 시점에선 논평을 삼가겠다”고 말을 아꼈다.\\n\\n가토 장관은 “다케시마(竹島·일본이 주장하는 독도의 명칭)는 역사적 사실에 비춰봐도, 국제법상으로도 명백한 일본 고유의 영토”라며 독도 영유권 주장을 되풀이했다.\\n\\n그러면서 “다케시마 문제에 대해서는 계속 우리나라의 영토, 영해, 영공을 단호히 지키겠다는 결의로 냉정하고 의연하게 대응해갈 생각”이라고 밝혔다.\\n<token>\\n일본 정부가 문재인 대통령이 도쿄올림픽을 계기로 일본을 방문하는 방향으로 한일 양국이 조율하고 있다는 15일 요미우리신문의 보도를 부인했다.\\n\\n정부 대변인인 가토 가쓰노부(加藤勝信) 관방장관은 이날 오전 정례 기자회견에서 관련 질문에 “말씀하신 보도와 같은 사실이 없는 것으로 안다”고 밝혔다.\\n\\n앞서 요미우리는 한국 측이 도쿄올림픽을 계기로 한 문 대통령의 방일을 타진했고, 일본 측은 수용하는 방향이라고 이날 보도했다.\\n\\n한국 측은 문 대통령의 방일 때 스가 요시히데(菅義偉) 총리와 처음으로 정상회담을 하겠다는 생각이라고 요미우리는 전했다.\\n\\n가토 장관은 한일 정상회담에 대한 일본 정부의 자세에 대해 “그런 사실이 없기 때문에 가정의 질문에 대해 답하는 것을 삼가겠다”고 말했다.\\n\\n그는 한국 측의 독도방어훈련에 ‘어떤 대항 조치를 생각하고 있느냐’는 질문에는 “한국 해군의 훈련에 대해 정부로서는 강한 관심을 가지고 주시하는 상황이어서 지금 시점에선 논평을 삼가겠다”고 말을 아꼈다.\\n\\n가토 장관은 “다케시마(竹島·일본이 주장하는 독도의 명칭)는 역사적 사실에 비춰봐도, 국제법상으로도 명백한 일본 고유의 영토”라며 독도 영유권 주장을 되풀이했다.\\n\\n그러면서 “다케시마 문제에 대해서는 계속 우리나라의 영토, 영해, 영공을 단호히 지키겠다는 결의로 냉정하고 의연하게 대응해갈 생각”이라고 밝혔다.\\n. '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nhubPG1c5zt"
   },
   "source": [
    "#5. html for SummaryBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U0HisA9GdU95"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import dash\n",
    "import dash_html_components as html\n",
    "import dash_core_components as dcc\n",
    "import dash_bootstrap_components as dbc\n",
    "from dash.dependencies import Input, Output, State\n",
    "from jupyter_dash import JupyterDash\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import torch\n",
    "def textbox(text, box=\"other\"):\n",
    "    style = {\n",
    "        \"max-width\": \"55%\",\n",
    "        \"width\": \"max-content\",\n",
    "        \"padding\": \"10px 15px\",\n",
    "        \"border-radius\": \"25px\"\n",
    "    }\n",
    "\n",
    "    if box == \"self\":\n",
    "        style[\"margin-left\"] = \"auto\"\n",
    "        style[\"margin-right\"] = 0\n",
    "\n",
    "        color = \"primary\"\n",
    "        inverse = True\n",
    "\n",
    "    elif box == \"other\":\n",
    "        style[\"margin-left\"] = 0\n",
    "        style[\"margin-right\"] = \"auto\"\n",
    "\n",
    "        color = \"light\"\n",
    "        inverse = False\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Incorrect option for `box`.\")\n",
    "\n",
    "    return dbc.Card(text, style=style, body=True, color=color, inverse=inverse)\n",
    "conversation = html.Div(\n",
    "    style={\n",
    "        \"width\": \"80%\",\n",
    "        \"max-width\": \"800px\",\n",
    "        \"height\": \"70vh\",\n",
    "        \"margin\": \"auto\",\n",
    "        \"overflow-y\": \"auto\",\n",
    "    },\n",
    "    id=\"display-conversation\",\n",
    ")\n",
    "\n",
    "controls = dbc.InputGroup(\n",
    "    style={\"width\": \"80%\", \"max-width\": \"800px\", \"margin\": \"auto\"},\n",
    "    children=[\n",
    "        dbc.Input(id=\"user-input\", placeholder=\"Write to the chatbot...\", type=\"text\"),\n",
    "        dbc.InputGroupAddon(dbc.Button(\"Submit\", id=\"submit\"), addon_type=\"append\",),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# Define app\n",
    "app = JupyterDash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
    "server = app.server\n",
    "\n",
    "\n",
    "# Define Layout\n",
    "app.layout = dbc.Container(\n",
    "    fluid=True,\n",
    "    style={'background-image': 'url(https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile21.uf.tistory.com%2Fimage%2F99A30D4B5CB15385210EA0)'},\n",
    "    children=[\n",
    "        html.H1(\"뉴스뚝딱\"),\n",
    "        html.Hr(),\n",
    "        dcc.Store(id=\"store-conversation\", data=\"\"),\n",
    "        conversation,\n",
    "        controls\n",
    "    ],\n",
    ")\n",
    "@app.callback(\n",
    "    Output(\"display-conversation\", \"children\"), [Input(\"store-conversation\", \"data\")]\n",
    ")\n",
    "def update_display(chat_history):\n",
    "    return [\n",
    "        textbox(x, box=\"self\") if i % 2 == 0 else textbox(x, box=\"other\")\n",
    "        for i, x in enumerate(chat_history.split('<token>'))\n",
    "    ]\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    [Output(\"store-conversation\", \"data\"), Output(\"user-input\", \"value\")],\n",
    "    [Input(\"submit\", \"n_clicks\"), Input(\"user-input\", \"n_submit\")],\n",
    "    [State(\"user-input\", \"value\"), State(\"store-conversation\", \"data\")],\n",
    ")\n",
    "def run_chatbot(n_clicks, n_submit, user_input, chat_history):\n",
    "    if n_clicks == 0:\n",
    "        return \"\", \"\"\n",
    "\n",
    "    if user_input is None or user_input == \"\":\n",
    "        return chat_history, \"\"\n",
    "\n",
    "    bot_input_ids = News_to_input(chat_history + user_input, openapi_key)\n",
    "        \n",
    "    chat_history_ids = summary(args, bot_input_ids, -1, '', None)\n",
    "    pred_lst = list(chat_history_ids[0][:3])\n",
    "    final_text = ''\n",
    "    for i,a in enumerate(user_input.split('. ')):\n",
    "        if i in pred_lst:\n",
    "            final_text = final_text+a+'. '\n",
    "    chat_history = user_input + '<token>' +final_text\n",
    "\n",
    "    return chat_history, \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UW0GrA7Mg5kr"
   },
   "source": [
    "#6. RUN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "i_dGcr31dBdp",
    "outputId": "27f634ca-568e-49a0-e60e-bc1e6fb643d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash app running on:\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "(async (port, path, text, element) => {\n",
       "    if (!google.colab.kernel.accessAllowed) {\n",
       "      return;\n",
       "    }\n",
       "    element.appendChild(document.createTextNode(''));\n",
       "    const url = await google.colab.kernel.proxyPort(port);\n",
       "    const anchor = document.createElement('a');\n",
       "    anchor.href = url + path;\n",
       "    anchor.target = '_blank';\n",
       "    anchor.setAttribute('data-href', url + path);\n",
       "    anchor.textContent = text;\n",
       "    element.appendChild(anchor);\n",
       "  })(8050, \"/\", \"http://127.0.0.1:8050/\", window.element)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "app.run_server(mode='external')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "3bYOoWzjYSip",
    "d3Uzgqx3cLy0",
    "ifDaOjcThpEr",
    "6nhubPG1c5zt"
   ],
   "name": "Newsdata_summarybot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
