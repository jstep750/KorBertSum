{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ee4fe65-4cf3-4b86-bc6d-bac6451fdb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /opt/ml/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "import string\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import BartForConditionalGeneration\n",
    "import random\n",
    "from itertools import combinations\n",
    "nltk.download('wordnet')\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('digit82/kobart-summarization')\n",
    "model = BartForConditionalGeneration.from_pretrained('digit82/kobart-summarization')\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 0)\n",
    "model.to(device)\n",
    "\n",
    "def make_summarization(sentences, tokenizer, model): # 문자열 생성 요약\n",
    "    if(len(sentences) < 4): return [max(sentences, key=lambda x:len(x))]\n",
    "    split = []\n",
    "    for i in range(len(sentences)//8):\n",
    "        split.append(sentences[:8])\n",
    "        sentences = sentences[8:]\n",
    "\n",
    "    for i in range(len(split)):\n",
    "        if(len(sentences) == 0): break\n",
    "        split[i].append(sentences.pop())\n",
    "    \n",
    "    if(len(sentences) != 0): split.append(sentences)\n",
    "    \n",
    "    split_sum = []\n",
    "    for sentences in split:\n",
    "        text = '\\n'.join(sentences)\n",
    "        start = time.time()\n",
    "        raw_input_ids = tokenizer.encode(text)\n",
    "        input_ids = [tokenizer.bos_token_id] + raw_input_ids + [tokenizer.eos_token_id]\n",
    "        \n",
    "        #model.generate(torch.tensor([input_ids[:1020]]).to(device), num_beams=3, max_length=256, eos_token_id=1)\n",
    "        summary_ids = model.generate(torch.tensor([input_ids]).to(device),  num_beams=4,  max_length=256, min_length=10,  eos_token_id=1)\n",
    "        print(f\"{time.time()-start:.4f} sec\")\n",
    "        sum_result = tokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True)\n",
    "        sum_result = sent_tokenize(sum_result)[0]\n",
    "        sum_result = delete_repeat_str(sum_result)\n",
    "        print(sum_result)\n",
    "        split_sum.append(sum_result)\n",
    "        print(len(split), len(split_sum))\n",
    "        print('-----------------------------------------------')\n",
    "    \n",
    "    if(len(split_sum)==1):\n",
    "        return [split_sum[0]]\n",
    "      \n",
    "    return split_sum\n",
    "\n",
    "\n",
    "def summarize_topic(document_df, topic_num, tokenizer, model): # df와 topic num을 넣으면 해당 topic num을 요약\n",
    "    sentences = []\n",
    "    numbers = []\n",
    "    for i,t in enumerate(document_df[document_df['cluster_label'] == topic_num]['opinion_text']):\n",
    "        sentences.append(eval(t)[1])\n",
    "        numbers.append(eval(t)[0])\n",
    "    result = make_summarization(sentences, tokenizer, model)\n",
    "    avg = sum(numbers)/len(numbers)\n",
    "    return (avg, result)\n",
    "\n",
    "\n",
    "def summarize_first_sentences(processed_sentences, tokenizer, model): # 문쟈열을 k-means로 토픽 별 분류(첫 문장)\n",
    "    clusternum = 1\n",
    "    document_df = get_clustered_df(processed_sentences, clusternum)\n",
    "    sum_result = []\n",
    "    for c in range(clusternum):\n",
    "        temp = get_topic_sentences(document_df, c)\n",
    "        summ = summarize_topic(document_df, c, tokenizer, model)\n",
    "        sum_result.append(summ)\n",
    "        print('===================================================')\n",
    "    \n",
    "    first_result = (sum_result[0][0], [min(sum_result[0][1], key=lambda x:len(x))])\n",
    "    print(first_result)\n",
    "    return [first_result]\n",
    "    \n",
    "\n",
    "def summarize_topk_sentences(processed_sentences, tokenizer, model): # 문쟈열을 k-means로 토픽 별 분류\n",
    "    clusternum = max(len(processed_sentences)//7, 1)\n",
    "    document_df = get_clustered_df(processed_sentences, clusternum)\n",
    "    sum_result = []\n",
    "    for c in range(clusternum):\n",
    "        temp = get_topic_sentences(document_df, c)\n",
    "        print('---------------------------------------------------')\n",
    "        summ = summarize_topic(document_df, c, tokenizer, model)\n",
    "        sum_result.append(summ)\n",
    "        print(summ)\n",
    "        print('***************************************************')\n",
    "        \n",
    "    return sorted(sum_result, key= lambda x: x[0])\n",
    "\n",
    "\n",
    "def get_clustered_df(sentences, clusternum):\n",
    "    print(clusternum)\n",
    "    \n",
    "    document_df = pd.DataFrame()\n",
    "    document_df['opinion_text'] = [str(t) for t in sentences]\n",
    "    \n",
    "    if(len(sentences) < 2):\n",
    "        document_df['cluster_label'] = 0\n",
    "        print('len document df', len(document_df))\n",
    "        return document_df.sort_values(by=['cluster_label'])\n",
    "    \n",
    "    remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "    lemmar = WordNetLemmatizer()\n",
    "    \n",
    "    # 토큰화한 각 단어들의 원형들을 리스트로 담아서 반환\n",
    "    def LemTokens(tokens):\n",
    "        return [lemmar.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # 텍스트를 Input으로 넣어서 토큰화시키고 토큰화된 단어들의 원형들을 리스트로 담아 반환\n",
    "    def LemNormalize(text):\n",
    "        return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "    \n",
    "    tfidf_vect = TfidfVectorizer(tokenizer=LemNormalize,\n",
    "                            ngram_range=(1,2),\n",
    "                            min_df=0.05, max_df=0.85)\n",
    "\n",
    "\n",
    "    ftr_vect = tfidf_vect.fit_transform(document_df['opinion_text'])\n",
    "    kmeans = KMeans(n_clusters=clusternum, max_iter=10000, random_state=42)\n",
    "    cluster_label = kmeans.fit_predict(ftr_vect)\n",
    "    \n",
    "    # 군집화한 레이블값들을 document_df 에 추가하기\n",
    "    document_df['cluster_label'] = cluster_label\n",
    "    return document_df.sort_values(by=['cluster_label'])\n",
    "    \n",
    "\n",
    "def get_topic_sentences(df, clabel):\n",
    "    lst = []\n",
    "    for i,t in enumerate(df[df['cluster_label'] == clabel]['opinion_text']):\n",
    "        print(i, t)\n",
    "        lst.append(t)\n",
    "    return lst \n",
    "\n",
    "\n",
    "def delete_similar(input_sentences):\n",
    "    if(len(input_sentences) < 2):\n",
    "        return input_sentences\n",
    "    sorted_sentences = sorted(input_sentences, key=lambda x:x[1][::-1])\n",
    "    prev_len = len(sorted_sentences)\n",
    "    \n",
    "    for i in range(5):\n",
    "        prev = sorted_sentences[0]\n",
    "        processed_sentences = []\n",
    "        for j,sentence in enumerate(sorted_sentences[1:]):\n",
    "            s1 = set(prev[1].split())\n",
    "            s2 = set(sentence[1].split())\n",
    "            actual_jaccard = float(len(s1.intersection(s2)))/float(len(s1.union(s2)))\n",
    "            if(actual_jaccard < 0.5): # if not similar\n",
    "                processed_sentences.append(prev)\n",
    "                prev = sentence\n",
    "            else:\n",
    "                print(prev)\n",
    "                print(sentence)\n",
    "                print(actual_jaccard)\n",
    "                print('-------------------------------------------')\n",
    "                \n",
    "        s1 = set(prev[1].split())\n",
    "        \n",
    "        if(len(processed_sentences) == 0):\n",
    "            processed_sentences.append(prev)\n",
    "            return processed_sentences\n",
    "        \n",
    "        s2 = set(processed_sentences[-1][1].split())\n",
    "        actual_jaccard = float(len(s1.intersection(s2)))/float(len(s1.union(s2)))\n",
    "        if(actual_jaccard < 0.5): # if not similar\n",
    "            processed_sentences.append(prev)\n",
    "        \n",
    "        sorted_sentences = sorted(processed_sentences, key=lambda x:x[1])\n",
    "        \n",
    "        if(prev_len == len(sorted_sentences)):\n",
    "            break\n",
    "        prev_len = len(sorted_sentences)\n",
    "        \n",
    "    return sorted_sentences\n",
    "\n",
    "\n",
    "def get_first_topk_sentences(df, topk_percent):\n",
    "    first_sentences = []\n",
    "    topk_sentences = []\n",
    "    for a,b in zip(df['context2'], df['topk']):\n",
    "        context = eval(str(a))\n",
    "        topk = eval(str(b))\n",
    "        k = int(len(topk)*(topk_percent/100))\n",
    "        topk = topk[:k]\n",
    "        \n",
    "        first = []\n",
    "        for item in topk:\n",
    "            if(item[0] == 0): \n",
    "                first.append(item)\n",
    "                \n",
    "        if(len(first) == 0):\n",
    "            first_sentences += [(0, context[0])]\n",
    "            topk_sentences += topk\n",
    "        else:\n",
    "            first_sentences += first\n",
    "            topk.remove(first[0])\n",
    "            topk_sentences += topk\n",
    "                \n",
    "    print('before delete similar:', len(first_sentences), len(topk_sentences))\n",
    "    first_sentences = delete_similar(first_sentences)\n",
    "    topk_sentences = delete_similar(topk_sentences)\n",
    "    print('after delete similar:', len(first_sentences), len(topk_sentences))\n",
    "    return first_sentences, topk_sentences\n",
    "\n",
    "\n",
    "def get_additional_topk_sentences(df, prev_topk_percent, next_topk_percent):\n",
    "    topk_sentences = []\n",
    "    next_topk_percent = prev_topk_percent + next_topk_percent\n",
    "    for a,b in zip(df['context2'], df['topk']):\n",
    "        context = eval(str(a))\n",
    "        topk = eval(str(b))\n",
    "        pk = int(len(topk)*(prev_topk_percent/100))\n",
    "        k = int(len(topk)*(next_topk_percent/100))\n",
    "        topk = topk[pk:k]\n",
    "        topk_sentences += topk\n",
    "                \n",
    "    print('before delete similar:', len(topk_sentences))\n",
    "    if(len(topk_sentences) == 0):\n",
    "        return topk_sentences\n",
    "    \n",
    "    topk_sentences = delete_similar(topk_sentences)\n",
    "    print('after delete similar:', len(topk_sentences))\n",
    "    return topk_sentences    \n",
    "\n",
    "\n",
    "def get_topk_sentences(k, user_input, model, tokenizer):\n",
    "    bot_input_ids = News_to_input(user_input, openapi_key)\n",
    "    \n",
    "    chat_history_ids = summary(args, bot_input_ids, -1, '', None, model)\n",
    "    pred_lst = list(chat_history_ids[0][:k])\n",
    "    final_text = []\n",
    "    for i,a in enumerate(user_input.split('.')):\n",
    "        if i in pred_lst:\n",
    "            final_text.append(a+'. ')\n",
    "    return final_text\n",
    "    \n",
    "\n",
    "\n",
    "def delete_repeat(user_text):\n",
    "    text = user_text.split()\n",
    "    x = len(text)\n",
    "    comb = list(combinations(range(x), 2))\n",
    "    sorted_comb = sorted(comb, key=lambda x: x[1]-x[0], reverse = True)\n",
    "    for item in sorted_comb:\n",
    "        start, end = item\n",
    "        if(end-start <= len(sorted_comb)/2 and end-start>2):\n",
    "            find_str = ' '.join(text[start:end])\n",
    "            rest_str = ' '.join(text[end:])\n",
    "            idx = rest_str.find(find_str)\n",
    "            if idx != -1:\n",
    "                print('deleted :', idx, '|', find_str)\n",
    "                print('ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ')\n",
    "                ret = ' '.join(text[:end]) + ' ' + rest_str[:idx] + rest_str[idx+len(find_str)+1:]\n",
    "                print(user_text)\n",
    "                print('ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ')\n",
    "                print(ret)\n",
    "                print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "                return ret\n",
    "    return user_text\n",
    "\n",
    "\n",
    "def delete_repeat_str(user_text):\n",
    "    prev = user_text\n",
    "    new = delete_repeat(prev)\n",
    "    while(new != prev):\n",
    "        prev = new\n",
    "        new = delete_repeat_str(prev)        \n",
    "    return new\n",
    "\n",
    "\n",
    "def calc_len(sum_result2):\n",
    "    ret = 0\n",
    "    for s in sum_result2:\n",
    "        tmp = 0\n",
    "        for c in s[1]:\n",
    "            tmp += len(c)\n",
    "        ret += tmp\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "546f4451-0af7-43ee-b0ab-02f501ffca50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bertopic_부동산_20221201_20221231_topk.csv',\n",
       " 'bertopic_부동산_20221201_20221203_topk.csv',\n",
       " 'bertopic_삼성전자_20221201_20221203_topk.csv',\n",
       " 'bertopic_삼성전자_20221201_20221215_topk.csv',\n",
       " 'bertopic_윤석열_20221201_20221215_topk.csv',\n",
       " 'bertopic_부동산_20221201_20221215_topk.csv',\n",
       " 'bertopic_윤석열_20221201_20221231_topk.csv',\n",
       " 'bertopic_삼성전자_20221201_20221231_topk.csv',\n",
       " 'bertopic_윤석열_20221201_20221203_topk.csv']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "\n",
    "all_paths = [\n",
    " 'bertopic(mpnet)',\n",
    " 'bertopic(paraphrase)',\n",
    " 'kmeans_12',\n",
    " 'hdbscan',\n",
    " 'kmeans_8',\n",
    " 'bertopic(SR-BERT)',\n",
    " 'kmeans_4']\n",
    "\n",
    "i = 0\n",
    "pathname = all_paths[i]\n",
    "mypath = f'./topk/raws/{all_paths[i]}'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "onlyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38c93624-5f19-4987-8f98-95606311706c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315 bertopic_부동산_20221201_20221231\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j = 0\n",
    "name = onlyfiles[j].split('_topk')[0]\n",
    "all_df = pd.read_csv(f'{mypath}/{onlyfiles[j]}')\n",
    "print(len(all_df), name)\n",
    "all_df.topic.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "95251edb-70ef-44f8-ba58-02027836acad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = 50\n",
    "print('Topic:', t)\n",
    "topic_context_df = all_df[all_df.topic == t]\n",
    "len(topic_context_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3db4f6d9-b274-4231-b1e8-1ac4c7b16996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before delete similar: 7 9\n",
      "(0, '이도(YIDO)는 관계사인 이도캐피탈자산운용이 금융위원회로부터 일반 사모집합투자업으로 등록돼 인가를 마쳤다고 26일 밝혔다. ')\n",
      "(0, '이도(YIDO)는 관계사인 이도캐피탈자산운용이 금융위원회로부터 일반 사모집합투자업으로 인가를 마치고 본격 출범한다고 26일 밝혔다. ')\n",
      "0.6428571428571429\n",
      "-------------------------------------------\n",
      "(3, ' 이를 통해 투자자에게는 부동산의 전통 수익 구조인 안정적인 배당은 물론 가치상승에 따른 capital gain(자본이익)까지 얻을 수 있는 기회를 제공하게 된다. ')\n",
      "(3, ' 이를 통해 투자자에게는 부동산의 전통 수익 구조인 안정적인 배당은 물론 가치상승에 따른 capital gain(자본이익)까지 얻을 수 있는 기회를 제공하게 된다. ')\n",
      "1.0\n",
      "-------------------------------------------\n",
      "after delete similar: 6 8\n",
      "1\n",
      "0 (0, '\"저평가된 자산 적극적으로 발굴해 가치 극대화\"다. ')\n",
      "1 (0, '금리인상과 고물가로 내년 경기가 악화할 것이란 전망 속에서 청년 일자리 창출을 위해 주식회사 이도(YIDO)가 업계 최고 수준의 체험형 인턴을 시행한다. ')\n",
      "2 (0, '부동산 대체투자 전문 자산운용사가 인가를 마치고 새롭게 출범한다. ')\n",
      "3 (0, '이 기사는 12월 27일 10:09 자본 시장의 혜안 “마켓인사이트”에 게재된 기사입니다. ')\n",
      "4 (0, '이도(YIDO)는 관계사인 이도캐피탈자산운용이 금융위원회로부터 일반 사모집합투자업으로 등록돼 인가를 마쳤다고 26일 밝혔다. ')\n",
      "5 (0, '체험형 인턴, 청년구직자에 전문 분야 직무 경험 제공다. ')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5866 sec\n",
      "deleted : 0 | 내년 경기가 악화할 것이란 전망 속에서\n",
      "ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
      "주식회사 이도(YIDO)가 업계 최고 수준의 체험형 인턴을 시행하여 내년 경기가 악화할 것이란 전망 속에서 내년 경기가 악화할 것이란 전망 속에서 청년 일자리 창출을 위해 주식회사 이도가 업계 최고 수준의 체험형 인턴을 시행한다.\n",
      "ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
      "주식회사 이도(YIDO)가 업계 최고 수준의 체험형 인턴을 시행하여 내년 경기가 악화할 것이란 전망 속에서 청년 일자리 창출을 위해 주식회사 이도가 업계 최고 수준의 체험형 인턴을 시행한다.\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "deleted : 50 | 업계 최고 수준의 체험형 인턴을\n",
      "ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
      "주식회사 이도(YIDO)가 업계 최고 수준의 체험형 인턴을 시행하여 내년 경기가 악화할 것이란 전망 속에서 청년 일자리 창출을 위해 주식회사 이도가 업계 최고 수준의 체험형 인턴을 시행한다.\n",
      "ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
      "주식회사 이도(YIDO)가 업계 최고 수준의 체험형 인턴을 시행하여 내년 경기가 악화할 것이란 전망 속에서 청년 일자리 창출을 위해 주식회사 이도가 시행한다.\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "주식회사 이도(YIDO)가 업계 최고 수준의 체험형 인턴을 시행하여 내년 경기가 악화할 것이란 전망 속에서 청년 일자리 창출을 위해 주식회사 이도가 시행한다.\n",
      "1 1\n",
      "-----------------------------------------------\n",
      "===================================================\n",
      "(0.0, ['주식회사 이도(YIDO)가 업계 최고 수준의 체험형 인턴을 시행하여 내년 경기가 악화할 것이란 전망 속에서 청년 일자리 창출을 위해 주식회사 이도가 시행한다.'])\n",
      "1\n",
      "0 (8, ' 2023년 체험형 인턴 1기는 내년 1월 15일까지 서류 접수가 이뤄지며 서류전형과 면접을 거쳐 오는 2023년 2월 4일 최종 합격자를 발표한다. ')\n",
      "1 (8, ' 골프장 자산 밸류업 전문성도 갖췄다. ')\n",
      "2 (4, ' 부동산 및 인프라 투자 및 전문 운영 역량을 갖춘 이도가 기업가치를 높인 뒤 매출 및 수익 등을 정상화하는데 역할을 하게 될 것으로 보인다. ')\n",
      "3 (5, ' 앞서 ㈜이도는 2021년 체험형 인턴 급여로 월 300만 원을 지급해 청년 구직자에게 큰 관심을 받았다. ')\n",
      "4 (6, ' 이도캐피탈매니지먼트의 초대 대표이사는 박성철 대표가 맡는다. ')\n",
      "5 (3, ' 이를 통해 투자자에게는 부동산의 전통 수익 구조인 안정적인 배당은 물론 가치상승에 따른 capital gain(자본이익)까지 얻을 수 있는 기회를 제공하게 된다. ')\n",
      "6 (13, ' 최정훈 이도 대표이사는 “이번 운용사 설립으로 부동산 개발 초기 투자는 물론 안정적인 자산에 대한 투자를 원하는 국내외 기관 투자자들의 니즈를 해소할 것”이라며 “저평가된 자산을 적극 발굴하여 가치를 극대화시키는 차별화된 운용 역량을 시장에 선보이겠다”고 말했다. ')\n",
      "7 (6, ' 취업포털 사이트 등에 따르면 2022년 신입사원의 평균 연봉은 2968만원, 대기업은 3422만 원으로 월 300만원에 못 미치고 있다. ')\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3850 sec\n",
      "2023년 체험형 인턴 1기는 내년 1월 15일까지 서류 접수가 이뤄지며 서류전형과 면접을 거쳐 오는 2023년 2월 4일 최종 합격자를 발표한다.\n",
      "1 1\n",
      "-----------------------------------------------\n",
      "(6.625, ['2023년 체험형 인턴 1기는 내년 1월 15일까지 서류 접수가 이뤄지며 서류전형과 면접을 거쳐 오는 2023년 2월 4일 최종 합격자를 발표한다.'])\n",
      "***************************************************\n",
      ".................170<570 get additional topk 15 ...............\n",
      "before delete similar: 7\n",
      "after delete similar: 7\n",
      "1\n",
      "0 (13, ' <ⓒ경제를 보는 눈, 세계를 보는 창 아시아경제( 무단전재 배포금지>다. ')\n",
      "1 (4, ' 아울러 이도는 전문 운영 역량을 통한 안정적인 운영 수익권을 얻고, 자산 소유권에 대한 수익권은 투자자에게 줌으로써 선진 경영 방식인 소유와 경영을 분리해 서로 견제 및 상호 보완하는 발전적인 경영방식을 이도캐피탈자산운용은 추구할 방침이다. ')\n",
      "2 (7, ' 업무는 2023년 3월 2일부터 6월 30일까지 4개월간 서울 중구 본사 씨티스퀘어 및 각 현장에서 이뤄진다. ')\n",
      "3 (4, ' 이도는 전문 운영 역량을 통한 안정적인 운영 수익권을 얻고, 자산 소유권에 대한 수익권은 투자자에게 줄 방침이다. ')\n",
      "4 (6, ' 이도는 프라임 오피스와 상업시설에 대한 자산관리(PM)·임대관리(LM)·시설관리(FM) 등을 아우르는 부동산 종합운영관리 서비스를 제공하고 있다. ')\n",
      "5 (5, ' 이를 통해 투자자에게 안정적인 배당을 제공하고, 가치상승에 따른 자본이익까지 얻을 수 있는 기회를 제공하겠다는 게 회사 측의 목표다. ')\n",
      "6 (10, ' 최정훈 ㈜이도 대표이사는 “금번 운용사 설립으로 부동산 개발 초기 투자는 물론 안정적인 자산에 대한 투자를 원하는 국내외 기관 투자자들의 니즈를 해소할 것”이라며 “저평가된 자산을 적극 발굴하여 가치를 극대화시키는 차별화된 운용 역량을 시장에 선보일 것”이라고 말했다. ')\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7923 sec\n",
      "이도는캐피탈자산운용은 2023년 3월 2일부터 6월 30일까지 4개월간 서울 중구 본사 씨티스퀘어 및 각 현장에서 전문 운영 역량을 통한 안정적인 운영 수익권을 얻고, 자산 소유권에 대한 수익권은 투자자에게 줌으로써 선진 경영 방식인 소유와 경영을 분리해 서로 견제 및 상호 보완하는 발전적인 경영방식을 추구할 방침이다.\n",
      "1 1\n",
      "-----------------------------------------------\n",
      "(7.0, ['이도는캐피탈자산운용은 2023년 3월 2일부터 6월 30일까지 4개월간 서울 중구 본사 씨티스퀘어 및 각 현장에서 전문 운영 역량을 통한 안정적인 운영 수익권을 얻고, 자산 소유권에 대한 수익권은 투자자에게 줌으로써 선진 경영 방식인 소유와 경영을 분리해 서로 견제 및 상호 보완하는 발전적인 경영방식을 추구할 방침이다.'])\n",
      "***************************************************\n",
      ".................349<570 get additional topk 22 ...............\n",
      "before delete similar: 7\n",
      "after delete similar: 7\n",
      "1\n",
      "0 (8, ' 서울 도심에 있는 프라임 오피스 씨티스퀘어의 공실률을 4개월만에 100%에서 0%로 낮추기도 했다. ')\n",
      "1 (4, ' 선진 경영 방식인 소유와 경영의 분리를 통해 서로 견제하고, 상호보완하는 발전적인 경영방식을 추구한다. ')\n",
      "2 (4, ' 아울러 이도는 전문 운영 역량을 통한 안정적인 운영 수익권을 얻고, 자산 소유권에 대한 수익권은 투자자에게 줌으로써 선진 경영 방식인 소유와 경영의 분리하여 서로 견제 및 상호 보완하는 발전적인 경영방식을 이도캐피탈자산운용은 추구할 방침이다. ')\n",
      "3 (7, ' 앞서 이도는 2021년 체험형 인턴 급여로 월 300만원을 지급해 청년 구직자에게 큰 관심을 받았다. ')\n",
      "4 (6, ' 이도는 전문적인 운영 역량을 통해 안정적인 운영 수익권을 얻고, 자산 소유권에 대한 수익권은 투자자에게 제공하겠다는 계획이다. ')\n",
      "5 (5, ' 이도캐피탈매니지먼트의 초대 대표이사에는 박성철 대표이사가 선임됐다. ')\n",
      "6 (12, ' 최정훈 ㈜이도 대표이사는 “이번 인턴 프로그램이 우리나라를 이끌 청년들의 직무 역량을 강화하고, 경제적 부담을 해소하는 기회가 되길 바란다”라며 “청년 일자리 창출을 통한 고용 활성화로 삶의 질을 높인다는 비전을 실천하는 동시에 ESG 경영도 강화할 것”이라고 말했다. ')\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8798 sec\n",
      "서울 도심에 있는 프라임 오피스 씨티스퀘어의 공실률을 4개월만에 100%에서 0%로 낮추기도 한 이도캐피탈자산운용은 전문 운영 역량을 통해 안정적인 운영 수익권을 얻고, 자산 소유권에 대한 수익권은 투자자에게 줌으로써 선진 경영 방식인 소유와 경영의 분리하여 서로 견제 및 상호 보완하는 발전적인 경영방식을 추구할 방침이다.\n",
      "1 1\n",
      "-----------------------------------------------\n",
      "(6.571428571428571, ['서울 도심에 있는 프라임 오피스 씨티스퀘어의 공실률을 4개월만에 100%에서 0%로 낮추기도 한 이도캐피탈자산운용은 전문 운영 역량을 통해 안정적인 운영 수익권을 얻고, 자산 소유권에 대한 수익권은 투자자에게 줌으로써 선진 경영 방식인 소유와 경영의 분리하여 서로 견제 및 상호 보완하는 발전적인 경영방식을 추구할 방침이다.'])\n",
      "***************************************************\n",
      ".................530<570 get additional topk 29 ...............\n",
      "before delete similar: 4\n",
      "after delete similar: 4\n",
      "1\n",
      "0 (6, ' 2023년 체험형 인턴 1기는 2023년 1월 15일까지 서류 접수가 진행되며, 서류전형과 면접을 거쳐 오는 2023년 2월 4일 최종 합격자가 발표된다. ')\n",
      "1 (8, ' 이도는 국내에서 프라임오피스 및 상업시설에 대한 PM(자산관리)·LM(임대관리)·FM(시설관리) 등 부동산 종합 운영관리 서비스를 제공한다. ')\n",
      "2 (7, ' 이도는 프라임오피스와 상업시설에 대한 자산관리(PM)·임대관리(LM)·시설관리(FM) 등을 아우르는 부동산 종합운영관리 서비스를 제공하고 있다. ')\n",
      "3 (5, ' 체험형 인턴의 급여는 대기업 대졸 신입사원보다 높은 수준인 월 300만원으로 업계 최고 수준이다. ')\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8367 sec\n",
      "국내에서 프라임오피스 및 상업시설에 대한 PM(자산관리)·LM(임대관리)·FM(시설관리) 등 부동산 종합 운영관리 서비스를 제공하는 이도는 프라임오피스와 상업시설에 대한 자산관리(PM)·임대관리(LM)·시설관리(FM) 등을 아우르는 부동산 종합운영관리 서비스를 제공하고 있다.\n",
      "1 1\n",
      "-----------------------------------------------\n",
      "(6.5, ['국내에서 프라임오피스 및 상업시설에 대한 PM(자산관리)·LM(임대관리)·FM(시설관리) 등 부동산 종합 운영관리 서비스를 제공하는 이도는 프라임오피스와 상업시설에 대한 자산관리(PM)·임대관리(LM)·시설관리(FM) 등을 아우르는 부동산 종합운영관리 서비스를 제공하고 있다.'])\n",
      "***************************************************\n",
      "======================= Final result =========================\n",
      "주식회사 이도(YIDO)가 업계 최고 수준의 체험형 인턴을 시행하여 내년 경기가 악화할 것이란 전망 속에서 청년 일자리 창출을 위해 주식회사 이도가 시행한다.\n",
      "\n",
      "국내에서 프라임오피스 및 상업시설에 대한 PM(자산관리)·LM(임대관리)·FM(시설관리) 등 부동산 종합 운영관리 서비스를 제공하는 이도는 프라임오피스와 상업시설에 대한 자산관리(PM)·임대관리(LM)·시설관리(FM) 등을 아우르는 부동산 종합운영관리 서비스를 제공하고 있다.\n",
      "\n",
      "서울 도심에 있는 프라임 오피스 씨티스퀘어의 공실률을 4개월만에 100%에서 0%로 낮추기도 한 이도캐피탈자산운용은 전문 운영 역량을 통해 안정적인 운영 수익권을 얻고, 자산 소유권에 대한 수익권은 투자자에게 줌으로써 선진 경영 방식인 소유와 경영의 분리하여 서로 견제 및 상호 보완하는 발전적인 경영방식을 추구할 방침이다.\n",
      "\n",
      "2023년 체험형 인턴 1기는 내년 1월 15일까지 서류 접수가 이뤄지며 서류전형과 면접을 거쳐 오는 2023년 2월 4일 최종 합격자를 발표한다.\n",
      "\n",
      "이도는캐피탈자산운용은 2023년 3월 2일부터 6월 30일까지 4개월간 서울 중구 본사 씨티스퀘어 및 각 현장에서 전문 운영 역량을 통한 안정적인 운영 수익권을 얻고, 자산 소유권에 대한 수익권은 투자자에게 줌으로써 선진 경영 방식인 소유와 경영을 분리해 서로 견제 및 상호 보완하는 발전적인 경영방식을 추구할 방침이다.\n",
      "\n",
      "\n",
      "3.9469 sec\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "first_sentences, topk_sentences = get_first_topk_sentences(topic_context_df, 15)\n",
    "\n",
    "sum_result1 = summarize_first_sentences(first_sentences, tokenizer, model)\n",
    "sum_result2 = sum_result1 + summarize_topk_sentences(topk_sentences, tokenizer, model)\n",
    "\n",
    "prev_topk = 15\n",
    "\n",
    "threshold = min(570, len(topic_context_df) * 120)\n",
    "while(calc_len(sum_result2) < threshold): # 너무 짧을 때\n",
    "    print(f'.................{calc_len(sum_result2)}<{threshold} get additional topk {prev_topk} ...............')\n",
    "    topk_sentences2 = get_additional_topk_sentences(topic_context_df, prev_topk, 7)\n",
    "    if(len(topk_sentences2) > 0):\n",
    "        sum_result3 = summarize_topk_sentences(topk_sentences2, tokenizer, model)\n",
    "        sum_result2 = sorted(sum_result3 + sum_result2, key=lambda x:x[0])\n",
    "    prev_topk += 7\n",
    "\n",
    "final_result = ''\n",
    "for v in sum_result2:\n",
    "    paragraph = [s.strip() for s in v[1]]\n",
    "    new_paragraph = [s if('.' in s[-2: ]) else s+'. ' for s in paragraph]\n",
    "    final_result += ' '.join(new_paragraph)+'\\n\\n'\n",
    "\n",
    "\n",
    "print('======================= Final result =========================')\n",
    "print(final_result)\n",
    "print(f\"{time.time()-start:.4f} sec\")\n",
    "print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "if not os.path.exists(f'./summary/{pathname}/'):\n",
    "    os.makedirs(f'./summary/{pathname}/')\n",
    "f = open(f'./summary/{pathname}/{name}_summary_{i}.txt', 'w')\n",
    "f.write(final_result)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2c1caa-ced7-4cfc-bae8-263a213f5982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
