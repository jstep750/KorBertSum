{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8ee4fe65-4cf3-4b86-bc6d-bac6451fdb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "import string\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import BartForConditionalGeneration\n",
    "import random\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('digit82/kobart-summarization')\n",
    "model = BartForConditionalGeneration.from_pretrained('digit82/kobart-summarization')\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 0)\n",
    "model.to(device)\n",
    "\n",
    "def make_summarization(sentences, tokenizer, model): # 문자열 생성 요약\n",
    "    if(len(sentences) < 4): return [max(sentences, key=lambda x:len(x))]\n",
    "    split = []\n",
    "    for i in range(len(sentences)//8):\n",
    "        split.append(sentences[:8])\n",
    "        sentences = sentences[8:]\n",
    "\n",
    "    for i in range(len(split)):\n",
    "        if(len(sentences) == 0): break\n",
    "        split[i].append(sentences.pop())\n",
    "    \n",
    "    if(len(sentences) != 0): split.append(sentences)\n",
    "    \n",
    "    split_sum = []\n",
    "    for sentences in split:\n",
    "        text = '\\n'.join(sentences)\n",
    "        start = time.time()\n",
    "        raw_input_ids = tokenizer.encode(text)\n",
    "        input_ids = [tokenizer.bos_token_id] + raw_input_ids + [tokenizer.eos_token_id]\n",
    "        \n",
    "        #model.generate(torch.tensor([input_ids[:1020]]).to(device), num_beams=3, max_length=256, eos_token_id=1)\n",
    "        summary_ids = model.generate(torch.tensor([input_ids]).to(device),  num_beams=4,  max_length=256, min_length=10,  eos_token_id=1)\n",
    "        print(f\"{time.time()-start:.4f} sec\")\n",
    "        sum_result = tokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True)\n",
    "        sum_result = sent_tokenize(sum_result)[0]\n",
    "        sum_result = delete_repeat_str(sum_result)\n",
    "        print(sum_result)\n",
    "        split_sum.append(sum_result)\n",
    "        print(len(split), len(split_sum))\n",
    "        print('-----------------------------------------------')\n",
    "    \n",
    "    if(len(split_sum)==1):\n",
    "        return [split_sum[0]]\n",
    "      \n",
    "    return split_sum\n",
    "\n",
    "\n",
    "def summarize_topic(document_df, topic_num, tokenizer, model): # df와 topic num을 넣으면 해당 topic num을 요약\n",
    "    sentences = []\n",
    "    numbers = []\n",
    "    for i,t in enumerate(document_df[document_df['cluster_label'] == topic_num]['opinion_text']):\n",
    "        sentences.append(eval(t)[1])\n",
    "        numbers.append(eval(t)[0])\n",
    "    result = make_summarization(sentences, tokenizer, model)\n",
    "    avg = sum(numbers)/len(numbers)\n",
    "    return (avg, result)\n",
    "\n",
    "\n",
    "def summarize_first_sentences(processed_sentences, tokenizer, model): # 문쟈열을 k-means로 토픽 별 분류(첫 문장)\n",
    "    clusternum = 1\n",
    "    document_df = get_clustered_df(processed_sentences, clusternum)\n",
    "    sum_result = []\n",
    "    for c in range(clusternum):\n",
    "        temp = get_topic_sentences(document_df, c)\n",
    "        summ = summarize_topic(document_df, c, tokenizer, model)\n",
    "        sum_result.append(summ)\n",
    "        print('===================================================')\n",
    "    \n",
    "    first_result = (sum_result[0][0], [min(sum_result[0][1], key=lambda x:len(x))])\n",
    "    print(first_result)\n",
    "    return [first_result]\n",
    "    \n",
    "\n",
    "def summarize_topk_sentences(processed_sentences, tokenizer, model): # 문쟈열을 k-means로 토픽 별 분류\n",
    "    clusternum = max(len(processed_sentences)//7, 1)\n",
    "    document_df = get_clustered_df(processed_sentences, clusternum)\n",
    "    sum_result = []\n",
    "    for c in range(clusternum):\n",
    "        temp = get_topic_sentences(document_df, c)\n",
    "        print('---------------------------------------------------')\n",
    "        summ = summarize_topic(document_df, c, tokenizer, model)\n",
    "        sum_result.append(summ)\n",
    "        print(summ)\n",
    "        print('***************************************************')\n",
    "        \n",
    "    return sorted(sum_result, key= lambda x: x[0])\n",
    "\n",
    "\n",
    "def get_clustered_df(sentences, clusternum):\n",
    "    print(clusternum)\n",
    "    \n",
    "    document_df = pd.DataFrame()\n",
    "    document_df['opinion_text'] = [str(t) for t in sentences]\n",
    "    \n",
    "    if(len(sentences) < 2):\n",
    "        document_df['cluster_label'] = 0\n",
    "        print('len document df', len(document_df))\n",
    "        return document_df.sort_values(by=['cluster_label'])\n",
    "    \n",
    "    remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "    lemmar = WordNetLemmatizer()\n",
    "    \n",
    "    # 토큰화한 각 단어들의 원형들을 리스트로 담아서 반환\n",
    "    def LemTokens(tokens):\n",
    "        return [lemmar.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # 텍스트를 Input으로 넣어서 토큰화시키고 토큰화된 단어들의 원형들을 리스트로 담아 반환\n",
    "    def LemNormalize(text):\n",
    "        return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "    \n",
    "    tfidf_vect = TfidfVectorizer(tokenizer=LemNormalize,\n",
    "                            ngram_range=(1,2),\n",
    "                            min_df=0.05, max_df=0.85)\n",
    "\n",
    "\n",
    "    ftr_vect = tfidf_vect.fit_transform(document_df['opinion_text'])\n",
    "    kmeans = KMeans(n_clusters=clusternum, max_iter=10000, random_state=42)\n",
    "    cluster_label = kmeans.fit_predict(ftr_vect)\n",
    "    \n",
    "    # 군집화한 레이블값들을 document_df 에 추가하기\n",
    "    document_df['cluster_label'] = cluster_label\n",
    "    return document_df.sort_values(by=['cluster_label'])\n",
    "    \n",
    "\n",
    "def get_topic_sentences(df, clabel):\n",
    "    lst = []\n",
    "    for i,t in enumerate(df[df['cluster_label'] == clabel]['opinion_text']):\n",
    "        print(i, t)\n",
    "        lst.append(t)\n",
    "    return lst \n",
    "\n",
    "\n",
    "def delete_similar(input_sentences):\n",
    "    if(len(input_sentences) < 2):\n",
    "        return input_sentences\n",
    "    sorted_sentences = sorted(input_sentences, key=lambda x:x[1][::-1])\n",
    "    prev_len = len(sorted_sentences)\n",
    "    \n",
    "    for i in range(5):\n",
    "        prev = sorted_sentences[0]\n",
    "        processed_sentences = []\n",
    "        for j,sentence in enumerate(sorted_sentences[1:]):\n",
    "            s1 = set(prev[1].split())\n",
    "            s2 = set(sentence[1].split())\n",
    "            actual_jaccard = float(len(s1.intersection(s2)))/float(len(s1.union(s2)))\n",
    "            if(actual_jaccard < 0.5): # if not similar\n",
    "                processed_sentences.append(prev)\n",
    "                prev = sentence\n",
    "            else:\n",
    "                print(prev)\n",
    "                print(sentence)\n",
    "                print(actual_jaccard)\n",
    "                print('-------------------------------------------')\n",
    "                \n",
    "        s1 = set(prev[1].split())\n",
    "        \n",
    "        if(len(processed_sentences) == 0):\n",
    "            processed_sentences.append(prev)\n",
    "            return processed_sentences\n",
    "        \n",
    "        s2 = set(processed_sentences[-1][1].split())\n",
    "        actual_jaccard = float(len(s1.intersection(s2)))/float(len(s1.union(s2)))\n",
    "        if(actual_jaccard < 0.5): # if not similar\n",
    "            processed_sentences.append(prev)\n",
    "        \n",
    "        sorted_sentences = sorted(processed_sentences, key=lambda x:x[1])\n",
    "        \n",
    "        if(prev_len == len(sorted_sentences)):\n",
    "            break\n",
    "        prev_len = len(sorted_sentences)\n",
    "        \n",
    "    return sorted_sentences\n",
    "\n",
    "\n",
    "def get_first_topk_sentences(df, topk_percent):\n",
    "    first_sentences = []\n",
    "    topk_sentences = []\n",
    "    for a,b in zip(df['context2'], df['topk']):\n",
    "        context = eval(str(a))\n",
    "        topk = eval(str(b))\n",
    "        k = int(len(topk)*(topk_percent/100))\n",
    "        topk = topk[:k]\n",
    "        \n",
    "        first = []\n",
    "        for item in topk:\n",
    "            if(item[0] == 0): \n",
    "                first.append(item)\n",
    "                \n",
    "        if(len(first) == 0):\n",
    "            first_sentences += [(0, context[0])]\n",
    "            topk_sentences += topk\n",
    "        else:\n",
    "            first_sentences += first\n",
    "            topk.remove(first[0])\n",
    "            topk_sentences += topk\n",
    "                \n",
    "    print('before delete similar:', len(first_sentences), len(topk_sentences))\n",
    "    first_sentences = delete_similar(first_sentences)\n",
    "    topk_sentences = delete_similar(topk_sentences)\n",
    "    print('after delete similar:', len(first_sentences), len(topk_sentences))\n",
    "    return first_sentences, topk_sentences\n",
    "\n",
    "\n",
    "def get_additional_topk_sentences(df, prev_topk_percent, next_topk_percent):\n",
    "    topk_sentences = []\n",
    "    next_topk_percent = prev_topk_percent + next_topk_percent\n",
    "    for a,b in zip(df['context2'], df['topk']):\n",
    "        context = eval(str(a))\n",
    "        topk = eval(str(b))\n",
    "        pk = int(len(topk)*(prev_topk_percent/100))\n",
    "        k = int(len(topk)*(next_topk_percent/100))\n",
    "        topk = topk[pk:k]\n",
    "        topk_sentences += topk\n",
    "                \n",
    "    print('before delete similar:', len(topk_sentences))\n",
    "    if(len(topk_sentences) == 0):\n",
    "        return topk_sentences\n",
    "    \n",
    "    topk_sentences = delete_similar(topk_sentences)\n",
    "    print('after delete similar:', len(topk_sentences))\n",
    "    return topk_sentences    \n",
    "\n",
    "\n",
    "def get_topk_sentences(k, user_input, model, tokenizer):\n",
    "    bot_input_ids = News_to_input(user_input, openapi_key)\n",
    "    \n",
    "    chat_history_ids = summary(args, bot_input_ids, -1, '', None, model)\n",
    "    pred_lst = list(chat_history_ids[0][:k])\n",
    "    final_text = []\n",
    "    for i,a in enumerate(user_input.split('.')):\n",
    "        if i in pred_lst:\n",
    "            final_text.append(a+'. ')\n",
    "    return final_text\n",
    "    \n",
    "\n",
    "\n",
    "def delete_repeat(user_text):\n",
    "    text = user_text.split()\n",
    "    x = len(text)\n",
    "    comb = list(combinations(range(x), 2))\n",
    "    sorted_comb = sorted(comb, key=lambda x: x[1]-x[0], reverse = True)\n",
    "    for item in sorted_comb:\n",
    "        start, end = item\n",
    "        if(end-start <= len(sorted_comb)/2 and end-start>2):\n",
    "            find_str = ' '.join(text[start:end])\n",
    "            rest_str = ' '.join(text[end:])\n",
    "            idx = rest_str.find(find_str)\n",
    "            if idx != -1:\n",
    "                print('deleted :', idx, '|', find_str)\n",
    "                print('ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ')\n",
    "                ret = ' '.join(text[:end]) + ' ' + rest_str[:idx] + rest_str[idx+len(find_str)+1:]\n",
    "                print(user_text)\n",
    "                print('ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ')\n",
    "                print(ret)\n",
    "                print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "                return ret\n",
    "    return user_text\n",
    "\n",
    "\n",
    "def delete_repeat_str(user_text):\n",
    "    prev = user_text\n",
    "    new = delete_repeat(prev)\n",
    "    while(new != prev):\n",
    "        prev = new\n",
    "        new = delete_repeat_str(prev)        \n",
    "    return new\n",
    "\n",
    "\n",
    "def calc_len(sum_result2):\n",
    "    ret = 0\n",
    "    for s in sum_result2:\n",
    "        tmp = 0\n",
    "        for c in s[1]:\n",
    "            tmp += len(c)\n",
    "        ret += tmp\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "546f4451-0af7-43ee-b0ab-02f501ffca50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bertopic_부동산_20221201_20221203_pre2.csv',\n",
       " 'bertopic_삼성전자_20221201_20221231_pre2.csv',\n",
       " 'bertopic_부동산_20221201_20221231_pre2.csv',\n",
       " 'bertopic_부동산_20221201_20221215_pre2.csv',\n",
       " 'bertopic_삼성전자_20221201_20221203_pre2.csv',\n",
       " 'bertopic_윤석열_20221201_20221203_pre2.csv',\n",
       " 'bertopic_윤석열_20221201_20221215_pre2.csv',\n",
       " 'bertopic_삼성전자_20221201_20221215_pre2.csv',\n",
       " 'bertopic_윤석열_20221201_20221231_pre2.csv']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "\n",
    "all_paths = [\n",
    " 'bertopic(mpnet)',\n",
    " 'bertopic(paraphrase)',\n",
    " 'kmeans_12',\n",
    " 'hdbscan',\n",
    " 'kmeans_8',\n",
    " 'bertopic(SR-BERT)',\n",
    " 'kmeans_4']\n",
    "\n",
    "i = 0\n",
    "mypath = f'./preprocessed2/{all_paths[i]}'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "onlyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "38c93624-5f19-4987-8f98-95606311706c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j = 2\n",
    "all_df = pd.read_csv(f'{mypath}/{onlyfiles[j]}')\n",
    "print(len(all_df))\n",
    "all_df.topic.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "95251edb-70ef-44f8-ba58-02027836acad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 39\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = 39\n",
    "print('Topic:', t)\n",
    "topic_context_df = all_df[all_df.topic == t]\n",
    "len(topic_context_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "50d3c56d-2d20-4b1a-b78a-2da6fc8bb1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'■ 진행 : 안보라 앵커\\n\\n■ 출연 : 윤지해 부동산R114 수석연구원\\n\\n\\n\\n* 아래 텍스트는 실제 방송 내용과 차이가 있을 수 있으니 보다 정확한 내용은 방송으로 확인하시기 바랍니다. 인용 시 [YTN 뉴스라이더] 명시해주시기 바랍니다.\\n\\n\\n\\n[앵커]\\n\\n추운 날씨만큼이나 부동산 시장에도 역대급 한파가 찾아왔습니다. 98년 외환위기 이후, 가장 극심한 침체라는 분석도 있습니다. 윤지해 부동산R114 수석연구원과 함께 전망해 봅니다. 어서 오세요.\\n\\n\\n\\n[윤지해]\\n\\n안녕하세요.\\n\\n\\n\\n[앵커]\\n\\n추운데 잘 오셨습니까?\\n\\n\\n\\n[윤지해]\\n\\n많이 춥네요.\\n\\n\\n\\n[앵커]\\n\\n오늘 날씨만큼이나 시장도 정말 춥네요. 그런데 요즘 깡통전세, 전세사기 이런 뉴스 저도 많이 전하거든요. 결국 집값이 떨어진 게 큰 이유라고 봐야 됩니까?\\n\\n\\n\\n[윤지해]\\n\\n지난 1년을 돌아보시면 우리가 급격히 상승하는 것들을 봤잖아요. 거기서도 왜곡이 굉장히 많이 일어나고요. 지금처럼 급격하게 하락을 하게 되면 시장에서도 상당히 이렇게 역전세 같은 그런 왜곡들이 일어납니다.\\n\\n\\n\\n[앵커]\\n\\n저희가 부동산 시장 얼었다, 얼었다, 이렇게 표현했는데 수치를 보니까 1월부터 11월까지 전국 아파트 값이 4.79%가 하락했습니다. 이게 한국부동산원이 조사한 건데 시세 조사한 이후로 가장 큰 하락폭이라고 합니다. 가장 큰 원인이라고 보세요?\\n\\n\\n\\n[윤지해]\\n\\n원인은 크게 세 가지입니다. 크게 봐서는 세 가지인데 일단 첫 번째는 이미 아시는 것처럼 급격하게 금리가 인상이 된 부분 하나가 있었고요. 그리고 추가적으로 정부가 지난해 10월에 대출규제, 그러니까 DSR이라고 하는 총부채원리금상환규제를 도입하다 보니까 아무래도 유동성이 축소되는 이슈가 있었고요. 거기에 최근 한 5년 사이에 많이 오르지 않았습니까? 그러다 보니까 수요자들이 가격에 대한 저항감이 커졌다, 이렇게 세 가지 이유로 보시면 될 것 같습니다.\\n\\n\\n\\n[앵커]\\n\\n가격 말씀해 주셔서요. 지역 보니까 세종, 대구, 수도권의 하락폭이 크더라고요. 이런 지역들은 지난해 상승폭이 컸던 지역인데 이게 과도한 급락인지 아니면 좀 거품이 빠진다고 표현해야 할지, 어떻게 전망하세요?\\n\\n\\n\\n[윤지해]\\n\\n일종의 되돌림이다라고 보시면 될 것 같아요. 예를 들어서 세종시 같은 경우는 2020년에만 40~50%가 올랐습니다. 일반적으로 부동산은 연간 10%만 올라도 굉장한 급등세입니다. 그럼에도 불구하고 수도권도 지난해 20% 정도 올랐고요. 대구 같은 경우도 최근 한 2년 사이에 20% 이상 올랐는데 경기권, 인천권 일부 지역은 40%, 50% 오르는, 그러니까 GTX 호재가 있었던 지역들, 이런 곳들은 올랐기 때문에 그 정도의 급격한 상승을 유발할 만한 요인들이 없는 것은 아니었습니다. 이른바 수요가 폭증하는 교통 호재에 따라서 오르고 이런 부분들이 있었는데 우리가 GTX만 보셔도 아직 착공을 안 한 곳들이 상당합니다.\\n\\n\\n\\n[앵커]\\n\\n언제 될지도 사실은 까마득하고.\\n\\n\\n\\n[윤지해]\\n\\n그럼에도 불구하고 미래가치를 너무 빨리 반영한 거죠. 우리가 예를 들어서 연간 5%씩 꾸준히 10년 동안 오른다면 사실 시장에서 거품이라고 인식하지 않을 텐데 연간 40%, 50% 오른다면 수요자가 체감할 때는 이건 좀 너무한 거 아니냐, 이런 생각이 들기는 합니다.\\n\\n\\n\\n[앵커]\\n\\n현실적으로 받아들이기 어려운 큰 폭의 상승세였기 때문이다, 이렇게 분석을 해 주셨고요.\\n\\n\\n\\n[윤지해]\\n\\n네, 그렇기 때문에 되돌림도 빠르다라고 보시면 될 것 같습니다.\\n\\n\\n\\n[앵커]\\n\\n거래량 보면 아파트 거래량이 역대 최저치래요. 그런데 최초로 50만 건 미만을 기록할 것이다, 이런 전망도 나오고 있고, 그런데 거래량이 이렇게 좀 줄어든 게 어떤 의미인지가 저는 궁금하더라고요.\\n\\n\\n\\n[윤지해]\\n\\n가장 큰 원인은 수요 위축이죠. 수요 위축의 원인은 금리인상과 대출규제, 앞서 말씀드린 부분들인데 사실 지금의 거래량이 정상은 아닙니다. 왜 그러냐면 우리가 2008년 글로벌 금융위기 국면에 있는 것은 아니잖아요, 현재 상황이. 그럼에도 불구하고 수요자가 대출 여력 이런 부분에 있어서 워낙 축소가 되다 보니까 실수요자조차도 쉽게 주거 이동이 안 되는 그런 국면이 있다고 볼 수 있는데 우리가 일반적으로 2008년 글로벌 금융위기 이후에 한 12년, 14년 정도의 시간이 있지 않았습니까? 그때 이후에 재고물량이 상당히 늘어났어요. 그래서 한 500만 채 정도의 재고물량이 늘어났는데 그럼에도 불구하고 그때보다 거래량이 적다는 얘기는 사실은 비정상적인 국면이다라고 보시면 될 것 같습니다.\\n\\n\\n\\n[앵커]\\n\\n어쨌든 정상적인 시장은 아니다라는 거네요. 그러니까 제 기억에 한참 아파트 값이 급등할 때 그때는 거래되는 것마다 신고가를 찍었고 그때도 사실 거래량이 많지는 않았습니다마는 그런 상황이었는데 이번에는 거래되는 양이 많지는 않지만 계속 저가 거래가 되고 있는 상황인 거예요. 그래서 이런 거래량 감소가 혹시 하락 시그널로도 볼 수 있는 건지, 이렇게 해석을 해도 되는 건지 궁금하더라고요.\\n\\n\\n\\n[윤지해]\\n\\n하락 시그널이라기보다는 이미 하락은 했고요. 한 상황에서 이 거래량이 장기간 유지가 되면 사실은 유관 산업들이 침체되기 시작합니다. 건설사와 관련된 문제든 혹은 인테리어 종사하시는 분이든 혹은 이사업체에 있든 중개사무소에 있으신 분들이든 이렇게 유관 산업과 관련된 산업들이 거래를 기반으로 해서 소득을 유발하거든요. 소득이 줄어든다는 것은 그 시장 자체가 침체가 되는 거잖아요.\\n\\n\\n\\n그러다 보면 아마 건설사 중심으로 부도가 되는 그런 사례들도 늘어나는 거고요. 그러다 보면 사실은 정부가 정책을 잘해서 시장을 잡으면 사실은 좋습니다마는 우리가 일반적으로 자산시장이 전체적으로 꺾일 때 부동산시장도 같이 꺾이는 거라면 그것은 정책에 의해서 그렇게 됐다라기보다는 거시경제 쪽의 침체 원인에서 이유를 찾아야 되지 않나 싶습니다.\\n\\n\\n\\n[앵커]\\n\\n어쨌든 유관 산업이 침체될 위기에 있는 상황인 것 같고 그 연관된 시장 중 하나가 청약시장인데 청약시장도 겨울왕국이더라고요. 지금 사실상 최대어라는 수식어가 늘 따라다니던 게 둔촌주공 분양이었는데 성적표는 생각보다 너무나 부진합니다. 이거 분양 시장에서도 사실상 관망세가 시작이 된 건가, 이런 생각도 드는데 어떻게 보세요?\\n\\n\\n\\n[윤지해]\\n\\n일단 지난해까지 혹은 그 전년도까지는 시장에서 묻지 마 청약이었습니다. 본인의 생활권 혹은 자산여건들을 고려하지 않고 시세차익 기대감에 묻지도 따지지도 않고 청약을 했다면 지금은 선별 청약의 시기로 왔습니다. 그래서 선별 청약하는 시기가 그러면 시장에 나쁘냐 하면 그렇지는 않고요. 사실은 이 시장이 정상입니다.\\n\\n\\n\\n[앵커]\\n\\n선별 청약하는 시장이.\\n\\n\\n\\n[윤지해]\\n\\n네, 맞습니다. 왜냐하면 본인이 거주하고 싶은 주택 혹은 본인이 자산 여건이 되는 수준 그리고 본인의 소득수준에 따라서 청약을 하고 3년 뒤에 입주를 하고 전매 제한이 8년 정도 됩니다, 둔촌주공도. 그러다 보니까 이렇게 실수요자 중심으로만 시장이 재편되는 게 시장에는 더 좋죠. 물론 둔촌주공 같은 경우는 시장에서 기대하는 것만큼 흥행이 되지는 않았습니다마는 그래도 5:1 수준에서, 순위 내에서 마감은 했기 때문에 크게 미계약이 많이 나오거나 그럴 것으로 보지는 않습니다.\\n\\n\\n\\n[앵커]\\n\\n그런데 청약시장도 얼어붙고 있다는 전망도 좀 있어서요. 수치를 또 보니까 전국의 청약경쟁률이 8년 만에 최저치래요. 이렇게 되면 선별 청약의 시기다라고 하지만 서울의 주요 지역에서도 혹시 미분양이 나오는 것 아니냐는 예측도 나와서요. 사실상 미분양이 많이 늘면 미분양 공포도 상당하잖아요. 어떤 문제점이 발생하겠습니까?\\n\\n\\n\\n[윤지해]\\n\\n미분양이 사실은 나오는 게 수요자한테 좋습니다. 왜냐하면 아까 설명드린 것처럼 묻지 마 청약을 하게 되면 인기가 없거나 분양가가 과도하게 비싸거나 혹은 입지적으로 떨어지는 곳들까지 다 마감이 되는 거잖아요.\\n\\n\\n\\n그러면 수요자가 생각하기에는 나도 청약해야 되지 않나, 이렇게 조급하게 선택을 하신다면 우리가 선별 청약시장이 됐을 때는 입지가 떨어지거나 가격이 과하게 비싸다면 수요자가 외면을 해야 되는 게 정상입니다. 그리고 우리가 일반적으로 생각했을 때 재고물량들이 조금 남아줘야 매수자 우위 시장이 형성이 되기 때문에 매수자가 협상력을 더 높일 수가 있습니다. 거기다가 건설사들이 본인이 가지고 있는 물건들이 팔리지 않게 되면 할인 분양이 됐든 중도금 무이자가 됐든 이런 이벤트성 수요자에게 맞춤형 조건들을 제시하기 시작하거든요.\\n\\n\\n\\n그 부분들이 수요자한테 반드시 나쁜 것은 아닌데 문제가 되는 부분들은 이런 겁니다. 미분양이 급격하게 늘어나게 되면 사실은 국내 같은 경우는 선분양 체제입니다. 그래서 공사비의 대다수를 일반분양자의 중도금과 계약금을 통해서 조달을 하거든요. 지금 조달금리가 굉장히 올라와 있지 않습니까? 그러니까 미분양이라는 얘기는 건설사의 자체 자금으로 공사비를 납입을 해야 된다는 의미이기 때문에 지금의 높아진 조달금리 상황에서 보유자금을 일정 수준 쓰게 된다면 파산에 이를 수도 있는 그런 국면으로 가게 되고 궁극적으로는 시장에 원하는 공급이 제때 이루어지지 않는 시기까지 오게 되는 부작용들이 유발된다고 보시면 될 것 같습니다.\\n\\n\\n\\n[앵커]\\n\\n긍정적으로 보자면 실수요자 입장에서는 좀 가격을 합리적으로 개선할 수 있는 반면에 또 이게 너무 위축이 되면 경기가 위험해질 수 있다, 이런 부분까지 짚어주신 것 같아요.\\n\\n\\n\\n[윤지해]\\n\\n맞습니다. 본인이 원하는 부분에서 공급이 제때 이루어지지 않게 되면 결국 초반에는 매수자 우위시장이어서 좋겠다고 생각하시겠습니다마는 나중에는 건설사들, 공급 주체잖아요. 그분들이 위축이 되면서 사실은 본인들이 원하는 수준의 물건들이 나오지 않게 되는 거죠.\\n\\n\\n\\n[앵커]\\n\\n세입자 입장에서 이제 보겠습니다. 임차권 등기명령을 신청한 건수가 작년보다 25.6%가 증가했다고 합니다. 일단 임차권 등기명령이 뭔지 간략하게 설명을 해 주신다면요?\\n\\n\\n\\n[윤지해]\\n\\n우리가 시장에서 보통 이사를 하려고 하면 임차인들은 임대인에게 보증금을 돌려줘라, 그렇게 요구를 하죠. 다만 그 보증금 수준이 시장 시세보다 굉장히 높다면 임대인들은 어디선가 자금을 끌어와야 됩니다. 그럼에도 불구하고 임차인에게 보증금을 제때 돌려주지 못하면 임차인 입장에서는 본인이 직장을 이전을 하거나 혹은 결혼을 하거나 혹은 병가를 내거나 이런 과정에서 이사를 꼭 해야 되는 케이스들이 있습니다.\\n\\n\\n\\n그러다 보면 임대인이 돈을 안 준다고 해도 주거 이전을 해야 되는 거죠. 그런 경우에 우리가 시장에서는 대항력과 우선변제권이라는 게 존재하는데 이 부분들에 대한 조건이 점유를 하거나 주민등록 이전을 해야 되는 조건들이 있습니다. 그런데 이 부분들이 이사를 하게 되면 모두 권리가 상실되잖아요. 그러다 보니까 법원에다가 집행권한을 받아서 임차권 등기명령을 받게 되면 이 대항력과 우선변제조건 등을 유지시켜주는 제도다 이렇게 보시면 되는데.\\n\\n\\n\\n[앵커]\\n\\n법적으로 세입자의 권한이나 지위를 좀 더 강화해준다, 이렇게 이해를 하면 되는 건가요?\\n\\n\\n\\n[윤지해]\\n\\n그러니까 주거이전의 자유를 조금 더 높여주는 정책입니다. 그러니까 등기부 상에 임차권 등기명령 제도가 있게 되면 임대인들은 새로운 임차인을 받을 수가 없거든요. 그래서 보증금을 제때 돌려줘야 되는 압박이 되는 거고요. 그리고 임차인 입장에서는 당장 이사를 가야 되니까 주민등록 이전을 하더라도 본인의 보증금을 받을 수 있는 조건들을 세워놓고 나가는 거다, 이런 제도로 보시면 될 것 같습니다.\\n\\n\\n\\n[앵커]\\n\\n그러면 조건들을 세운 거고 임차권 등기명령을 신청한다고 해서 보증금을 다 돌려받을 수 있다는 보장이 있는 건 아닌 거죠?\\n\\n\\n\\n[윤지해]\\n\\n그렇죠. 사인 간 거래이기 때문에 결국은 임대인이 적당한 요건, 그러니까 본인이 보증금 반환할 요건이 성립되지 않으면 본인이 대출을 받든 혹은 본인의 여유자금을 활용하든 자산을 팔아서든 그분들을 반드시 돌려줘야 되잖아요. 그분들을 안 돌려주게 되면 결국 법적인 책임을 지게 되는 거죠.\\n\\n\\n\\n[앵커]\\n\\n그렇군요. 이번에는 규제 얘기해 보겠습니다. 지금 저희가 너무 뜨거운 시장도 안 좋고 너무 위축된 시장도 모두에게 좋지 않다 이렇게 얘기를 풀어가고 있습니다. 정부의 목표는 너무 뚜렷한 것 같아요. 서민 중산층의 주거 안정, 집값 하향안정화 이 부분인데 이 부분을 유지하고 맞춰나가기 위해서는 어떤 규제 완화가 필요하겠습니까? 일단 정부는 규제를 완화한다고 하거든요.\\n\\n\\n\\n[윤지해]\\n\\n정부가 최근에 전방위적으로 경착륙을 막기 위해서 많은 규제들을 풀고 있습니다. 예를 들어서 재산세, 보유세와 관련된 부분들은 맞춰주고 있고요. 거기에 취득세 중과 부분도 손을 대고 있고요. 그리고 최근에 서울 지역과 몇몇 지역이 규제 지역으로 남아있습니다마는 그 규제 지역도 풀 예정에 있고 그리고 최근에는 다주택자와 관련된 대출규제도 풀 예정에 있고요.\\n\\n\\n\\n그리고 서민들 중심으로 저리의 모기지 금리를 적용하는 전세반환 형태의 대출들을 도입을 하고 있고요. 특례보금자리론 이런 형태입니다. 거기에 다주택자와 관련돼서 임대사업자 등록제도도 손을 보고 있어서 이렇게 전방위적인 규제들을 상당히 도입을 하고 있습니다마는 그중에서도 가장 주요한 부분들은 결국은 서민, 실소유자 중심의 대출규제, 이른바 DSR라고 했던 소득 대비 집값에 대한 부분들을 고려하는 부분들인데 이 부분들을 완화를 해 주면 여력이 되시는 자산이 되거나 소득이 좀 되시는 분들 중심으로 시장의 급매물들을 소화할 수 있는 여력들이 된다라고 볼 수가 있고요. 다만 핵심은 그 부분에 있지는 않고요. 시장을 안정적으로 관리를 하려면 결국 정부 차원에서 공급량을 늘리는 게 가장 중요합니다.\\n\\n\\n\\n[앵커]\\n\\n지금 시점에서 공급량을 늘리는 게 맞습니까?\\n\\n\\n\\n[윤지해]\\n\\n맞습니다. 왜냐하면 지금 미분양이 늘어나고 하는 부분들은 경착륙에 대한 우려감은 있습니다마는 사실은 이 부분들, 시장에 버퍼로서 작동하는 물건들은 굉장히 많은 게 좋습니다, 서민들 입장에서. 아까 말씀드린 것처럼 매수자 우위시장을 계속 형성할 수 있기 때문인데 거기에 보조하는 것들이 정부가 저가의 주택들을 원활하게 공급을 해 주는 겁니다. 왜 그러냐면 민간 주도의 공급 계획을 세우고 있습니다마는 사실 민간은 이익에 치중할 수밖에 없거든요. 그러다 보면 공공이 낮은 가격, 시세의 70%, 80% 되는 가격에 공급을 원활하게 해 주는 것이 장기적으로 시장을 안정시키는 데 가장 큰 도움이 되는 정책이다 볼 수 있습니다.\\n\\n\\n\\n[앵커]\\n\\n알겠습니다. 여기까지 조언 듣도록 하겠습니다. 지금까지 윤지해 부동산R114 수석연구원과 함께했습니다. 고맙습니다.\\n\\n[저작권자(c) YTN 무단전재 및 재배포 금지]'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_context_df.raw[185]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3db4f6d9-b274-4231-b1e8-1ac4c7b16996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before delete similar: 12 26\n",
      "(0, '* 아래 텍스트는 실제 방송 내용과 차이가 있을 수 있으니 보다 정확한 내용은 방송으로 확인하시기 바랍니다. ')\n",
      "(0, '* 아래 텍스트는 실제 방송 내용과 차이가 있을 수 있으니 보다 정확한 내용은 방송으로 확인하시기 바랍니다. ')\n",
      "1.0\n",
      "-------------------------------------------\n",
      "(0, '* 아래 텍스트는 실제 방송 내용과 차이가 있을 수 있으니 보다 정확한 내용은 방송으로 확인하시기 바랍니다. ')\n",
      "(0, '* 아래 텍스트는 실제 방송 내용과 차이가 있을 수 있으니 보다 정확한 내용은 방송으로 확인하시기 바랍니다. ')\n",
      "1.0\n",
      "-------------------------------------------\n",
      "(0, '* 아래 텍스트는 실제 방송 내용과 차이가 있을 수 있으니 보다 정확한 내용은 방송으로 확인하시기 바랍니다. ')\n",
      "(0, '* 아래 텍스트는 실제 방송 내용과 차이가 있을 수 있으니 보다 정확한 내용은 방송으로 확인하시기 바랍니다. ')\n",
      "1.0\n",
      "-------------------------------------------\n",
      "(0, '* 아래 텍스트는 실제 방송 내용과 차이가 있을 수 있으니 보다 정확한 내용은 방송으로 확인하시기 바랍니다. ')\n",
      "(0, '* 아래 텍스트는 실제 방송 내용과 차이가 있을 수 있으니 보다 정확한 내용은 방송으로 확인하시기 바랍니다. ')\n",
      "1.0\n",
      "-------------------------------------------\n",
      "(0, '* 아래 텍스트는 실제 방송 내용과 차이가 있을 수 있으니 보다 정확한 내용은 방송으로 확인하시기 바랍니다. ')\n",
      "(0, '* 아래 텍스트는 실제 방송 내용과 차이가 있을 수 있으니 보다 정확한 내용은 방송으로 확인하시기 바랍니다. ')\n",
      "1.0\n",
      "-------------------------------------------\n",
      "(0, '* 아래 텍스트는 실제 방송 내용과 차이가 있을 수 있으니 보다 정확한 내용은 방송으로 확인하시기 바랍니다. ')\n",
      "(0, '* 아래 텍스트는 실제 방송 내용과 차이가 있을 수 있으니 보다 정확한 내용은 방송으로 확인하시기 바랍니다. ')\n",
      "1.0\n",
      "-------------------------------------------\n",
      "(0, '* 아래 텍스트는 실제 방송 내용과 차이가 있을 수 있으니 보다 정확한 내용은 방송으로 확인하시기 바랍니다. ')\n",
      "(0, '* 아래 텍스트는 실제 방송 내용과 차이가 있을 수 있으니 보다 정확한 내용은 방송으로 확인하시기 바랍니다. ')\n",
      "1.0\n",
      "-------------------------------------------\n",
      "(0, '* 아래 텍스트는 실제 방송 내용과 차이가 있을 수 있으니 보다 정확한 내용은 방송으로 확인하시기 바랍니다. ')\n",
      "(0, '* 아래 텍스트는 실제 방송 내용과 차이가 있을 수 있으니 보다 정확한 내용은 방송으로 확인하시기 바랍니다. ')\n",
      "1.0\n",
      "-------------------------------------------\n",
      "(0, '* 아래 텍스트는 실제 방송 내용과 차이가 있을 수 있으니 보다 정확한 내용은 방송으로 확인하시기 바랍니다. ')\n",
      "(0, '* 아래 텍스트는 실제 방송 내용과 차이가 있을 수 있으니 보다 정확한 내용은 방송으로 확인하시기 바랍니다. ')\n",
      "1.0\n",
      "-------------------------------------------\n",
      "(0, '* 아래 텍스트는 실제 방송 내용과 차이가 있을 수 있으니 보다 정확한 내용은 방송으로 확인하시기 바랍니다. ')\n",
      "(0, '* 아래 텍스트는 실제 방송 내용과 차이가 있을 수 있으니 보다 정확한 내용은 방송으로 확인하시기 바랍니다. ')\n",
      "1.0\n",
      "-------------------------------------------\n",
      "(0, '* 아래 텍스트는 실제 방송 내용과 차이가 있을 수 있으니 보다 정확한 내용은 방송으로 확인하시기 바랍니다. ')\n",
      "(0, '* 아래 텍스트는 실제 방송 내용과 차이가 있을 수 있으니 보다 정확한 내용은 방송으로 확인하시기 바랍니다. ')\n",
      "1.0\n",
      "-------------------------------------------\n",
      "after delete similar: 1 26\n",
      "1\n",
      "len document df 1\n",
      "0 (0, '* 아래 텍스트는 실제 방송 내용과 차이가 있을 수 있으니 보다 정확한 내용은 방송으로 확인하시기 바랍니다. ')\n",
      "===================================================\n",
      "(0.0, ['* 아래 텍스트는 실제 방송 내용과 차이가 있을 수 있으니 보다 정확한 내용은 방송으로 확인하시기 바랍니다. '])\n",
      "3\n",
      "0 (19, ' HUG(주택도시보증공사)를 통해서 가입하는 경우들이 있었던 것 같은데, 그런데 대위변제 절차를 밟지 못하고 있다고 합니다. ')\n",
      "1 (12, ' 전달보다 0.6% 줄어드는 모습을 보였습니다. ')\n",
      "2 (8, ' 이 이후로 24년 만에 가장 높은 오름폭이었습니다. ')\n",
      "3 (6, ' 유동성 부분이 늘어나고 동원할 수 있는 자금의 크기가 커지니까 부동산 시장에 효과가 있겠지만 단기적으로는 어려운 부분이 있습니다. ')\n",
      "4 (6, ' 그게 지금 연 3.25%가 됐습니다. ')\n",
      "5 (7, ' 그래서 매매가 자체를 부풀려서 매매가에 꽉 찬 ‘깡통 전세’를 받아서 보증금을 가로채는 방식의 사기가 일어나게 되는 겁니다. ')\n",
      "6 (5, ' 예적금으로 들어가 있는 시중의 유동성, 한국은행 발표에 따르더라도 약 9월 기준으로 해서 우리나라에 3750조가 넘는 돈이 시중에 자금으로 잠겨져 있는 부분들도 있습니다. ')\n",
      "7 (7, ' 그래서 아직 입법이 완료가 되지는 않았는데 실제로 당해세 같은 경우에 확정일자 이전에 발생한 건 선순위 배당이 가능하도록 하지만 그 이후에 발생한 것까지 나라에서 먼저 세금을 추징해 간다, 이거는 좀 불합리하다는 판단에서 확정일자 받은 임대차보증금 이후에 발생한 당해세 같은 경우에는 임대차 보증금을 먼저 변제받을 수 있도록 지금 입법이 진행되고 있습니다. ')\n",
      "8 (12, ' 그러니까 전망에 대한 답을 달라고 하면 일단 상반기는 불가능하고, 내년 하반기 정도는 시장 동향을 보고 미분양이라든지 급격하게 내려간 급매 정도는 충분히 검토할 타이밍도 다가온다고 보여집니다. ')\n",
      "9 (16, ' 그런데도 4개 주택형에서는 결국 미달이 됐습니다. ')\n",
      "10 (8, ' 그만큼 전셋값과 집값이 동시에 올라서 놀랐다, 문제다 이랬는데 올해 들어서 상황이 좀 바뀌었습니다. ')\n",
      "11 (6, ' 시장은 올해 연말이니까 한번 간단하게 정리하고 예측하고 전략을 짜는 데 도움이 됐으면 좋겠습니다. ')\n",
      "12 (20, ' 내년에는 기준금리 인하가 없을 거라고 못 박아야 이야기를 하기도 했습니다. ')\n",
      "13 (16, ' 수요 위축의 원인은 금리인상과 대출규제, 앞서 말씀드린 부분들인데 사실 지금의 거래량이 정상은 아닙니다. ')\n",
      "14 (8, ' 손뼉이 마주쳐야 소리가 나는 법인데 지금은 일단 한쪽만 존재하는 상황이다. ')\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python3.6/lib/python3.6/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  next_indices = next_tokens // vocab_size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2407 sec\n",
      "HUG(주택도시보증공사)를 통해서 가입하는 경우들이 있었던 것 같은데, 대위변제 절차를 밟지 못하고 있다고 한다.\n",
      "2 1\n",
      "-----------------------------------------------\n",
      "0.2292 sec\n",
      "내년 상반 정도는 시장 동향을 보고 미분양이라든지 급격하게 내려간 급매 정도는 충분히 검토할 타이밍도 다가온다고 보여집니다.\n",
      "2 2\n",
      "-----------------------------------------------\n",
      "(10.4, ['HUG(주택도시보증공사)를 통해서 가입하는 경우들이 있었던 것 같은데, 대위변제 절차를 밟지 못하고 있다고 한다.', '내년 상반 정도는 시장 동향을 보고 미분양이라든지 급격하게 내려간 급매 정도는 충분히 검토할 타이밍도 다가온다고 보여집니다.'])\n",
      "***************************************************\n",
      "0 (10, ' 예를 들어서 세종시 같은 경우는 2020년에만 40~50%가 올랐습니다. ')\n",
      "1 (10, ' 한마디로 시장이 롤러코스터를 타고 있다. ')\n",
      "2 (10, ' 가장 큰 영향을 미친 건 역시 물가라고 볼 수 있겠습니다. ')\n",
      "3 (10, ' ◆ 조세영: 그렇습니다. ')\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python3.6/lib/python3.6/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  next_indices = next_tokens // vocab_size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2401 sec\n",
      "카카오 한마디로 시장이 롤러코스터를 타고 롤러코스터를 타고 있는 가운데 가장 큰 영향을 미친 건 역시 물가라고 볼 수 있겠다.\n",
      "1 1\n",
      "-----------------------------------------------\n",
      "(10.0, ['카카오 한마디로 시장이 롤러코스터를 타고 롤러코스터를 타고 있는 가운데 가장 큰 영향을 미친 건 역시 물가라고 볼 수 있겠다.'])\n",
      "***************************************************\n",
      "0 (14, ' 통지해야 할 집주인이 사라진 상황이라고 보시면 되겠습니다. ')\n",
      "1 (7, ' 그래서 이게 역대급이라는 부분의 기록은 이미 세웠지만, 앞으로도 이 부분과 관련해서는 시장이 굉장히 많은 부담으로 작용을 할 것으로 예상이 됩니다. ')\n",
      "2 (8, ' 그래서 섣불리 아직 바닥 가격이라든가 이른 반등이나 이런 것들을 예상하기가 쉽지 않은 상황이라고 보실 수 있습니다. ')\n",
      "3 (14, ' 그 경우에 있어서 84제곱미터의 경우에는 6억 원 이상 하락한 것으로 나타나고 있습니다. ')\n",
      "4 (14, ' 이 얘기는 다시 얘기하자면 앞으로는 경기침체가 정말 본격화될 수 있다는 뜻입니다. ')\n",
      "5 (15, ' 불분명한 상황이라고 보시면 되겠습니다. ')\n",
      "6 (5, ' 기준금리의 첫 번째 목표는 물가고 두 번째로는 물가 안정 같은 것들을 이야기하지만 세상에 모든 경제현상에서 기준금리에서 자유로운 경제현상은 없다고 보시면 되겠습니다. ')\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python3.6/lib/python3.6/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  next_indices = next_tokens // vocab_size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2194 sec\n",
      "통지해야 할 집주인이 사라진 상황이라고 보시면 되겠다.\n",
      "1 1\n",
      "-----------------------------------------------\n",
      "(11.0, ['통지해야 할 집주인이 사라진 상황이라고 보시면 되겠다.'])\n",
      "***************************************************\n",
      ".................293<570 get additional topk 15 ...............\n",
      "before delete similar: 13\n",
      "after delete similar: 13\n",
      "1\n",
      "0 (8, ' ◆ 조세영: 네, 그렇습니다. ')\n",
      "1 (13, ' 개인적인 의견하고 개별적인 여론조사를 통한다면 내년 상반기에 적극적으로 내 집 장만이라든지, 투자 수요가 몰리기에는 좀 한계가 있다고 보여집니다. ')\n",
      "2 (8, ' 그 경우에는 매매 가격과 전세가가 거의 동일하거나 비슷한 상황입니다. ')\n",
      "3 (12, ' 그래서 이런 상황들을 보자면 내년 상반기 정도 하락 수준을 판단을 하려면 급매물 중심으로 거래량이 살아나면서 두 자릿수 정도의 하락까지는 좀 더 추가적으로 진행이 돼야 수요자들이 오른 수준에 비해서 적정하게 가격이 떨어지고 있다라고 체감을 하실 수 있지 않을까 싶습니다. ')\n",
      "4 (6, ' 그런데 그 부분들이 여러 가지 대출이나 시장에 참여하는 많은 분들의 마음마저 얼어붙게 만드는, 그런 것으로 인해서 서울이나 지방 가릴 것 없이 전국의 아파트값이 2003년에 한국부동산원이 집값 조사를 한 이후로 최대의 하락폭을 기록을 한 것 같습니다. ')\n",
      "5 (24, ' 그런데 여기에서 내년 말 최종 금리 수준을 5에서 5.25%로 제시했습니다. ')\n",
      "6 (6, ' 당연히 자산시장에도 큰 영향을 미치고 부동산 시장에도 많은 영향을 미칩니다. ')\n",
      "7 (8, ' 무시무시한 속도로 기준금리를 높였습니다. ')\n",
      "8 (8, ' 상속을 다 거부하게 되면 그러면 재산관리인을 선임해서 법원에서 진행을 하게 됩니다. ')\n",
      "9 (10, ' 역시 두 번째 이유는 미국을 이야기하지 않을 수가 없습니다. ')\n",
      "10 (7, ' 우선 네 가지 특성이 있습니다. ')\n",
      "11 (17, ' 이게 큰 원인이었다고 볼 수 있습니다. ')\n",
      "12 (7, ' 이게 한국부동산원이 조사한 건데 시세 조사한 이후로 가장 큰 하락폭이라고 합니다. ')\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python3.6/lib/python3.6/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  next_indices = next_tokens // vocab_size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3942 sec\n",
      "조세영: 내년 상반기 정도 하락 수준을 판단 하려면 급매물 중심으로 거래량이 살아나면서 두 자릿수 정도의 하락까지는 좀 더 추가적으로 진행이 돼야 수요자들이 오른 수준에 비해서 적정하게 가격이 떨어지고 있다라고 체감을 하실 수 있지 않을까 싶다.\n",
      "2 1\n",
      "-----------------------------------------------\n",
      "0.3938 sec\n",
      "deleted : 5 | 지은이 두 번째 이유는 미국을 이야기하지 않을 수가\n",
      "ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
      "지은 우선 우선 네 가지 특성이 큰 원인이었다고 볼 수 있는데,  지은이 재산관리인을 선임해서 법원에서 진행을 하게 되는데  지은이 두 번째 이유는 미국을 이야기하지 않을 수가 없으며,  지은이 두 번째 이유는 미국을 이야기하지 않을 수가 없습니다.\n",
      "ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
      "지은 우선 우선 네 가지 특성이 큰 원인이었다고 볼 수 있는데, 지은이 재산관리인을 선임해서 법원에서 진행을 하게 되는데 지은이 두 번째 이유는 미국을 이야기하지 않을 수가 없으며, 없습니다.\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "지은 우선 우선 네 가지 특성이 큰 원인이었다고 볼 수 있는데, 지은이 재산관리인을 선임해서 법원에서 진행을 하게 되는데 지은이 두 번째 이유는 미국을 이야기하지 않을 수가 없으며, 없습니다.\n",
      "2 2\n",
      "-----------------------------------------------\n",
      "(10.307692307692308, ['조세영: 내년 상반기 정도 하락 수준을 판단 하려면 급매물 중심으로 거래량이 살아나면서 두 자릿수 정도의 하락까지는 좀 더 추가적으로 진행이 돼야 수요자들이 오른 수준에 비해서 적정하게 가격이 떨어지고 있다라고 체감을 하실 수 있지 않을까 싶다.', '지은 우선 우선 네 가지 특성이 큰 원인이었다고 볼 수 있는데, 지은이 재산관리인을 선임해서 법원에서 진행을 하게 되는데 지은이 두 번째 이유는 미국을 이야기하지 않을 수가 없으며, 없습니다.'])\n",
      "***************************************************\n",
      ".................537<570 get additional topk 22 ...............\n",
      "before delete similar: 16\n",
      "after delete similar: 16\n",
      "2\n",
      "0 (12, ' ◆ 조세영: 그렇습니다. ')\n",
      "1 (10, ' 그래서 19년 만에 가장 큰 폭으로 가격이 하락했습니다. ')\n",
      "2 (23, ' 그러니까 FOMC에 참석했던 사람들이 앞으로 기준금리가 어떻게 될 거라는 걸 점을 찍어서 시장에 알려주는 일종의 포워드 가이던스라고 볼 수 있습니다. ')\n",
      "3 (7, ' 그러니까 여기에서 의문을 표하는 목소리도 있습니다. ')\n",
      "4 (19, ' 그렇기 때문에 기준금리 인상이 조만간 마무리될 거다라는 데는 큰 이견은 없는 상태입니다. ')\n",
      "5 (8, ' 또 경제 성장하고 주택 가격하고 상당 부분 좀 유심히 볼 필요가 있지 않나, 아까도 얘기한 것처럼 1%대의 경제 성장이라면 법인도 그렇고 개인도 그렇고 내 집 장만하는 데 굉장히 어려움을 겪을 것으로 보여집니다. ')\n",
      "6 (10, ' 실수요자에 대한 규제 완화 방침도 곧 밝힐 예정입니다. ')\n",
      "7 (7, ' 아무래도 첫 번째 가장 큰 이유는 역시 소비자물가상승률로 봐야 될 것 같습니다.가 나왔는데 올해 소비자물가 상승률이 1년 동안 5.1% 오른 것으로 나타났습니다. ')\n",
      "8 (13, ' 예전에 가락 주공아파트를 재건축한 단지라고 볼 수 있습니다. ')\n",
      "9 (18, ' 우선 세금도 세 가지 종류가 있습니다. ')\n",
      "10 (8, ' 원인은 크게 세 가지입니다. ')\n",
      "11 (16, ' 이미 말씀하셨던 것처럼 예상이 됐던 빅스텝이었는데 지금 미 증시 같은 경우에는 3거래일 연속 떨어지면서 요동치고 있습니다. ')\n",
      "12 (8, ' 전국의 주택 매매값이 1.37% 하락했습니다. ')\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python3.6/lib/python3.6/site-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  next_indices = next_tokens // vocab_size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1830 sec\n",
      "19년 만에 가장 큰 폭으로 가격이 하락한 이유는 소비자물가상승률로 소비자물가상승률로 봐야 될 것 같다.\n",
      "2 1\n",
      "-----------------------------------------------\n",
      "1.4687 sec\n",
      "deleted : 0 | 가락 주공아파트를 재건축한 단지라고 볼 수 있는 가락 주공아파트를 재건축한 단지라고 볼 수 있는 가락 주공아파트를 재건축한 단지라고 볼 수 있는 가락 주공아파트를 재건축한 단지라고 볼 수 있는 가락 주공아파트를 재건축한 단지라고 볼 수 있는\n",
      "ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
      "가락 주공아파트를 재건축한 단지라고 볼 수 있는 가락 주공아파트를 재건축한 단지라고 볼 수 있는 가락 주공아파트를 재건축한 단지라고 볼 수 있는 가락 주공아파트를 재건축한 단지라고 볼 수 있는 가락 주공아파트를 재건축한 단지라고 볼 수 있는 가락 주공아파트를 재건축한 단지라고 볼 수 있는 가락 주공아파트를 재건축한 단지라고 볼 수 있는 가락 주공아파트를 재건축한 단지라고 볼 수 있는 가락 주공아파트를 재건축한 단지라고 볼 수 있는 가락 주공아파트를 재건축한 단지라고 볼 수 있는 가락 주공아파트를 재건축한 단지이다.\n",
      "ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
      "가락 주공아파트를 재건축한 단지라고 볼 수 있는 가락 주공아파트를 재건축한 단지라고 볼 수 있는 가락 주공아파트를 재건축한 단지라고 볼 수 있는 가락 주공아파트를 재건축한 단지라고 볼 수 있는 가락 주공아파트를 재건축한 단지라고 볼 수 있는 가락 주공아파트를 재건축한 단지이다.\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "deleted : 12 | 가락 주공아파트를 재건축한 단지라고 볼 수 있는 가락 주공아파트를 재건축한 단지라고 볼 수 있는 가락 주공아파트를 재건축한\n",
      "ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
      "가락 주공아파트를 재건축한 단지라고 볼 수 있는 가락 주공아파트를 재건축한 단지라고 볼 수 있는 가락 주공아파트를 재건축한 단지라고 볼 수 있는 가락 주공아파트를 재건축한 단지라고 볼 수 있는 가락 주공아파트를 재건축한 단지라고 볼 수 있는 가락 주공아파트를 재건축한 단지이다.\n",
      "ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
      "가락 주공아파트를 재건축한 단지라고 볼 수 있는 가락 주공아파트를 재건축한 단지라고 볼 수 있는 가락 주공아파트를 재건축한 단지라고 볼 수 있는 단지이다.\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "deleted : 0 | 가락 주공아파트를 재건축한 단지라고 볼 수 있는\n",
      "ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
      "가락 주공아파트를 재건축한 단지라고 볼 수 있는 가락 주공아파트를 재건축한 단지라고 볼 수 있는 가락 주공아파트를 재건축한 단지라고 볼 수 있는 단지이다.\n",
      "ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
      "가락 주공아파트를 재건축한 단지라고 볼 수 있는 가락 주공아파트를 재건축한 단지라고 볼 수 있는 단지이다.\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "deleted : 0 | 가락 주공아파트를 재건축한 단지라고 볼 수 있는\n",
      "ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
      "가락 주공아파트를 재건축한 단지라고 볼 수 있는 가락 주공아파트를 재건축한 단지라고 볼 수 있는 단지이다.\n",
      "ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
      "가락 주공아파트를 재건축한 단지라고 볼 수 있는 단지이다.\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "가락 주공아파트를 재건축한 단지라고 볼 수 있는 단지이다.\n",
      "2 2\n",
      "-----------------------------------------------\n",
      "(12.23076923076923, ['19년 만에 가장 큰 폭으로 가격이 하락한 이유는 소비자물가상승률로 소비자물가상승률로 봐야 될 것 같다.', '가락 주공아파트를 재건축한 단지라고 볼 수 있는 단지이다.'])\n",
      "***************************************************\n",
      "0 (6, ' 그렇게 되면 경매나 공매에 부쳐져서 낙찰을 받는다. ')\n",
      "1 (6, ' 더 밝혀질 예정으로 보여서 규모가 계속 커지고 있는 상황입니다. ')\n",
      "2 (6, ' 서울 강남권 주요 아파트 지역처럼 투자자들이 선호하는 지역 입지가 우수해서 낙폭이 상대적으로 크지 않았던 곳들도 최근에 하락폭이 커지고 있는 상황입니다. ')\n",
      "---------------------------------------------------\n",
      "(6.0, [' 서울 강남권 주요 아파트 지역처럼 투자자들이 선호하는 지역 입지가 우수해서 낙폭이 상대적으로 크지 않았던 곳들도 최근에 하락폭이 커지고 있는 상황입니다. '])\n",
      "***************************************************\n",
      "======================= Final result =========================\n",
      "* 아래 텍스트는 실제 방송 내용과 차이가 있을 수 있으니 보다 정확한 내용은 방송으로 확인하시기 바랍니다.\n",
      "\n",
      "서울 강남권 주요 아파트 지역처럼 투자자들이 선호하는 지역 입지가 우수해서 낙폭이 상대적으로 크지 않았던 곳들도 최근에 하락폭이 커지고 있는 상황입니다.\n",
      "\n",
      "카카오 한마디로 시장이 롤러코스터를 타고 롤러코스터를 타고 있는 가운데 가장 큰 영향을 미친 건 역시 물가라고 볼 수 있겠다.\n",
      "\n",
      "조세영: 내년 상반기 정도 하락 수준을 판단 하려면 급매물 중심으로 거래량이 살아나면서 두 자릿수 정도의 하락까지는 좀 더 추가적으로 진행이 돼야 수요자들이 오른 수준에 비해서 적정하게 가격이 떨어지고 있다라고 체감을 하실 수 있지 않을까 싶다. 지은 우선 우선 네 가지 특성이 큰 원인이었다고 볼 수 있는데, 지은이 재산관리인을 선임해서 법원에서 진행을 하게 되는데 지은이 두 번째 이유는 미국을 이야기하지 않을 수가 없으며, 없습니다.\n",
      "\n",
      "HUG(주택도시보증공사)를 통해서 가입하는 경우들이 있었던 것 같은데, 대위변제 절차를 밟지 못하고 있다고 한다. 내년 상반 정도는 시장 동향을 보고 미분양이라든지 급격하게 내려간 급매 정도는 충분히 검토할 타이밍도 다가온다고 보여집니다.\n",
      "\n",
      "통지해야 할 집주인이 사라진 상황이라고 보시면 되겠다.\n",
      "\n",
      "19년 만에 가장 큰 폭으로 가격이 하락한 이유는 소비자물가상승률로 소비자물가상승률로 봐야 될 것 같다. 가락 주공아파트를 재건축한 단지라고 볼 수 있는 단지이다.\n",
      "\n",
      "\n",
      "3.5669 sec\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "first_sentences, topk_sentences = get_first_topk_sentences(topic_context_df, 15)\n",
    "\n",
    "sum_result1 = summarize_first_sentences(first_sentences, tokenizer, model)\n",
    "sum_result2 = sum_result1 + summarize_topk_sentences(topk_sentences, tokenizer, model)\n",
    "\n",
    "prev_topk = 15\n",
    "\n",
    "threshold = min(570, len(topic_context_df) * 100)\n",
    "while(calc_len(sum_result2) < threshold): # 너무 짧을 때\n",
    "    print(f'.................{calc_len(sum_result2)}<{threshold} get additional topk {prev_topk} ...............')\n",
    "    topk_sentences2 = get_additional_topk_sentences(topic_context_df, prev_topk, 7)\n",
    "    if(len(topk_sentences2) > 0):\n",
    "        sum_result3 = summarize_topk_sentences(topk_sentences2, tokenizer, model)\n",
    "        sum_result2 = sorted(sum_result3 + sum_result2, key=lambda x:x[0])\n",
    "    prev_topk += 7\n",
    "\n",
    "final_result = ''\n",
    "for v in sum_result2:\n",
    "    paragraph = [s.strip() for s in v[1]]\n",
    "    new_paragraph = [s if('.' in s[-2: ]) else s+'. ' for s in paragraph]\n",
    "    final_result += ' '.join(new_paragraph)+'\\n\\n'\n",
    "\n",
    "\n",
    "print('======================= Final result =========================')\n",
    "print(final_result)\n",
    "print(f\"{time.time()-start:.4f} sec\")\n",
    "print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "f = open(f'./bertopic(mpnet)/{name}_summary_{i}.txt', 'w')\n",
    "f.write(final_result)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2c1caa-ced7-4cfc-bae8-263a213f5982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
