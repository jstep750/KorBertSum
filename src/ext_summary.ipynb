{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9aaff8e-6588-455f-8a07-9c0031dbf81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python3.6/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Main training workflow\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import signal\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertConfig\n",
    "\n",
    "\n",
    "import distributed\n",
    "from models import data_loader, model_builder\n",
    "from models.data_loader import load_dataset\n",
    "from models.model_builder import Summarizer\n",
    "from tensorboardX import SummaryWriter\n",
    "from models.reporter import ReportMgr\n",
    "from models.stats import Statistics\n",
    "from others.logging import logger\n",
    "# from models.trainer import build_trainer\n",
    "# build_trainer의 dependency package pyrouge.utils가 import되지 않아 직접 셀에 삽입\n",
    "from others.logging import logger, init_logger\n",
    "import easydict\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    \"encoder\":'classifier',\n",
    "    \"mode\":'summary',\n",
    "    \"bert_data_path\":'../bert_sample/korean',\n",
    "    \"model_path\":'../models/bert_classifier',\n",
    "    \"bert_model\":'../001_bert_morp_pytorch',\n",
    "    \"result_path\":'../results/korean',\n",
    "    \"temp_dir\":'.',\n",
    "    \"bert_config_path\":'../001_bert_morp_pytorch/bert_config.json',\n",
    "    \"batch_size\":1000,\n",
    "    \"use_interval\":True,\n",
    "    \"hidden_size\":128,\n",
    "    \"ff_size\":512,\n",
    "    \"heads\":4,\n",
    "    \"inter_layers\":2,\n",
    "    \"rnn_size\":512,\n",
    "    \"param_init\":0,\n",
    "    \"param_init_glorot\":True,\n",
    "    \"dropout\":0.1,\n",
    "    \"optim\":'adam',\n",
    "    \"lr\":2e-3,\n",
    "    \"report_every\":1,\n",
    "    \"save_checkpoint_steps\":5,\n",
    "    \"block_trigram\":True,\n",
    "    \"recall_eval\":False,\n",
    "    \n",
    "    \"accum_count\":1,\n",
    "    \"world_size\":1,\n",
    "    \"visible_gpus\":'0',\n",
    "    \"gpu_ranks\":'0',\n",
    "    \"log_file\":'../logs/bert_classifier',\n",
    "    \"test_from\":'../models/bert_classifier2/model_step_35000.pt'\n",
    "})\n",
    "\n",
    "\n",
    "def build_trainer(args, device_id, model,\n",
    "                  optim):\n",
    "    \"\"\"\n",
    "    Simplify `Trainer` creation based on user `opt`s*\n",
    "    Args:\n",
    "        opt (:obj:`Namespace`): user options (usually from argument parsing)\n",
    "        model (:obj:`onmt.models.NMTModel`): the model to train\n",
    "        fields (dict): dict of fields\n",
    "        optim (:obj:`onmt.utils.Optimizer`): optimizer used during training\n",
    "        data_type (str): string describing the type of data\n",
    "            e.g. \"text\", \"img\", \"audio\"\n",
    "        model_saver(:obj:`onmt.models.ModelSaverBase`): the utility object\n",
    "            used to save the model\n",
    "    \"\"\"\n",
    "    device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
    "\n",
    "\n",
    "    grad_accum_count = args.accum_count\n",
    "    n_gpu = args.world_size\n",
    "\n",
    "    if device_id >= 0:\n",
    "        gpu_rank = int(args.gpu_ranks[device_id])\n",
    "    else:\n",
    "        gpu_rank = 0\n",
    "        n_gpu = 0\n",
    "\n",
    "    #print('gpu_rank %d' % gpu_rank)\n",
    "\n",
    "    tensorboard_log_dir = args.model_path\n",
    "\n",
    "    writer = SummaryWriter(tensorboard_log_dir, comment=\"Unmt\")\n",
    "\n",
    "    report_manager = ReportMgr(args.report_every, start_time=-1, tensorboard_writer=writer)\n",
    "\n",
    "    trainer = Trainer(args, model, optim, grad_accum_count, n_gpu, gpu_rank, report_manager)\n",
    "\n",
    "    # print(tr)\n",
    "    if (model):\n",
    "        n_params = _tally_parameters(model)\n",
    "        #logger.info('* number of parameters: %d' % n_params)\n",
    "\n",
    "    return trainer\n",
    "\n",
    "class Trainer(object):\n",
    "    \"\"\"\n",
    "    Class that controls the training process.\n",
    "\n",
    "    Args:\n",
    "            model(:py:class:`onmt.models.model.NMTModel`): translation model\n",
    "                to train\n",
    "            train_loss(:obj:`onmt.utils.loss.LossComputeBase`):\n",
    "               training loss computation\n",
    "            valid_loss(:obj:`onmt.utils.loss.LossComputeBase`):\n",
    "               training loss computation\n",
    "            optim(:obj:`onmt.utils.optimizers.Optimizer`):\n",
    "               the optimizer responsible for update\n",
    "            trunc_size(int): length of truncated back propagation through time\n",
    "            shard_size(int): compute loss in shards of this size for efficiency\n",
    "            data_type(string): type of the source input: [text|img|audio]\n",
    "            norm_method(string): normalization methods: [sents|tokens]\n",
    "            grad_accum_count(int): accumulate gradients this many times.\n",
    "            report_manager(:obj:`onmt.utils.ReportMgrBase`):\n",
    "                the object that creates reports, or None\n",
    "            model_saver(:obj:`onmt.models.ModelSaverBase`): the saver is\n",
    "                used to save a checkpoint.\n",
    "                Thus nothing will be saved if this parameter is None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,  args, model,  optim,\n",
    "                  grad_accum_count=1, n_gpu=1, gpu_rank=1,\n",
    "                  report_manager=None):\n",
    "        # Basic attributes.\n",
    "        self.args = args\n",
    "        self.save_checkpoint_steps = args.save_checkpoint_steps\n",
    "        self.model = model\n",
    "        self.optim = optim\n",
    "        self.grad_accum_count = grad_accum_count\n",
    "        self.n_gpu = n_gpu\n",
    "        self.gpu_rank = gpu_rank\n",
    "        self.report_manager = report_manager\n",
    "\n",
    "        self.loss = torch.nn.BCELoss(reduction='none')\n",
    "        assert grad_accum_count > 0\n",
    "        # Set model in training mode.\n",
    "        if (model):\n",
    "            self.model.train()\n",
    "\n",
    "    def summary(self, test_iter, step, cal_lead=False, cal_oracle=False):\n",
    "        \"\"\" Validate model.\n",
    "            valid_iter: validate data iterator\n",
    "        Returns:\n",
    "            :obj:`nmt.Statistics`: validation loss statistics\n",
    "        \"\"\"\n",
    "        # Set model in validating mode.\n",
    "        def _get_ngrams(n, text):\n",
    "            ngram_set = set()\n",
    "            text_length = len(text)\n",
    "            max_index_ngram_start = text_length - n\n",
    "            for i in range(max_index_ngram_start + 1):\n",
    "                ngram_set.add(tuple(text[i:i + n]))\n",
    "            return ngram_set\n",
    "\n",
    "        def _block_tri(c, p):\n",
    "            tri_c = _get_ngrams(3, c.split())\n",
    "            for s in p:\n",
    "                tri_s = _get_ngrams(3, s.split())\n",
    "                if len(tri_c.intersection(tri_s))>0:\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        if (not cal_lead and not cal_oracle):\n",
    "            self.model.eval()\n",
    "        stats = Statistics()\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_iter:\n",
    "                src = batch.src\n",
    "                labels = batch.labels\n",
    "                segs = batch.segs\n",
    "                clss = batch.clss\n",
    "                mask = batch.mask\n",
    "                mask_cls = batch.mask_cls\n",
    "\n",
    "\n",
    "                gold = []\n",
    "                pred = []\n",
    "\n",
    "                if (cal_lead):\n",
    "                    selected_ids = [list(range(batch.clss.size(1)))] * batch.batch_size\n",
    "                elif (cal_oracle):\n",
    "                    selected_ids = [[j for j in range(batch.clss.size(1)) if labels[i][j] == 1] for i in\n",
    "                                    range(batch.batch_size)]\n",
    "                else:\n",
    "                    sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
    "\n",
    "                    # loss = self.loss(sent_scores, labels.float())\n",
    "                    # loss = (loss * mask.float()).sum()\n",
    "                    # batch_stats = Statistics(float(loss.cpu().data.numpy()), len(labels))\n",
    "                    # stats.update(batch_stats)\n",
    "\n",
    "                    sent_scores = sent_scores + mask.float()\n",
    "                    sent_scores = sent_scores.cpu().data.numpy()\n",
    "                    selected_ids = np.argsort(-sent_scores, 1)\n",
    "                # selected_ids = np.sort(selected_ids,1)\n",
    "                \n",
    "\n",
    "        return selected_ids\n",
    "\n",
    "\n",
    "    def _gradient_accumulation(self, true_batchs, normalization, total_stats,\n",
    "                               report_stats):\n",
    "        if self.grad_accum_count > 1:\n",
    "            self.model.zero_grad()\n",
    "\n",
    "        for batch in true_batchs:\n",
    "            if self.grad_accum_count == 1:\n",
    "                self.model.zero_grad()\n",
    "\n",
    "            src = batch.src\n",
    "            labels = batch.labels\n",
    "            segs = batch.segs\n",
    "            clss = batch.clss\n",
    "            mask = batch.mask\n",
    "            mask_cls = batch.mask_cls\n",
    "\n",
    "            sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
    "\n",
    "            loss = self.loss(sent_scores, labels.float())\n",
    "            loss = (loss*mask.float()).sum()\n",
    "            (loss/loss.numel()).backward()\n",
    "            # loss.div(float(normalization)).backward()\n",
    "\n",
    "            batch_stats = Statistics(float(loss.cpu().data.numpy()), normalization)\n",
    "\n",
    "\n",
    "            total_stats.update(batch_stats)\n",
    "            report_stats.update(batch_stats)\n",
    "\n",
    "            # 4. Update the parameters and statistics.\n",
    "            if self.grad_accum_count == 1:\n",
    "                # Multi GPU gradient gather\n",
    "                if self.n_gpu > 1:\n",
    "                    grads = [p.grad.data for p in self.model.parameters()\n",
    "                             if p.requires_grad\n",
    "                             and p.grad is not None]\n",
    "                    distributed.all_reduce_and_rescale_tensors(\n",
    "                        grads, float(1))\n",
    "                self.optim.step()\n",
    "\n",
    "        # in case of multi step gradient accumulation,\n",
    "        # update only after accum batches\n",
    "        if self.grad_accum_count > 1:\n",
    "            if self.n_gpu > 1:\n",
    "                grads = [p.grad.data for p in self.model.parameters()\n",
    "                         if p.requires_grad\n",
    "                         and p.grad is not None]\n",
    "                distributed.all_reduce_and_rescale_tensors(\n",
    "                    grads, float(1))\n",
    "            self.optim.step()\n",
    "\n",
    "    def _save(self, step):\n",
    "        real_model = self.model\n",
    "        # real_generator = (self.generator.module\n",
    "        #                   if isinstance(self.generator, torch.nn.DataParallel)\n",
    "        #                   else self.generator)\n",
    "\n",
    "        model_state_dict = real_model.state_dict()\n",
    "        # generator_state_dict = real_generator.state_dict()\n",
    "        checkpoint = {\n",
    "            'model': model_state_dict,\n",
    "            # 'generator': generator_state_dict,\n",
    "            'opt': self.args,\n",
    "            'optim': self.optim,\n",
    "        }\n",
    "        checkpoint_path = os.path.join(self.args.model_path, 'model_step_%d.pt' % step)\n",
    "        #logger.info(\"Saving checkpoint %s\" % checkpoint_path)\n",
    "        # checkpoint_path = '%s_step_%d.pt' % (FLAGS.model_path, step)\n",
    "        if (not os.path.exists(checkpoint_path)):\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            return checkpoint, checkpoint_path\n",
    "\n",
    "    def _start_report_manager(self, start_time=None):\n",
    "        \"\"\"\n",
    "        Simple function to start report manager (if any)\n",
    "        \"\"\"\n",
    "        if self.report_manager is not None:\n",
    "            if start_time is None:\n",
    "                self.report_manager.start()\n",
    "            else:\n",
    "                self.report_manager.start_time = start_time\n",
    "\n",
    "    def _maybe_gather_stats(self, stat):\n",
    "        \"\"\"\n",
    "        Gather statistics in multi-processes cases\n",
    "\n",
    "        Args:\n",
    "            stat(:obj:onmt.utils.Statistics): a Statistics object to gather\n",
    "                or None (it returns None in this case)\n",
    "\n",
    "        Returns:\n",
    "            stat: the updated (or unchanged) stat object\n",
    "        \"\"\"\n",
    "        if stat is not None and self.n_gpu > 1:\n",
    "            return Statistics.all_gather_stats(stat)\n",
    "        return stat\n",
    "\n",
    "    def _maybe_report_training(self, step, num_steps, learning_rate,\n",
    "                               report_stats):\n",
    "        \"\"\"\n",
    "        Simple function to report training stats (if report_manager is set)\n",
    "        see `onmt.utils.ReportManagerBase.report_training` for doc\n",
    "        \"\"\"\n",
    "        if self.report_manager is not None:\n",
    "            return self.report_manager.report_training(\n",
    "                step, num_steps, learning_rate, report_stats,\n",
    "                multigpu=self.n_gpu > 1)\n",
    "\n",
    "    def _report_step(self, learning_rate, step, train_stats=None,\n",
    "                     valid_stats=None):\n",
    "        \"\"\"\n",
    "        Simple function to report stats (if report_manager is set)\n",
    "        see `onmt.utils.ReportManagerBase.report_step` for doc\n",
    "        \"\"\"\n",
    "        if self.report_manager is not None:\n",
    "            return self.report_manager.report_step(\n",
    "                learning_rate, step, train_stats=train_stats,\n",
    "                valid_stats=valid_stats)\n",
    "\n",
    "    def _maybe_save(self, step):\n",
    "        \"\"\"\n",
    "        Save the model if a model saver is set\n",
    "        \"\"\"\n",
    "        if self.model_saver is not None:\n",
    "            self.model_saver.maybe_save(step)\n",
    "\n",
    "            \n",
    "def summary(args, b_list, device_id, pt, step, model):\n",
    "    device_id = 0\n",
    "    device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
    "    if (pt != ''):\n",
    "        test_from = pt\n",
    "    else:\n",
    "        test_from = args.test_from\n",
    "    \n",
    "    opt = vars(checkpoint['opt'])\n",
    "    for k in opt.keys():\n",
    "        if (k in model_flags):\n",
    "            setattr(args, k, opt[k])\n",
    "    #print(args)\n",
    "\n",
    "    #model.load_cp(checkpoint)\n",
    "    #model.eval()\n",
    "\n",
    "    test_iter =data_loader.Dataloader(args, _lazy_dataset_loader(b_list),\n",
    "                                  args.batch_size, device,\n",
    "                                  shuffle=False, is_test=True)\n",
    "    trainer = build_trainer(args, device_id, model, None)\n",
    "    result = trainer.summary(test_iter,step)\n",
    "    return result\n",
    "\n",
    "def _tally_parameters(model):\n",
    "    n_params = sum([p.nelement() for p in model.parameters()])\n",
    "    return n_params\n",
    "\n",
    "args.gpu_ranks = [int(i) for i in args.gpu_ranks.split(',')]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.visible_gpus\n",
    "\n",
    "init_logger(args.log_file)\n",
    "device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
    "device_id = 0 if device == \"cuda\" else -1\n",
    "model_flags = ['hidden_size', 'ff_size', 'heads', 'inter_layers','encoder','ff_actv', 'use_interval','rnn_size']\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import urllib3\n",
    "from glob import glob\n",
    "import collections\n",
    "import six\n",
    "import gc\n",
    "import gluonnlp as nlp\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.utils import download as _download\n",
    "\n",
    "def do_lang ( openapi_key, text ) :\n",
    "    openApiURL = \"http://aiopen.etri.re.kr:8000/WiseNLU\"\n",
    "    requestJson = {  \n",
    "        \"argument\": {\n",
    "            \"text\": text,\n",
    "            \"analysis_code\": \"morp\"\n",
    "        }\n",
    "    }\n",
    "    http = urllib3.PoolManager()\n",
    "    response = http.request(\n",
    "        \"POST\",\n",
    "        openApiURL,\n",
    "        headers={\"Content-Type\": \"application/json; charset=UTF-8\", \"Authorization\" :  openapi_key},\n",
    "        body=json.dumps(requestJson)\n",
    "    )\n",
    "\n",
    "    json_data = json.loads(response.data.decode('utf-8'))\n",
    "    json_result = json_data[\"result\"]\n",
    "    \n",
    "    if json_result == -1:\n",
    "        json_reason = json_data[\"reason\"]\n",
    "        if \"Invalid Access Key\" in json_reason:\n",
    "            logger.info(json_reason)\n",
    "            logger.info(\"Please check the openapi access key.\")\n",
    "            sys.exit()\n",
    "        return \"openapi error - \" + json_reason\n",
    "    else:\n",
    "        json_data = json.loads(response.data.decode('utf-8'))\n",
    "    \n",
    "        json_return_obj = json_data[\"return_object\"]\n",
    "        \n",
    "        return_result = \"\"\n",
    "        json_sentence = json_return_obj[\"sentence\"]\n",
    "        for json_morp in json_sentence:\n",
    "            for morp in json_morp[\"morp\"]:\n",
    "                return_result = return_result+str(morp[\"lemma\"])+\"/\"+str(morp[\"type\"])+\" \"\n",
    "\n",
    "        return return_result\n",
    "\n",
    "def get_kobert_vocab(cachedir=\"./tmp/\"):\n",
    "    # Add BOS,EOS vocab\n",
    "    tokenizer = {\n",
    "        \"url\": \"s3://skt-lsl-nlp-model/KoBERT/tokenizers/kobert_news_wiki_ko_cased-1087f8699e.spiece\",\n",
    "        #\"fname\": \"/opt/ml/SumAI/bertsum/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\",\n",
    "        \"chksum\": \"ae5711deb3\",\n",
    "    }\n",
    "    \n",
    "    vocab_info = tokenizer\n",
    "    vocab_file = _download(\n",
    "        #vocab_info[\"url\"], vocab_info[\"fname\"], vocab_info[\"chksum\"], cachedir=cachedir\n",
    "        vocab_info[\"url\"], vocab_info[\"chksum\"], cachedir=cachedir\n",
    "    )\n",
    "\n",
    "    vocab_b_obj = nlp.vocab.BERTVocab.from_sentencepiece(\n",
    "        vocab_file[0], padding_token=\"[PAD]\", bos_token=\"[BOS]\", eos_token=\"[EOS]\"\n",
    "    )\n",
    "    return vocab_b_obj\n",
    "\n",
    "    \n",
    "class BertData():\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        #self.tokenizer = Tokenizer(vocab_file_path)\n",
    "        self.sep_vid = self.tokenizer.vocab['[SEP]']\n",
    "        self.cls_vid = self.tokenizer.vocab['[CLS]']\n",
    "        self.pad_vid = self.tokenizer.vocab['[PAD]']\n",
    "\n",
    "    def preprocess(self, src):\n",
    "\n",
    "        if (len(src) == 0):\n",
    "            return None\n",
    "\n",
    "        original_src_txt = [''.join(s) for s in src]\n",
    "\n",
    "\n",
    "        idxs = [i for i, s in enumerate(src) if (len(s) > 0)]\n",
    "\n",
    "        src = [src[i][:20000] for i in idxs]\n",
    "        src = src[:10000]\n",
    "\n",
    "        if (len(src) < 3):\n",
    "            return None\n",
    "\n",
    "        src_txt = [''.join(sent) for sent in src]\n",
    "        text = ' [SEP] [CLS] '.join(src_txt)\n",
    "        src_subtokens = text.split(' ')\n",
    "        src_subtokens = src_subtokens[:510]\n",
    "        src_subtokens = ['[CLS]'] + src_subtokens + ['[SEP]']\n",
    "\n",
    "        src_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(src_subtokens)\n",
    "        _segs = [-1] + [i for i, t in enumerate(src_subtoken_idxs) if t == self.sep_vid]\n",
    "        segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]\n",
    "        segments_ids = []\n",
    "        for i, s in enumerate(segs):\n",
    "            if (i % 2 == 0):\n",
    "                segments_ids += s * [0]\n",
    "            else:\n",
    "                segments_ids += s * [1]\n",
    "        cls_ids = [i for i, t in enumerate(src_subtoken_idxs) if t == self.cls_vid]\n",
    "        labels = None\n",
    "        tgt_txt = None\n",
    "        src_txt = [original_src_txt[i] for i in idxs]\n",
    "        return src_subtoken_idxs, labels, segments_ids, cls_ids, src_txt, tgt_txt\n",
    "    \n",
    "def convert_to_unicode(text):\n",
    "    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
    "    if six.PY3:\n",
    "        if isinstance(text, str):\n",
    "            return text\n",
    "        elif isinstance(text, bytes):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    elif six.PY2:\n",
    "        if isinstance(text, str):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        elif isinstance(text, unicode):\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    else:\n",
    "        raise ValueError(\"Not running on Python2 or Python 3?\")\n",
    "        \n",
    "class Tokenizer(object):\n",
    "    \n",
    "    def __init__(self, vocab_file_path):\n",
    "        self.vocab_file_path = vocab_file_path\n",
    "        \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "        vocab = collections.OrderedDict()\n",
    "        index = 0\n",
    "        with open(self.vocab_file_path, \"r\", encoding='utf-8') as reader:\n",
    "\n",
    "            while True:\n",
    "                token = convert_to_unicode(reader.readline())\n",
    "                if not token:\n",
    "                    break\n",
    "\n",
    "          ### joonho.lim @ 2019-03-15\n",
    "                if token.find('n_iters=') == 0 or token.find('max_length=') == 0 :\n",
    "\n",
    "                    continue\n",
    "                token = token.split('\\t')[0].strip('_')\n",
    "\n",
    "                token = token.strip()\n",
    "                vocab[token] = index\n",
    "                index += 1\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        \"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                ids.append(self.vocab[token])\n",
    "            except:\n",
    "                ids.append(1)\n",
    "        if len(ids) > 10000:\n",
    "            raise ValueError(\n",
    "                \"Token indices sequence length is longer than the specified maximum \"\n",
    "                \" sequence length for this BERT model ({} > {}). Running this\"\n",
    "                \" sequence through BERT will result in indexing errors\".format(len(ids), 10000)\n",
    "            )\n",
    "        return ids\n",
    "\n",
    "\n",
    "def _lazy_dataset_loader(pt_file):\n",
    "    dataset = pt_file    \n",
    "    yield dataset\n",
    "    \n",
    "def News_to_input(text, openapi_key, tokenizer):\n",
    "    newstemp = do_lang(openapi_key, text)\n",
    "    news = newstemp.split(' ./SF ')[:-1]\n",
    "    \n",
    "    bertdata = BertData(tokenizer)\n",
    "    sent_labels = [0] * len(news)\n",
    "    tmp = bertdata.preprocess(news)\n",
    "    if(not tmp): return None\n",
    "    #print(tmp)\n",
    "    b_data_dict = {\"src\":tmp[0],\n",
    "               \"tgt\": [0],\n",
    "               \"labels\":[0,0,0],\n",
    "               \"src_sent_labels\":sent_labels,\n",
    "               \"segs\":tmp[2],\n",
    "               \"clss\":tmp[3],\n",
    "               \"src_txt\":tmp[4],\n",
    "               \"tgt_txt\":'hehe'}\n",
    "    b_list = []\n",
    "    b_list.append(b_data_dict) \n",
    "    return b_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fe9987f-1fb7-4fe5-8902-8272956a5c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /opt/ml/KorBertSum/src/./tmp/kobert_news_wiki_ko_cased-1087f8699e.spiece\n",
      "using cached model. /opt/ml/KorBertSum/src/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-23 13:36:02,157 INFO] loading archive file ../001_bert_morp_pytorch\n",
      "[2023-01-23 13:36:02,159 INFO] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30349\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6b9a65280a84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mchat_history_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbot_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat_history_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'------------------------------------------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-e83c79663f8d>\u001b[0m in \u001b[0;36msummary\u001b[0;34m(args, b_list, device_id, pt, step, model)\u001b[0m\n\u001b[1;32m    362\u001b[0m                                   shuffle=False, is_test=True)\n\u001b[1;32m    363\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-e83c79663f8d>\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, test_iter, step, cal_lead, cal_oracle)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m                 \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/KorBertSum/src/models/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mdataset_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_dataset_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/KorBertSum/src/models/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m                 \u001b[0;31m# fast-forward if loaded from state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterations_this_epoch\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/KorBertSum/src/models/data_loader.py\u001b[0m in \u001b[0;36mcreate_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;34m\"\"\" Create batches \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mp_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/KorBertSum/src/models/data_loader.py\u001b[0m in \u001b[0;36mbatch_buffer\u001b[0;34m(self, data, batch_size)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbatch_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_so_far\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'src'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "openapi_key = '9318dc23-24ac-4b59-a99e-a29ec170bf02'\n",
    "chat_history = \"뉴스 \"\n",
    "user_input = '''\n",
    "일본 정부가 문재인 대통령이 도쿄올림픽을 계기로 일본을 방문하는 방향으로 한일 양국이 조율하고 있다는 15일 요미우리신문의 보도를 부인했다.\n",
    "정부 대변인인 가토 가쓰노부(加藤勝信) 관방장관은 이날 오전 정례 기자회견에서 관련 질문에 “말씀하신 보도와 같은 사실이 없는 것으로 안다”고 밝혔다.\n",
    "앞서 요미우리는 한국 측이 도쿄올림픽을 계기로 한 문 대통령의 방일을 타진했고, 일본 측은 수용하는 방향이라고 이날 보도했다.\n",
    "한국 측은 문 대통령의 방일 때 스가 요시히데(菅義偉) 총리와 처음으로 정상회담을 하겠다는 생각이라고 요미우리는 전했다.\n",
    "가토 장관은 한일 정상회담에 대한 일본 정부의 자세에 대해 “그런 사실이 없기 때문에 가정의 질문에 대해 답하는 것을 삼가겠다”고 말했다.\n",
    "그는 한국 측의 독도방어훈련에 ‘어떤 대항 조치를 생각하고 있느냐’는 질문에는 “한국 해군의 훈련에 대해 정부로서는 강한 관심을 가지고 주시하는 상황이어서 지금 시점에선 논평을 삼가겠다”고 말을 아꼈다.\n",
    "가토 장관은 “다케시마(竹島·일본이 주장하는 독도의 명칭)는 역사적 사실에 비춰봐도, 국제법상으로도 명백한 일본 고유의 영토”라며 독도 영유권 주장을 되풀이했다.\n",
    "그러면서 “다케시마 문제에 대해서는 계속 우리나라의 영토, 영해, 영공을 단호히 지키겠다는 결의로 냉정하고 의연하게 대응해갈 생각”이라고 밝혔다.\n",
    "'''\n",
    "vocab = get_kobert_vocab()\n",
    "tokenizer = nlp.data.BERTSPTokenizer(get_tokenizer(), vocab, lower=False)\n",
    "\n",
    "\n",
    "bot_input_ids = News_to_input(chat_history + user_input, openapi_key, tokenizer)\n",
    "print('------------------------------------------------------------')\n",
    "\n",
    "\n",
    "#logger.info('Loading checkpoint from %s' % test_from)\n",
    "checkpoint = torch.load('../models/bert_classifier/model_step_1000.pt', map_location=lambda storage, loc: storage)\n",
    "config = BertConfig.from_json_file(args.bert_config_path)\n",
    "model = Summarizer(args, device, load_pretrained_bert=False, bert_config = config)\n",
    "model.load_cp(checkpoint)\n",
    "model.eval()  \n",
    "\n",
    "chat_history_ids = summary(args, bot_input_ids, 0, '', None, model)\n",
    "print(chat_history_ids)\n",
    "print('------------------------------------------------------------')\n",
    "print(chat_history_ids[0])\n",
    "pred_lst = list(chat_history_ids[0][:3])\n",
    "print(pred_lst)\n",
    "print('------------------------------------------------------------')\n",
    "final_text = ''\n",
    "for i,a in enumerate(user_input.split('. ')):\n",
    "    if i in pred_lst:\n",
    "        print(i, a)\n",
    "        final_text = final_text+a+'. '\n",
    "chat_history = user_input + '<token>' +final_text\n",
    "print('------------------------------------------------------------')\n",
    "print(final_text)\n",
    "print('------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d678ab3-cabf-4460-8fd4-893b7ee4c63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, '\\n가토 장관은 “다케시마(竹島·일본이 주장하는 독도의 명칭)는 역사적 사실에 비춰봐도, 국제법상으로도 명백한 일본 고유의 영토”라며 독도 영유권 주장을 되풀이했다'), (4, '\\n가토 장관은 한일 정상회담에 대한 일본 정부의 자세에 대해 “그런 사실이 없기 때문에 가정의 질문에 대해 답하는 것을 삼가겠다”고 말했다'), (2, '\\n앞서 요미우리는 한국 측이 도쿄올림픽을 계기로 한 문 대통령의 방일을 타진했고, 일본 측은 수용하는 방향이라고 이날 보도했다'), (3, '\\n한국 측은 문 대통령의 방일 때 스가 요시히데(菅義偉) 총리와 처음으로 정상회담을 하겠다는 생각이라고 요미우리는 전했다'), (0, '\\n일본 정부가 문재인 대통령이 도쿄올림픽을 계기로 일본을 방문하는 방향으로 한일 양국이 조율하고 있다는 15일 요미우리신문의 보도를 부인했다'), (1, '\\n정부 대변인인 가토 가쓰노부(加藤勝信) 관방장관은 이날 오전 정례 기자회견에서 관련 질문에 “말씀하신 보도와 같은 사실이 없는 것으로 안다”고 밝혔다'), (5, '\\n그는 한국 측의 독도방어훈련에 ‘어떤 대항 조치를 생각하고 있느냐’는 질문에는 “한국 해군의 훈련에 대해 정부로서는 강한 관심을 가지고 주시하는 상황이어서 지금 시점에선 논평을 삼가겠다”고 말을 아꼈다'), (7, '\\n그러면서 “다케시마 문제에 대해서는 계속 우리나라의 영토, 영해, 영공을 단호히 지키겠다는 결의로 냉정하고 의연하게 대응해갈 생각”이라고 밝혔다')]\n"
     ]
    }
   ],
   "source": [
    "def get_top_sentences(user_input, model, tokenizer):\n",
    "    bot_input_ids = News_to_input(user_input, openapi_key, tokenizer)\n",
    "    \n",
    "    chat_history_ids = summary(args, bot_input_ids, 0, '', None, model)\n",
    "    pred_lst = list(chat_history_ids[0])\n",
    "    #print(pred_lst)\n",
    "    final_text = []\n",
    "    for p in pred_lst:\n",
    "        if(p < len(user_input.split('.'))):\n",
    "            final_text.append((p, user_input.split('.')[p]))\n",
    "    return final_text\n",
    "    \n",
    "\n",
    "print(get_top_sentences(user_input, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0adf3358-48bb-486d-bd90-67a7345dff88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-22 14:57:38,326 INFO] loading archive file ../001_bert_morp_pytorch\n",
      "[2023-01-22 14:57:38,328 INFO] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30349\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "checkpoint = torch.load('../models/bert_classifier2/model_step_35000.pt', map_location=lambda storage, loc: storage)\n",
    "config = BertConfig.from_json_file(args.bert_config_path)\n",
    "model = Summarizer(args, device, load_pretrained_bert=False, bert_config = config)\n",
    "model.load_cp(checkpoint)\n",
    "model.eval()  \n",
    "\n",
    "def add_topk_to_df(df, model, tokenizer):\n",
    "    start = time.time()\n",
    "    topk = []\n",
    "    for i,context in enumerate(tqdm(df['context'])):\n",
    "        context = eval(context)[:30]\n",
    "        top = None\n",
    "        if(len(context) > 3):\n",
    "            top = get_top_sentences(' '.join(context), model, tokenizer)\n",
    "        if(top):\n",
    "            topk.append(top)\n",
    "        else:\n",
    "            topk.append('Hello world')\n",
    "            \n",
    "    df['topk'] = topk\n",
    "    df = df.drop(df[df['topk'] == 'Hello world'].index)\n",
    "    print(f\"{time.time()-start:.4f} sec\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0ded50e3-c936-4438-820e-a76650b10ae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     ['국정과제 점검회의 공공임대주택 무조건 아냐...다주택자 중과세 등 세 부담 경감...\n",
       "1     ['대통령 방산 금융지원 강화 주문 1주일 만에 이행 계획 발표 중소기업 협력사 지...\n",
       "2     ['viewer 이재명 대통령실 부대변인이 12일 오후 서울 용산 대통령실 청사에서...\n",
       "3     ['국내 기가 팩토리 건설 기 대속 포항 고양 강원도 유치 나서 세금 감면 규제 완...\n",
       "4     ['강원도 테슬라 공장 유치 도전 후보지는 강릉 뉴스9 강릉 입력 23 31 수정 ...\n",
       "                            ...                        \n",
       "83    ['김주현 금융위원장이 15일 다주택자와 임대사업자들에 대해 주택담보대출을 받을 수...\n",
       "84    ['연합뉴스 김주현 금융위원장은 부동산 시장의 연착륙을 위해 다주택자나 임대사업자에...\n",
       "85    ['윤석열 대통령이 15일 서울 청와대 영빈관에서 열린 제1차 국정과제 점검회의를 ...\n",
       "86    ['김주현 금융위원장은 다주택자나 임대사업자에게도 주택담보대출을 허용하는 방안을 추...\n",
       "87    ['viewer 윤석 열 대통령이 15일 서울 청와대 영빈관에서 열린 제1차 국정과...\n",
       "Name: context, Length: 88, dtype: object"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "name = 'hdbscan_윤석열_20221201_20221215_crwal_news_context'\n",
    "topic_df = pd.read_csv(f'./hdbscan/{name}.csv')\n",
    "topic_df.context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c7161541-22a5-4e4e-80f6-990ac5b63bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:46<00:00,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.1130 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = add_topk_to_df(topic_df, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bb3a2552-5350-448d-973b-84763aaeaac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'./hdbscan/{name}_pre.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7adc12ef-aec3-4224-88a2-311965d5c167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hdbscan_부동산_20221201_20221215_crwal_news_context.pkl',\n",
       " 'hdbscan_윤석열_20221201_20221203_crwal_news_context.csv',\n",
       " 'hdbscan_삼성전자_20221201_20221215_crwal_news_context.csv',\n",
       " 'hdbscan_삼성전자_20221201_20221203_crwal_news_context.pkl',\n",
       " 'hdbscan_삼성전자_20221201_20221231_crwal_news_context.pkl',\n",
       " 'hdbscan_부동산_20221201_20221231_crwal_news_context.csv',\n",
       " 'hdbscan_삼성전자_20221201_20221203_crwal_news_context.csv',\n",
       " 'hdbscan_삼성전자_20221201_20221231_crwal_news_context.csv',\n",
       " 'hdbscan_윤석열_20221201_20221231_crwal_news_context.pkl',\n",
       " 'hdbscan_윤석열_20221201_20221215_crwal_news_context.pkl',\n",
       " 'hdbscan_부동산_20221201_20221203_crwal_news_context.pkl',\n",
       " 'hdbscan_부동산_20221201_20221215_crwal_news_context.csv',\n",
       " 'hdbscan_부동산_20221201_20221203_crwal_news_context.csv',\n",
       " 'hdbscan_부동산_20221201_20221231_crwal_news_context.pkl',\n",
       " 'hdbscan_윤석열_20221201_20221203_crwal_news_context.pkl',\n",
       " 'hdbscan_윤석열_20221201_20221215_crwal_news_context.csv',\n",
       " 'hdbscan_삼성전자_20221201_20221215_crwal_news_context.pkl',\n",
       " 'hdbscan_윤석열_20221201_20221231_crwal_news_context.csv']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = './hdbscan/'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "onlyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "60580317-5288-469e-824e-9642c02eb8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:06<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.1755 sec\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0069 sec\n",
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:08<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.5157 sec\n",
      "53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:26<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.5827 sec\n",
      "251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [01:58<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118.5586 sec\n",
      "126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [01:05<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.4341 sec\n",
      "306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 306/306 [02:37<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157.7319 sec\n",
      "134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [01:09<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.1316 sec\n",
      "111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 111/111 [00:58<00:00,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.6607 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for file in onlyfiles:\n",
    "    if(file.split('.')[1] == 'csv'):\n",
    "        name = file.split('.')[0]\n",
    "        topic_df = pd.read_csv(f'./{mypath}/{name}.csv')\n",
    "        print(len(topic_df))\n",
    "        df = add_topk_to_df(topic_df, model, tokenizer)\n",
    "        df.to_csv(f'./{mypath}/{name}_pre.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6123736f-efd0-438c-9e49-fb765039114b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
