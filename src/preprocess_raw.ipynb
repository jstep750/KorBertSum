{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9839f01e-32e9-43e2-8439-b6c458780a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_sentences(user_input, model, tokenizer):\n",
    "    bot_input_ids = News_to_input(user_input, openapi_key, tokenizer)\n",
    "    \n",
    "    chat_history_ids = summary(args, bot_input_ids, 0, '', None, model)\n",
    "    pred_lst = list(chat_history_ids[0])\n",
    "    final_text = []\n",
    "    for p in pred_lst:\n",
    "        if(p < len(user_input.split('. ')) and len(user_input.split('. ')[p]) > 10):\n",
    "            final_text.append((p, user_input.split('. ')[p]))\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2eb87b15-49c0-4b1b-861d-a17f66beed76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python3.6/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Main training workflow\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import signal\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertConfig\n",
    "\n",
    "\n",
    "import distributed\n",
    "from models import data_loader, model_builder\n",
    "from models.data_loader import load_dataset\n",
    "from models.model_builder import Summarizer\n",
    "from tensorboardX import SummaryWriter\n",
    "from models.reporter import ReportMgr\n",
    "from models.stats import Statistics\n",
    "from others.logging import logger\n",
    "# from models.trainer import build_trainer\n",
    "# build_trainer의 dependency package pyrouge.utils가 import되지 않아 직접 셀에 삽입\n",
    "from others.logging import logger, init_logger\n",
    "import easydict\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    \"encoder\":'classifier',\n",
    "    \"mode\":'summary',\n",
    "    \"bert_data_path\":'../bert_sample/korean',\n",
    "    \"model_path\":'../models/bert_classifier',\n",
    "    \"bert_model\":'../001_bert_morp_pytorch',\n",
    "    \"result_path\":'../results/korean',\n",
    "    \"temp_dir\":'.',\n",
    "    \"bert_config_path\":'../001_bert_morp_pytorch/bert_config.json',\n",
    "    \"batch_size\":1000,\n",
    "    \"use_interval\":True,\n",
    "    \"hidden_size\":128,\n",
    "    \"ff_size\":512,\n",
    "    \"heads\":4,\n",
    "    \"inter_layers\":2,\n",
    "    \"rnn_size\":512,\n",
    "    \"param_init\":0,\n",
    "    \"param_init_glorot\":True,\n",
    "    \"dropout\":0.1,\n",
    "    \"optim\":'adam',\n",
    "    \"lr\":2e-3,\n",
    "    \"report_every\":1,\n",
    "    \"save_checkpoint_steps\":5,\n",
    "    \"block_trigram\":True,\n",
    "    \"recall_eval\":False,\n",
    "    \n",
    "    \"accum_count\":1,\n",
    "    \"world_size\":1,\n",
    "    \"visible_gpus\":'0',\n",
    "    \"gpu_ranks\":'0',\n",
    "    \"log_file\":'../logs/bert_classifier',\n",
    "    \"test_from\":'../models/bert_classifier2/model_step_35000.pt'\n",
    "})\n",
    "\n",
    "\n",
    "def build_trainer(args, device_id, model,\n",
    "                  optim):\n",
    "    \"\"\"\n",
    "    Simplify `Trainer` creation based on user `opt`s*\n",
    "    Args:\n",
    "        opt (:obj:`Namespace`): user options (usually from argument parsing)\n",
    "        model (:obj:`onmt.models.NMTModel`): the model to train\n",
    "        fields (dict): dict of fields\n",
    "        optim (:obj:`onmt.utils.Optimizer`): optimizer used during training\n",
    "        data_type (str): string describing the type of data\n",
    "            e.g. \"text\", \"img\", \"audio\"\n",
    "        model_saver(:obj:`onmt.models.ModelSaverBase`): the utility object\n",
    "            used to save the model\n",
    "    \"\"\"\n",
    "    device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
    "\n",
    "\n",
    "    grad_accum_count = args.accum_count\n",
    "    n_gpu = args.world_size\n",
    "\n",
    "    if device_id >= 0:\n",
    "        gpu_rank = int(args.gpu_ranks[device_id])\n",
    "    else:\n",
    "        gpu_rank = 0\n",
    "        n_gpu = 0\n",
    "\n",
    "    #print('gpu_rank %d' % gpu_rank)\n",
    "\n",
    "    tensorboard_log_dir = args.model_path\n",
    "\n",
    "    writer = SummaryWriter(tensorboard_log_dir, comment=\"Unmt\")\n",
    "\n",
    "    report_manager = ReportMgr(args.report_every, start_time=-1, tensorboard_writer=writer)\n",
    "\n",
    "    trainer = Trainer(args, model, optim, grad_accum_count, n_gpu, gpu_rank, report_manager)\n",
    "\n",
    "    # print(tr)\n",
    "    if (model):\n",
    "        n_params = _tally_parameters(model)\n",
    "        #logger.info('* number of parameters: %d' % n_params)\n",
    "\n",
    "    return trainer\n",
    "\n",
    "class Trainer(object):\n",
    "    \"\"\"\n",
    "    Class that controls the training process.\n",
    "\n",
    "    Args:\n",
    "            model(:py:class:`onmt.models.model.NMTModel`): translation model\n",
    "                to train\n",
    "            train_loss(:obj:`onmt.utils.loss.LossComputeBase`):\n",
    "               training loss computation\n",
    "            valid_loss(:obj:`onmt.utils.loss.LossComputeBase`):\n",
    "               training loss computation\n",
    "            optim(:obj:`onmt.utils.optimizers.Optimizer`):\n",
    "               the optimizer responsible for update\n",
    "            trunc_size(int): length of truncated back propagation through time\n",
    "            shard_size(int): compute loss in shards of this size for efficiency\n",
    "            data_type(string): type of the source input: [text|img|audio]\n",
    "            norm_method(string): normalization methods: [sents|tokens]\n",
    "            grad_accum_count(int): accumulate gradients this many times.\n",
    "            report_manager(:obj:`onmt.utils.ReportMgrBase`):\n",
    "                the object that creates reports, or None\n",
    "            model_saver(:obj:`onmt.models.ModelSaverBase`): the saver is\n",
    "                used to save a checkpoint.\n",
    "                Thus nothing will be saved if this parameter is None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,  args, model,  optim,\n",
    "                  grad_accum_count=1, n_gpu=1, gpu_rank=1,\n",
    "                  report_manager=None):\n",
    "        # Basic attributes.\n",
    "        self.args = args\n",
    "        self.save_checkpoint_steps = args.save_checkpoint_steps\n",
    "        self.model = model\n",
    "        self.optim = optim\n",
    "        self.grad_accum_count = grad_accum_count\n",
    "        self.n_gpu = n_gpu\n",
    "        self.gpu_rank = gpu_rank\n",
    "        self.report_manager = report_manager\n",
    "\n",
    "        self.loss = torch.nn.BCELoss(reduction='none')\n",
    "        assert grad_accum_count > 0\n",
    "        # Set model in training mode.\n",
    "        if (model):\n",
    "            self.model.train()\n",
    "\n",
    "    def summary(self, test_iter, step, cal_lead=False, cal_oracle=False):\n",
    "        \"\"\" Validate model.\n",
    "            valid_iter: validate data iterator\n",
    "        Returns:\n",
    "            :obj:`nmt.Statistics`: validation loss statistics\n",
    "        \"\"\"\n",
    "        # Set model in validating mode.\n",
    "        def _get_ngrams(n, text):\n",
    "            ngram_set = set()\n",
    "            text_length = len(text)\n",
    "            max_index_ngram_start = text_length - n\n",
    "            for i in range(max_index_ngram_start + 1):\n",
    "                ngram_set.add(tuple(text[i:i + n]))\n",
    "            return ngram_set\n",
    "\n",
    "        def _block_tri(c, p):\n",
    "            tri_c = _get_ngrams(3, c.split())\n",
    "            for s in p:\n",
    "                tri_s = _get_ngrams(3, s.split())\n",
    "                if len(tri_c.intersection(tri_s))>0:\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        if (not cal_lead and not cal_oracle):\n",
    "            self.model.eval()\n",
    "        stats = Statistics()\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_iter:\n",
    "                src = batch.src\n",
    "                labels = batch.labels\n",
    "                segs = batch.segs\n",
    "                clss = batch.clss\n",
    "                mask = batch.mask\n",
    "                mask_cls = batch.mask_cls\n",
    "\n",
    "\n",
    "                gold = []\n",
    "                pred = []\n",
    "\n",
    "                if (cal_lead):\n",
    "                    selected_ids = [list(range(batch.clss.size(1)))] * batch.batch_size\n",
    "                elif (cal_oracle):\n",
    "                    selected_ids = [[j for j in range(batch.clss.size(1)) if labels[i][j] == 1] for i in\n",
    "                                    range(batch.batch_size)]\n",
    "                else:\n",
    "                    sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
    "\n",
    "                    # loss = self.loss(sent_scores, labels.float())\n",
    "                    # loss = (loss * mask.float()).sum()\n",
    "                    # batch_stats = Statistics(float(loss.cpu().data.numpy()), len(labels))\n",
    "                    # stats.update(batch_stats)\n",
    "\n",
    "                    sent_scores = sent_scores + mask.float()\n",
    "                    sent_scores = sent_scores.cpu().data.numpy()\n",
    "                    selected_ids = np.argsort(-sent_scores, 1)\n",
    "                # selected_ids = np.sort(selected_ids,1)\n",
    "                \n",
    "\n",
    "        return selected_ids\n",
    "\n",
    "\n",
    "    def _gradient_accumulation(self, true_batchs, normalization, total_stats,\n",
    "                               report_stats):\n",
    "        if self.grad_accum_count > 1:\n",
    "            self.model.zero_grad()\n",
    "\n",
    "        for batch in true_batchs:\n",
    "            if self.grad_accum_count == 1:\n",
    "                self.model.zero_grad()\n",
    "\n",
    "            src = batch.src\n",
    "            labels = batch.labels\n",
    "            segs = batch.segs\n",
    "            clss = batch.clss\n",
    "            mask = batch.mask\n",
    "            mask_cls = batch.mask_cls\n",
    "\n",
    "            sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
    "\n",
    "            loss = self.loss(sent_scores, labels.float())\n",
    "            loss = (loss*mask.float()).sum()\n",
    "            (loss/loss.numel()).backward()\n",
    "            # loss.div(float(normalization)).backward()\n",
    "\n",
    "            batch_stats = Statistics(float(loss.cpu().data.numpy()), normalization)\n",
    "\n",
    "\n",
    "            total_stats.update(batch_stats)\n",
    "            report_stats.update(batch_stats)\n",
    "\n",
    "            # 4. Update the parameters and statistics.\n",
    "            if self.grad_accum_count == 1:\n",
    "                # Multi GPU gradient gather\n",
    "                if self.n_gpu > 1:\n",
    "                    grads = [p.grad.data for p in self.model.parameters()\n",
    "                             if p.requires_grad\n",
    "                             and p.grad is not None]\n",
    "                    distributed.all_reduce_and_rescale_tensors(\n",
    "                        grads, float(1))\n",
    "                self.optim.step()\n",
    "\n",
    "        # in case of multi step gradient accumulation,\n",
    "        # update only after accum batches\n",
    "        if self.grad_accum_count > 1:\n",
    "            if self.n_gpu > 1:\n",
    "                grads = [p.grad.data for p in self.model.parameters()\n",
    "                         if p.requires_grad\n",
    "                         and p.grad is not None]\n",
    "                distributed.all_reduce_and_rescale_tensors(\n",
    "                    grads, float(1))\n",
    "            self.optim.step()\n",
    "\n",
    "    def _save(self, step):\n",
    "        real_model = self.model\n",
    "        # real_generator = (self.generator.module\n",
    "        #                   if isinstance(self.generator, torch.nn.DataParallel)\n",
    "        #                   else self.generator)\n",
    "\n",
    "        model_state_dict = real_model.state_dict()\n",
    "        # generator_state_dict = real_generator.state_dict()\n",
    "        checkpoint = {\n",
    "            'model': model_state_dict,\n",
    "            # 'generator': generator_state_dict,\n",
    "            'opt': self.args,\n",
    "            'optim': self.optim,\n",
    "        }\n",
    "        checkpoint_path = os.path.join(self.args.model_path, 'model_step_%d.pt' % step)\n",
    "        #logger.info(\"Saving checkpoint %s\" % checkpoint_path)\n",
    "        # checkpoint_path = '%s_step_%d.pt' % (FLAGS.model_path, step)\n",
    "        if (not os.path.exists(checkpoint_path)):\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            return checkpoint, checkpoint_path\n",
    "\n",
    "    def _start_report_manager(self, start_time=None):\n",
    "        \"\"\"\n",
    "        Simple function to start report manager (if any)\n",
    "        \"\"\"\n",
    "        if self.report_manager is not None:\n",
    "            if start_time is None:\n",
    "                self.report_manager.start()\n",
    "            else:\n",
    "                self.report_manager.start_time = start_time\n",
    "\n",
    "    def _maybe_gather_stats(self, stat):\n",
    "        \"\"\"\n",
    "        Gather statistics in multi-processes cases\n",
    "\n",
    "        Args:\n",
    "            stat(:obj:onmt.utils.Statistics): a Statistics object to gather\n",
    "                or None (it returns None in this case)\n",
    "\n",
    "        Returns:\n",
    "            stat: the updated (or unchanged) stat object\n",
    "        \"\"\"\n",
    "        if stat is not None and self.n_gpu > 1:\n",
    "            return Statistics.all_gather_stats(stat)\n",
    "        return stat\n",
    "\n",
    "    def _maybe_report_training(self, step, num_steps, learning_rate,\n",
    "                               report_stats):\n",
    "        \"\"\"\n",
    "        Simple function to report training stats (if report_manager is set)\n",
    "        see `onmt.utils.ReportManagerBase.report_training` for doc\n",
    "        \"\"\"\n",
    "        if self.report_manager is not None:\n",
    "            return self.report_manager.report_training(\n",
    "                step, num_steps, learning_rate, report_stats,\n",
    "                multigpu=self.n_gpu > 1)\n",
    "\n",
    "    def _report_step(self, learning_rate, step, train_stats=None,\n",
    "                     valid_stats=None):\n",
    "        \"\"\"\n",
    "        Simple function to report stats (if report_manager is set)\n",
    "        see `onmt.utils.ReportManagerBase.report_step` for doc\n",
    "        \"\"\"\n",
    "        if self.report_manager is not None:\n",
    "            return self.report_manager.report_step(\n",
    "                learning_rate, step, train_stats=train_stats,\n",
    "                valid_stats=valid_stats)\n",
    "\n",
    "    def _maybe_save(self, step):\n",
    "        \"\"\"\n",
    "        Save the model if a model saver is set\n",
    "        \"\"\"\n",
    "        if self.model_saver is not None:\n",
    "            self.model_saver.maybe_save(step)\n",
    "\n",
    "            \n",
    "def summary(args, b_list, device_id, pt, step, model):\n",
    "    device_id = 0\n",
    "    device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
    "    if (pt != ''):\n",
    "        test_from = pt\n",
    "    else:\n",
    "        test_from = args.test_from\n",
    "    \n",
    "    opt = vars(checkpoint['opt'])\n",
    "    for k in opt.keys():\n",
    "        if (k in model_flags):\n",
    "            setattr(args, k, opt[k])\n",
    "    #print(args)\n",
    "\n",
    "    #model.load_cp(checkpoint)\n",
    "    #model.eval()\n",
    "\n",
    "    test_iter =data_loader.Dataloader(args, _lazy_dataset_loader(b_list),\n",
    "                                  args.batch_size, device,\n",
    "                                  shuffle=False, is_test=True)\n",
    "    trainer = build_trainer(args, device_id, model, None)\n",
    "    result = trainer.summary(test_iter,step)\n",
    "    return result\n",
    "\n",
    "def _tally_parameters(model):\n",
    "    n_params = sum([p.nelement() for p in model.parameters()])\n",
    "    return n_params\n",
    "\n",
    "args.gpu_ranks = [int(i) for i in args.gpu_ranks.split(',')]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.visible_gpus\n",
    "\n",
    "init_logger(args.log_file)\n",
    "device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
    "device_id = 0 if device == \"cuda\" else -1\n",
    "model_flags = ['hidden_size', 'ff_size', 'heads', 'inter_layers','encoder','ff_actv', 'use_interval','rnn_size']\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import urllib3\n",
    "from glob import glob\n",
    "import collections\n",
    "import six\n",
    "import gc\n",
    "import gluonnlp as nlp\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.utils import download as _download\n",
    "\n",
    "def do_lang ( openapi_key, text ) :\n",
    "    openApiURL = \"http://aiopen.etri.re.kr:8000/WiseNLU\"\n",
    "    requestJson = {  \n",
    "        \"argument\": {\n",
    "            \"text\": text,\n",
    "            \"analysis_code\": \"morp\"\n",
    "        }\n",
    "    }\n",
    "    http = urllib3.PoolManager()\n",
    "    response = http.request(\n",
    "        \"POST\",\n",
    "        openApiURL,\n",
    "        headers={\"Content-Type\": \"application/json; charset=UTF-8\", \"Authorization\" :  openapi_key},\n",
    "        body=json.dumps(requestJson)\n",
    "    )\n",
    "\n",
    "    json_data = json.loads(response.data.decode('utf-8'))\n",
    "    json_result = json_data[\"result\"]\n",
    "    \n",
    "    if json_result == -1:\n",
    "        json_reason = json_data[\"reason\"]\n",
    "        if \"Invalid Access Key\" in json_reason:\n",
    "            logger.info(json_reason)\n",
    "            logger.info(\"Please check the openapi access key.\")\n",
    "            sys.exit()\n",
    "        return \"openapi error - \" + json_reason\n",
    "    else:\n",
    "        json_data = json.loads(response.data.decode('utf-8'))\n",
    "    \n",
    "        json_return_obj = json_data[\"return_object\"]\n",
    "        \n",
    "        return_result = \"\"\n",
    "        json_sentence = json_return_obj[\"sentence\"]\n",
    "        for json_morp in json_sentence:\n",
    "            for morp in json_morp[\"morp\"]:\n",
    "                return_result = return_result+str(morp[\"lemma\"])+\"/\"+str(morp[\"type\"])+\" \"\n",
    "\n",
    "        return return_result\n",
    "\n",
    "def get_kobert_vocab(cachedir=\"./tmp/\"):\n",
    "    # Add BOS,EOS vocab\n",
    "    tokenizer = {\n",
    "        \"url\": \"s3://skt-lsl-nlp-model/KoBERT/tokenizers/kobert_news_wiki_ko_cased-1087f8699e.spiece\",\n",
    "        #\"fname\": \"/opt/ml/SumAI/bertsum/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\",\n",
    "        \"chksum\": \"ae5711deb3\",\n",
    "    }\n",
    "    \n",
    "    vocab_info = tokenizer\n",
    "    vocab_file = _download(\n",
    "        #vocab_info[\"url\"], vocab_info[\"fname\"], vocab_info[\"chksum\"], cachedir=cachedir\n",
    "        vocab_info[\"url\"], vocab_info[\"chksum\"], cachedir=cachedir\n",
    "    )\n",
    "\n",
    "    vocab_b_obj = nlp.vocab.BERTVocab.from_sentencepiece(\n",
    "        vocab_file[0], padding_token=\"[PAD]\", bos_token=\"[BOS]\", eos_token=\"[EOS]\"\n",
    "    )\n",
    "    return vocab_b_obj\n",
    "\n",
    "    \n",
    "class BertData():\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        #self.tokenizer = Tokenizer(vocab_file_path)\n",
    "        self.sep_vid = self.tokenizer.vocab['[SEP]']\n",
    "        self.cls_vid = self.tokenizer.vocab['[CLS]']\n",
    "        self.pad_vid = self.tokenizer.vocab['[PAD]']\n",
    "\n",
    "    def preprocess(self, src):\n",
    "\n",
    "        if (len(src) == 0):\n",
    "            return None\n",
    "\n",
    "        original_src_txt = [''.join(s) for s in src]\n",
    "\n",
    "\n",
    "        idxs = [i for i, s in enumerate(src) if (len(s) > 0)]\n",
    "\n",
    "        src = [src[i][:20000] for i in idxs]\n",
    "        src = src[:10000]\n",
    "\n",
    "        if (len(src) < 3):\n",
    "            return None\n",
    "\n",
    "        src_txt = [''.join(sent) for sent in src]\n",
    "        text = ' [SEP] [CLS] '.join(src_txt)\n",
    "        src_subtokens = text.split(' ')\n",
    "        src_subtokens = src_subtokens[:510]\n",
    "        src_subtokens = ['[CLS]'] + src_subtokens + ['[SEP]']\n",
    "\n",
    "        src_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(src_subtokens)\n",
    "        _segs = [-1] + [i for i, t in enumerate(src_subtoken_idxs) if t == self.sep_vid]\n",
    "        segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]\n",
    "        segments_ids = []\n",
    "        for i, s in enumerate(segs):\n",
    "            if (i % 2 == 0):\n",
    "                segments_ids += s * [0]\n",
    "            else:\n",
    "                segments_ids += s * [1]\n",
    "        cls_ids = [i for i, t in enumerate(src_subtoken_idxs) if t == self.cls_vid]\n",
    "        labels = None\n",
    "        tgt_txt = None\n",
    "        src_txt = [original_src_txt[i] for i in idxs]\n",
    "        return src_subtoken_idxs, labels, segments_ids, cls_ids, src_txt, tgt_txt\n",
    "    \n",
    "def convert_to_unicode(text):\n",
    "    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
    "    if six.PY3:\n",
    "        if isinstance(text, str):\n",
    "            return text\n",
    "        elif isinstance(text, bytes):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    elif six.PY2:\n",
    "        if isinstance(text, str):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        elif isinstance(text, unicode):\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    else:\n",
    "        raise ValueError(\"Not running on Python2 or Python 3?\")\n",
    "        \n",
    "class Tokenizer(object):\n",
    "    \n",
    "    def __init__(self, vocab_file_path):\n",
    "        self.vocab_file_path = vocab_file_path\n",
    "        \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "        vocab = collections.OrderedDict()\n",
    "        index = 0\n",
    "        with open(self.vocab_file_path, \"r\", encoding='utf-8') as reader:\n",
    "\n",
    "            while True:\n",
    "                token = convert_to_unicode(reader.readline())\n",
    "                if not token:\n",
    "                    break\n",
    "\n",
    "          ### joonho.lim @ 2019-03-15\n",
    "                if token.find('n_iters=') == 0 or token.find('max_length=') == 0 :\n",
    "\n",
    "                    continue\n",
    "                token = token.split('\\t')[0].strip('_')\n",
    "\n",
    "                token = token.strip()\n",
    "                vocab[token] = index\n",
    "                index += 1\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        \"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                ids.append(self.vocab[token])\n",
    "            except:\n",
    "                ids.append(1)\n",
    "        if len(ids) > 10000:\n",
    "            raise ValueError(\n",
    "                \"Token indices sequence length is longer than the specified maximum \"\n",
    "                \" sequence length for this BERT model ({} > {}). Running this\"\n",
    "                \" sequence through BERT will result in indexing errors\".format(len(ids), 10000)\n",
    "            )\n",
    "        return ids\n",
    "\n",
    "\n",
    "def _lazy_dataset_loader(pt_file):\n",
    "    dataset = pt_file    \n",
    "    yield dataset\n",
    "    \n",
    "def News_to_input(text, openapi_key, tokenizer):\n",
    "    newstemp = do_lang(openapi_key, text)\n",
    "    news = newstemp.split(' ./SF ')[:-1]\n",
    "    \n",
    "    bertdata = BertData(tokenizer)\n",
    "    sent_labels = [0] * len(news)\n",
    "    tmp = bertdata.preprocess(news)\n",
    "    if(not tmp): return None\n",
    "    #print(tmp)\n",
    "    b_data_dict = {\"src\":tmp[0],\n",
    "               \"tgt\": [0],\n",
    "               \"labels\":[0,0,0],\n",
    "               \"src_sent_labels\":sent_labels,\n",
    "               \"segs\":tmp[2],\n",
    "               \"clss\":tmp[3],\n",
    "               \"src_txt\":tmp[4],\n",
    "               \"tgt_txt\":'hehe'}\n",
    "    b_list = []\n",
    "    b_list.append(b_data_dict) \n",
    "    return b_list\n",
    "\n",
    "openapi_key = '9318dc23-24ac-4b59-a99e-a29ec170bf02'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "39abaf4d-64a5-40eb-8784-e94d995941a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-23 12:41:19,747 INFO] loading archive file ../001_bert_morp_pytorch\n",
      "[2023-01-23 12:41:19,748 INFO] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30349\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /opt/ml/KorBertSum/src/./tmp/kobert_news_wiki_ko_cased-1087f8699e.spiece\n",
      "using cached model. /opt/ml/KorBertSum/src/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "checkpoint = torch.load('../models/bert_classifier2/model_step_35000.pt', map_location=lambda storage, loc: storage)\n",
    "config = BertConfig.from_json_file(args.bert_config_path)\n",
    "model = Summarizer(args, device, load_pretrained_bert=False, bert_config = config)\n",
    "model.load_cp(checkpoint)\n",
    "model.eval()  \n",
    "\n",
    "def add_topk_to_df(df, model, tokenizer):\n",
    "    \n",
    "    start = time.time()\n",
    "    topk = []\n",
    "    for i,context in enumerate(tqdm(df['context2'])):\n",
    "        context = eval(context)[:30]\n",
    "        top = None\n",
    "        if(len(context) > 3):\n",
    "            top = get_top_sentences(' '.join(context), model, tokenizer)\n",
    "        if(top):\n",
    "            topk.append(top)\n",
    "        else:\n",
    "            topk.append('Hello world')\n",
    "            \n",
    "    df['topk'] = topk\n",
    "    df = df.drop(df[df['topk'] == 'Hello world'].index)\n",
    "    print(f\"{time.time()-start:.4f} sec\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "87764c16-096a-4d91-ab95-c1209759385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_context_to_df(df):\n",
    "    contexts = []\n",
    "    for i, cont in enumerate(tqdm(df['raw'])):\n",
    "        if(cont):\n",
    "            contexts.append(text_filter(cont))\n",
    "        else:\n",
    "            contexts.append('delete this')\n",
    "    df['context2'] = contexts\n",
    "    df = df.drop(df[df['context2'] == 'delete this'].index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "54cf8690-0993-4e1f-9136-bdec22da7c2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './preprocessed/kmeans_4/kmeans_4_부동산_20221201_20221203_crwal_news_context_raw.csv_pre.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-311-e63e4880aa8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./preprocessed/kmeans_4/kmeans_4_부동산_20221201_20221203_crwal_news_context_raw.csv_pre.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/python3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './preprocessed/kmeans_4/kmeans_4_부동산_20221201_20221203_crwal_news_context_raw.csv_pre.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"./preprocessed/kmeans_4/kmeans_4_부동산_20221201_20221203_crwal_news_context_raw.csv_pre.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "eb971cce-9f80-4aef-9e35-19346952149d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 18/88 [00:10<00:42,  1.66it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-319-0006bacc4353>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_topk_to_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-316-28752ae01088>\u001b[0m in \u001b[0;36madd_topk_to_df\u001b[0;34m(df, model, tokenizer)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mtop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_top_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mtopk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-283-2c4b76cce2db>\u001b[0m in \u001b[0;36mget_top_sentences\u001b[0;34m(user_input, model, tokenizer)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mbot_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNews_to_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopenapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mchat_history_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbot_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mpred_lst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat_history_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfinal_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-179-e83c79663f8d>\u001b[0m in \u001b[0;36msummary\u001b[0;34m(args, b_list, device_id, pt, step, model)\u001b[0m\n\u001b[1;32m    362\u001b[0m                                   shuffle=False, is_test=True)\n\u001b[1;32m    363\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-179-e83c79663f8d>\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, test_iter, step, cal_lead, cal_oracle)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m                 \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/KorBertSum/src/models/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_dataset_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/KorBertSum/src/models/data_loader.py\u001b[0m in \u001b[0;36m_next_dataset_iterator\u001b[0;34m(self, dataset_iter)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cur_dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df2 = add_topk_to_df(df, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ce7fa5-4533-44e8-9d88-f89e3890c8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ‘세종 에버파크’가 지난달 25일부터 전국 만 19세 이상 수요자들로부터 선착순으로 발기인 접수를 받고 있다. \n",
      "1 특히 최근 오픈한 사업설명회장이 문전성시를 이루면서, 가파른 금리 인상 등으로 인해 침체된 부동산 시장과는 대비되는 모습이다. \n",
      "2 세종시 연기면 일원에 위치한 ‘세종 에버파크’는 지하 2층~지상 37층, 24개 동, 전용면적 74·84㎡, 총 3,012세대 규모의 대단지다. \n",
      "3 시공은 대형 건설사인 현대건설로 예정됐다. \n",
      "4 ‘세종 에버파크’는 그동안 세종시에서 보기 드물었던 3,012세대의 대단지로 공급된다는 소식에 수요자들의 꾸준한 관심이 이어졌다. \n",
      "5 지난 18일 사업설명회장 오픈 당시에는 입장하려는 방문객들의 긴 줄이 늘어져 있었으며, 세종시 무주택자 및 세종시 소재 직장인 무주택자들을 대상으로 진행한 발기인 가입 우선 접수도 성황리에 마감됐다. \n",
      "6 ‘세종 에버파크’는 장기일반 민간임대주택과 공공지원 민간임대주택 2가지 방식으로 조성된다. \n",
      "7 전체 3,000여 세대 중 먼저 50% 이하는 장기일반 민간임대주택으로 발기인을 모집한다. \n",
      "8 이어 촉진지구 지정 후 전체 세대의 50% 이상은 공공지원 민간임대주택으로 공급할 예정이다. \n",
      "9 장기일반 민간임대주택 발기인은 청약 통장 보유 여부에 관계없이 신청할 수 있고, 재당첨 제한도 받지 않는다. \n",
      "10 다주택자도 신청할 수 있으며, 지위권 보유 기간 동안 취득세·종합부동산세 등 세금에 대한 부담이 없다. \n",
      "11 초기 출자금 완납 후에는 주택 소유 여부와 관계없이 지위권 전매가 가능하다. \n",
      "12 또한 사업 승인 후에는 매매 전환 합의를 통해 10년간 확정 전세가로 임대료 인상 없이 내 집처럼 살다가 10년 후 분양 전환할 수 있다. \n",
      "13 ‘세종 에버파크’는 상품성도 뛰어나다. \n",
      "14 먼저 4Bay, 판상형 구조 중심으로 설계된다. \n",
      "15 판상형 아파트는 방과 거실이 전면에 배치돼, 집안 내부에 해가 잘 들어오고 환기가 우수하다. \n",
      "16 발코니 확장 시 서비스 면적이 크게 늘어나 보다 넓게 공간을 쓸 수 있다. \n",
      "17 4Bay 구조도 강점이다. \n",
      "18 방 3칸과 거실이 전면 향으로 배치되는 4Bay는 모든 방과 거실에 골고루 햇빛이 잘 들고 창이 넓어 바람도 잘 통한다. \n",
      "19 또 일조량이 풍부하고, 통풍도 수월하다. \n",
      "20 여기에 단지 내에는 입주민을 배려한 다양한 특화시설이 들어선다. \n",
      "21 반려동물과 여가를 즐길 수 있는 펫센터, 비가와도 놀 수 있는 필로티 하부의 놀이마당이 조성될 예정이다.에 전용 정차 구역인 드롭오프존을 설계했다. \n",
      "22 사업 관계자는 “지난 10월 공급촉진지구 수용 알림을 받은 상황이며, 세종시의 의견을 적극적으로 수렴해 인허가 절차를 밟고 있는 중”이라며 “또한 지난달 유치가 확정된 2027회 숙소로 제공을 검토 중에 있는 등 지역 발전에도 신경을 쓰고 있다”고 전했다. \n",
      "23 이어 “여기에 주택 사업 통합 심의 의무화 적용 사업지로 원활한 사업 진행이 기대되고 있으며, 실제로 세종시 발기인 우선 모집에 많은 수요자가 몰리며 빠르게 마감됐고, 일반 발기인 접수도 얼마 되지 않아 마감될 것으로 예상된다”고 덧붙였다. \n",
      "24 ‘세종 에버파크’는 사업설명회장을 운영 중이며, 홈페이지에서는 유니트를 360도로 촬영한 VR(가상현실) 영상 및 모형도, 단지 배치, 입지 환경 등 상세 정보를 확인할 수 있다. \n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(eval(df2.context2[3])):\n",
    "    print(i,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73027f2a-55c5-4fd6-9a7d-81125eb401c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(17, ' 4Bay 구조도 강점이다. '),\n",
       " (16, ' 발코니 확장 시 서비스 면적이 크게 늘어나 보다 넓게 공간을 쓸 수 있다. '),\n",
       " (5,\n",
       "  ' 지난 18일 사업설명회장 오픈 당시에는 입장하려는 방문객들의 긴 줄이 늘어져 있었으며, 세종시 무주택자 및 세종시 소재 직장인 무주택자들을 대상으로 진행한 발기인 가입 우선 접수도 성황리에 마감됐다. '),\n",
       " (8, ' 이어 촉진지구 지정 후 전체 세대의 50% 이상은 공공지원 민간임대주택으로 공급할 예정이다. '),\n",
       " (6, ' ‘세종 에버파크’는 장기일반 민간임대주택과 공공지원 민간임대주택 2가지 방식으로 조성된다. '),\n",
       " (10, ' 다주택자도 신청할 수 있으며, 지위권 보유 기간 동안 취득세·종합부동산세 등 세금에 대한 부담이 없다. '),\n",
       " (14, ' 먼저 4Bay, 판상형 구조 중심으로 설계된다. '),\n",
       " (13, ' ‘세종 에버파크’는 상품성도 뛰어나다. '),\n",
       " (15, ' 판상형 아파트는 방과 거실이 전면에 배치돼, 집안 내부에 해가 잘 들어오고 환기가 우수하다. '),\n",
       " (4,\n",
       "  ' ‘세종 에버파크’는 그동안 세종시에서 보기 드물었던 3,012세대의 대단지로 공급된다는 소식에 수요자들의 꾸준한 관심이 이어졌다. '),\n",
       " (7, ' 전체 3,000여 세대 중 먼저 50% 이하는 장기일반 민간임대주택으로 발기인을 모집한다. '),\n",
       " (12,\n",
       "  ' 또한 사업 승인 후에는 매매 전환 합의를 통해 10년간 확정 전세가로 임대료 인상 없이 내 집처럼 살다가 10년 후 분양 전환할 수 있다. '),\n",
       " (2,\n",
       "  ' 세종시 연기면 일원에 위치한 ‘세종 에버파크’는 지하 2층~지상 37층, 24개 동, 전용면적 74·84㎡, 총 3,012세대 규모의 대단지다. '),\n",
       " (9, ' 장기일반 민간임대주택 발기인은 청약 통장 보유 여부에 관계없이 신청할 수 있고, 재당첨 제한도 받지 않는다. '),\n",
       " (3, ' 시공은 대형 건설사인 현대건설로 예정됐다. '),\n",
       " (11, ' 초기 출자금 완납 후에는 주택 소유 여부와 관계없이 지위권 전매가 가능하다. '),\n",
       " (0, '‘세종 에버파크’가 지난달 25일부터 전국 만 19세 이상 수요자들로부터 선착순으로 발기인 접수를 받고 있다. '),\n",
       " (1,\n",
       "  ' 특히 최근 오픈한 사업설명회장이 문전성시를 이루면서, 가파른 금리 인상 등으로 인해 침체된 부동산 시장과는 대비되는 모습이다. ')]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.topk[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53709537-e192-4aef-8cb9-dda8f556fdcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, '\\n가토 장관은 “다케시마(竹島·일본이 주장하는 독도의 명칭)는 역사적 사실에 비춰봐도, 국제법상으로도 명백한 일본 고유의 영토”라며 독도 영유권 주장을 되풀이했다'), (4, '\\n가토 장관은 한일 정상회담에 대한 일본 정부의 자세에 대해 “그런 사실이 없기 때문에 가정의 질문에 대해 답하는 것을 삼가겠다”고 말했다'), (2, '\\n앞서 요미우리는 한국 측이 도쿄올림픽을 계기로 한 문 대통령의 방일을 타진했고, 일본 측은 수용하는 방향이라고 이날 보도했다'), (3, '\\n한국 측은 문 대통령의 방일 때 스가 요시히데(菅義偉) 총리와 처음으로 정상회담을 하겠다는 생각이라고 요미우리는 전했다'), (0, '\\n일본 정부가 문재인 대통령이 도쿄올림픽을 계기로 일본을 방문하는 방향으로 한일 양국이 조율하고 있다는 15일 요미우리신문의 보도를 부인했다'), (1, '\\n정부 대변인인 가토 가쓰노부(加藤勝信) 관방장관은 이날 오전 정례 기자회견에서 관련 질문에 “말씀하신 보도와 같은 사실이 없는 것으로 안다”고 밝혔다'), (5, '\\n그는 한국 측의 독도방어훈련에 ‘어떤 대항 조치를 생각하고 있느냐’는 질문에는 “한국 해군의 훈련에 대해 정부로서는 강한 관심을 가지고 주시하는 상황이어서 지금 시점에선 논평을 삼가겠다”고 말을 아꼈다'), (7, '\\n그러면서 “다케시마 문제에 대해서는 계속 우리나라의 영토, 영해, 영공을 단호히 지키겠다는 결의로 냉정하고 의연하게 대응해갈 생각”이라고 밝혔다')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "\n",
    "print(get_top_sentences(user_input, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "efbd0615-cc6f-449e-933b-8fa8d312054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_paths = [\n",
    " 'bertopic(mpnet)',\n",
    " 'bertopic(paraphrase)',\n",
    " 'kmeans_12',\n",
    " 'hdbscan',\n",
    " 'kmeans_8',\n",
    " 'bertopic(SR-BERT)',\n",
    " 'kmeans_4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "53bcf1d7-bb0e-40a0-95c3-b08b978c370b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./bertopic(paraphrase)\n",
      "['bertopic_윤석열_20221201_20221231_crwal_news_context_raw.csv', 'bertopic_부동산_20221201_20221203_crwal_news_context_raw.csv', 'bertopic_삼성전자_20221201_20221215_crwal_news_context_raw.csv', 'bertopic_윤석열_20221201_20221203_crwal_news_context_raw.csv', 'bertopic_부동산_20221201_20221231_crwal_news_context_raw.csv', 'bertopic_삼성전자_20221201_20221231_crwal_news_context_raw.csv', 'bertopic_삼성전자_20221201_20221203_crwal_news_context_raw.csv', 'bertopic_윤석열_20221201_20221215_crwal_news_context_raw.csv', 'bertopic_부동산_20221201_20221215_crwal_news_context_raw.csv']\n",
      "0 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/111 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'text_filter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-cb52362ee9f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mnew_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_context_to_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'./preprocessed/{mypath}/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'./preprocessed/{mypath}/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-4dca5d75e9e1>\u001b[0m in \u001b[0;36madd_context_to_df\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcont\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'raw'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0mcontexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mcontexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'delete this'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_filter' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "mypath = f'./{all_paths[6]}'\n",
    "onlyfiles = [f for f in listdir(mypath+'/raw') if isfile(join(mypath+'/raw', f)) and f.split('.')[1] == 'csv']\n",
    "onlyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfba0711-4823-41a5-b4bc-4876aee48644",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,file in enumerate(onlyfiles):\n",
    "    filepath = f'./preprocessed/{mypath}/{file}_pre.csv'\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(i, len(df))\n",
    "    new_df = add_topk_to_df(df, model, tokenizer)\n",
    "    if not os.path.exists(f'./preprocessed2/{mypath}/'):\n",
    "        os.makedirs(f'./preprocessed2/{mypath}/')\n",
    "    new_df.to_csv(f'./preprocessed2/{mypath}/{file}_pre2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "e0f08e33-2c23-41c3-a086-a09e791ab07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,file in enumerate(onlyfiles):\n",
    "    filepath = f'{mypath}/raw/{onlyfiles[i]}'\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(i, len(df))\n",
    "    new_df = add_context_to_df(df)\n",
    "    if not os.path.exists(f'./preprocessed/{mypath}/'):\n",
    "        os.makedirs(f'./preprocessed/{mypath}/')\n",
    "    new_df.to_csv(f'./preprocessed/{mypath}/{file}_pre.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "62c14bb9-1116-464d-955b-1651577feb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "def clean_byline(text):\n",
    "    # byline\n",
    "    pattern_email = re.compile(r'[-_0-9a-z]+@[-_0-9a-z]+(?:\\.[0-9a-z]+)+', flags=re.IGNORECASE)\n",
    "    pattern_url = re.compile(r'(?:https?:\\/\\/)?[-_0-9a-z]+[^~][^%](?:\\.[-_0-9a-z]+)+[^%]', flags=re.IGNORECASE)\n",
    "    pattern_others = re.compile(r'\\[[^\\]]*[◼|기자|뉴스|사진|자료|자료사진|출처|특파원|교수|작가|대표|논설|고문|주필|부문장|팀장|장관|원장|연구원|이사장|위원|실장|차장|부장|에세이|화백|사설|소장|단장|과장|기획자|경제|한겨례|일보|미디어|데일리|한겨례|타임즈|위키트리|큐레이터|저작권|평론가|©|©|ⓒ|\\@|\\/|=|▶|무단|전재|재배포|금지][^\\]]*?\\][ 제공]*')\n",
    "    pattern_tag = re.compile(r'\\<[^>][^가-힣 ]*?>')\n",
    "    result = pattern_email.sub('', text)\n",
    "    result = pattern_url.sub('', result)\n",
    "    result = pattern_others.sub('', result)\n",
    "    result = pattern_tag.sub('', result)\n",
    "\n",
    "    # 본문 시작 전 꺽쇠로 쌓인 바이라인 제거\n",
    "    pattern_bracket = re.compile(r'^((?:\\[.+\\])|(?:【.+】)|(?:<.+>)|(?:◆.+◆)\\s)')\n",
    "    result = pattern_bracket.sub('', result).strip()\n",
    "    return result\n",
    "\n",
    "\n",
    "def text_filter(text): # str -> 전처리 -> 문장 배열\n",
    "    text = clean_byline(text)\n",
    "    #exclude_pattern = re.compile(r'[^\\% 0-9a-zA-Zㄱ-ㅣ가-힣.,]+')\n",
    "    #exclusions = exclude_pattern.findall(text)\n",
    "    #result = exclude_pattern.sub(' ', text).strip()\n",
    "    #spacing = Spacing()\n",
    "    #kospacing_txt = spacing(result) \n",
    "    result = text.strip()\n",
    "    sentences = sent_tokenize(result) \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f4fc1fb2-099b-422a-80dd-2fff171e37cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['신혜진 기자 어쩌구저쩌구 (2%~3&) 뉴스 제목 오늘부터 마스크 해제 책 이름 <안녕하세요> 저는 신혜진입니다']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text = \"신혜진 기자 어쩌구저쩌구 (2%~3&) 뉴스 제목 [차민수 기자] <tag>오늘</tag>부터 <p>마스크 해제</p> 책 이름 <안녕하세요> 저는 신혜진입니다 [연합뉴스] 제공\"\n",
    "#text = \"여야는 앞서 종부세 중과세율(1.2~6.0%)을 적용하는 조정대상지역 2주택 이상 소유자에 대해 중과를 폐지하고 일반세율(0.5~2.7%)을 적용하는 방안 등을 담은 잠정 합의안을 도출한 상태다. 종부세법 개정안은 예산안이 처리되는대로 국회 본회의에 오를 것으로 보인다. \"\n",
    "#text = '''서대구 역세권 사업 ‘공공주도로 개발’ 뉴스9(대구) 입력 2022.12.01 (21:47) 수정 2022.12.01 (22:05)\\n\\n댓글\\n\\n좋아요\\n\\n공유하기 글씨 크게보기\\n\\n가\\n\\n글씨 작게보기\\n\\n고화질 표준화질 자동재생 키보드 컨트롤 안내 동영상영역 시작 동영상 시작 동영상영역 끝 동영상 고정 취소 이전기사 이전기사 다음기사 다음기사\\n\\n고화질 표준화질 자동재생 키보드 컨트롤 안내 동영상영역 시작 동영상 시작 동영상영역 끝 동영상설명 동영상 고정 취소\\n\\n[앵커]\\n\\n\\n\\n대구시가 서대구 역세권 개발 방식을 민·관 공동추진에서 공공주도로 바꾸기로 했습니다.'''\n",
    "\n",
    "text_filter(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3630e556-e8f6-4258-9577-c58bb68a2b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[차민수 기자]', '[연합뉴스]']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pattern_others2 = re.compile(r'\\<[^가-힣][^>]*?\\>')\n",
    "#pattern_others2 = re.compile(r'\\[.[^]]*?\\]')\n",
    "pattern_others = re.compile(r'\\[[^\\]]*[기자|사진|자료|자료사진|출처|특파원|교수|작가|대표|논설|고문|주필|부문장|팀장|장관|원장|연구원|이사장|위원|실장|차장|부장|에세이|화백|사설|소장|단장|과장|기획자|큐레이터|저작권|평론가|©|©|ⓒ|\\@|\\/|=|▶|무단|전재|재배포|금지][^\\]]*?\\]')\n",
    "\n",
    "re.findall(pattern_others, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33df40e3-6b1f-420f-9c58-93e5120fc812",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "\n",
    "path = 'hdbscan'\n",
    "onlyfiles = [f for f in listdir(path+'/raw') if isfile(join(path+'/raw', f)) and f.split('.')[1] == 'csv']\n",
    "\n",
    "\n",
    "filepath = f'{path}/raw/{onlyfiles[i]}'\n",
    "df = pd.read_csv(filepath)\n",
    "raw = df.raw[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e54d88d1-7200-494d-972a-89b8aa992c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'외지인 아파트 매입 두달새 5.8%P·1년새 17%P 하락\\n\\n서울거주자 춘천아파트 매입 한 달간 7건 사상 최저\\n\\n지방 부동산 활성화 위한 취득세 중과 해제 시급해져\\n\\n◇1일부터 시행되는 LTV·주택담보대출 완화 내용<제공=연합뉴스>\\n\\n수도권을 포함한 전국적인 부동산 규제지역 해제 이후 강원도내 아파트에 대한 외지인 투자가 급감한 것으로 나타났다.\\n\\n한국부동산원에 따르면 10월 기준 도내 아파트 매매거래 884건 중 외지인이 아파트를 매입한 거래는 245건, 27.7%로 조사됐다. 규제지역 해제 직전인 지난 8월에는 이 비율이 33.5%였다. 외지인 투자가 활발했던 지난해 10월 도내 아파트 매매거래 2,742건 중 외지인 매입 비율이 45.4%(1,246건)였던 점을 감안하면 1년 새 17.7%포인트나 하락한 셈이다.\\n\\n외지인들이 지난 8월까지 비규제지역인 강원도의 아파트를 활발하게 매입했지만 정부의 수도권 규제지역 해제가 본격화된 9월부터 그 수치가 뚝 떨어진 것.\\n\\n지역별로는 수도권과 가까운 춘천과 원주에서 외지인 매입 감소 폭이 컸다. 춘천지역 아파트를 서울 거주자가 산 경우는 지난해 10월 93건에서 올해 10월 7건으로 92.5% 급감했다. 춘천에서 월 단위로 서울 거주자 매입이 10건 이하를 기록한 것은 통계 작성이 시작된 2006년 1월 이후 사상 처음이다. 원주의 경우 같은 기간 서울 거주자 매입은 132건에서 10건으로 92.4%나 줄었다. 지난해 외지인 매입 비율이 전국 1위를 기록할 정도로 높았던 만큼 하락도 가팔랐다.\\n\\n토지 거래에서도 외지인 이탈이 두드러졌다. 지난 10월 강원지역 순수토지 거래(필지 수 기준) 중 외지인은 2,030건을 매입해 전년동월(3,532건) 대비 42.5% 감소했다. 같은 기간 도내 거주자가 매입한 건수는 15.4% 감소에 그쳤다.\\n\\n규제지역은 지난 9월부터 차례로 해제되는 추세로 최근에는 서울, 경기 일부를 제외한 모든 지역을 규제대상에서 해제했다. 더욱이 1일부터 15억원 초과 아파트에도 주택담보대출이 허용되는 등 추가 완화 방안이 본격화되면서 도내 외지인 매입 규모는 더욱 줄어들 전망이다.\\n\\n이처럼 외지인들의 도내 아파트와 토지 매입이 줄어들 것으로 예상되자 지방 부동산의 취득세 중과 해제 등을 요구하는 목소리가 높아지고 있다.\\n\\n강문식 한국공인중개사협회 춘천시지회장은 “수도권 규제지역 해제로 외지 투자자들이 지방으로 내려올 이유가 사라졌다”면서 “지역 부동산 활성화를 위해선 취득세 중과 해제가 시급한 만큼 지자체가 나서서 정부를 설득할 필요가 있다”고 말했다.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "5b571695-3415-4645-9f0a-d9395eaf78e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['수도권을 포함한 전국적인 부동산 규제지역 해제 이후 강원도내 아파트에 대한 외지인 투자가 급감한 것으로 나타났다. ',\n",
       " '한국부동산원에 따르면 10월 기준 도내 아파트 매매거래 884건 중 외지인이 아파트를 매입한 거래는 245건, 27.7%로 조사됐다. ',\n",
       " '규제지역 해제 직전인 지난 8월에는 이 비율이 33.5%였다. ',\n",
       " '외지인 투자가 활발했던 지난해 10월 도내 아파트 매매거래 2,742건 중 외지인 매입 비율이 45.4%(1,246건)였던 점을 감안하면 1년 새 17.7%포인트나 하락한 셈이다. ',\n",
       " '지역별로는 수도권과 가까운 춘천과 원주에서 외지인 매입 감소 폭이 컸다. ',\n",
       " '춘천지역 아파트를 서울 거주자가 산 경우는 지난해 10월 93건에서 올해 10월 7건으로 92.5% 급감했다. ',\n",
       " '춘천에서 월 단위로 서울 거주자 매입이 10건 이하를 기록한 것은 통계 작성이 시작된 2006년 1월 이후 사상 처음이다. ',\n",
       " '원주의 경우 같은 기간 서울 거주자 매입은 132건에서 10건으로 92.4%나 줄었다. ',\n",
       " '지난해 외지인 매입 비율이 전국 1위를 기록할 정도로 높았던 만큼 하락도 가팔랐다. ',\n",
       " '토지 거래에서도 외지인 이탈이 두드러졌다. ',\n",
       " '지난 10월 강원지역 순수토지 거래(필지 수 기준) 중 외지인은 2,030건을 매입해 전년동월(3,532건) 대비 42.5% 감소했다. ',\n",
       " '같은 기간 도내 거주자가 매입한 건수는 15.4% 감소에 그쳤다. ',\n",
       " '규제지역은 지난 9월부터 차례로 해제되는 추세로 최근에는 서울, 경기 일부를 제외한 모든 지역을 규제대상에서 해제했다. ',\n",
       " '더욱이 1일부터 15억원 초과 아파트에도 주택담보대출이 허용되는 등 추가 완화 방안이 본격화되면서 도내 외지인 매입 규모는 더욱 줄어들 전망이다. ',\n",
       " '이처럼 외지인들의 도내 아파트와 토지 매입이 줄어들 것으로 예상되자 지방 부동산의 취득세 중과 해제 등을 요구하는 목소리가 높아지고 있다. ',\n",
       " '강문식 한국공인중개사협회 춘천시지회장은 “수도권 규제지역 해제로 외지 투자자들이 지방으로 내려올 이유가 사라졌다”면서 “지역 부동산 활성화를 위해선 취득세 중과 해제가 시급한 만큼 지자체가 나서서 정부를 설득할 필요가 있다”고 말했다. ']"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_filter(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e77592f7-566a-41c5-82e9-4ba6a1b5f910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['페이지\\n\\n윤 대통령, 제1차 국정과제 점검회의 다주택자 세 중과 해제 검토 주택공급, 공공·민간 믹스…이달말 공공주택 50만가구 사전청약\\n\\n윤석열 대통령은 15일 국정과제 점검회의에서 규제 완화와 다주택자 세 중과 해제 검토,주택공급, 공공·민간 믹스 등 3대개혁 청사진을 제시했다.',\n",
       " '윤 대통령은 이날 오후 2시부터 청와대 영빈관에서 열린 제1차 국정과제 점검회의에서 국민 패널 2명으로부터 ‘내 집 마련’과 ‘부동산 경기 활성화’ 방안과 관련한 질문을 받고 부동산 시장 연착륙을 위한 규제 완화 필요성을 강조했다.',\n",
       " '전 정부에서 강화된 부동산 규제를 일제히 풀 경우 부작용을 우려해 그간 속도 조절을 해왔지만, 고금리 등으로 상황이 급변한 시장 대응을 위해 다시 규제 완화 고삐를 죄겠다고 했다.이같은 발언은 규제완화를 통한 부동산 시장을 활성화시키겠다는 것으로 풀이된다.',\n",
       " '불합리한 복합 규제 때문…규제 완화 ‘속도’\\n\\n윤 대통령은 “제가 정부를 맡기 전까지 공급 측면과 수요 측면의 불합리한 복합 규제 때문에 집값이 너무 천정부지로 솟고 거래물량이 위축됐다”며 “시장 정상화가 중요하다고 해서 많은 규제를 풀고 정상화하려고 했는데, 고금리 상황 때문에 다시 부동산 가격이 하락하는 추세를 보였다”고 진단했다.',\n",
       " '그러면서 “(전 정부의) 잘못된 정책으로 인한 현상이라고 해도, 그것을 일시에 제거하다 보면 시장에 혼란이 일어나서 결국 국민에게 불편을 줄 수 있어서 시장 정상화의 속도를 조율해야 한다고 생각했다”며 수요 규제를 조금 더 빠른 속도로 풀어나가서 시장이 안정 찾는데 최선의 노력할 것이라고 강조했다.',\n",
       " '이 같은 발언은 최근 고금리와 집값 하락 우려로 거래가 메마르면서 곳곳에서 ‘경고음’이 터지는 데 따른 것으로 풀이된다.',\n",
       " '내놓은 매물이 나가지 않아 이사를 못가는 상황이 발생하거나, ‘영끌’ 매입자들의 금리 부담 급증, 집값 하락세에 따른 보유세 부담 호소, 빠른 월세화 현상 등이다.',\n",
       " '다주택자 세 중과 해제 검토…주택공급, 공공·민간 믹스\\n\\n종합부동산세의 경우 국회에서 관련법 개정 논의가 이어지고 있다.',\n",
       " '여야는 앞서 종부세 중과세율(~%)을 적용하는 조정대상지역 2주택 이상 소유자에 대해 중과를 폐지하고 일반세율(~%)을 적용하는 방안 등을 담은 잠정 합의안을 도출한 상태다.',\n",
       " '종부세법 개정안은 예산안이 처리되는대로 국회 본회의에 오를 것으로 보인다.',\n",
       " '서울과 경기 4곳만 남은 규제지역의 추가 해제 여부 역시 시장의 관심사다.',\n",
       " '윤 대통령은 이날 주택공급계획과 관련해 “민간과 공공임대를 잘 믹스해서(섞어서) 공급하려고 한다”고 말했다.',\n",
       " '이어 “공공임대주택 많이 지어서 공급하다 보면 중앙정부나 지방정부가 상당한 재정부담을 안게 된다”며 “납세자에게도 굉장히 큰 부담이 되고 전반적으로 우리 경제에 부담 요인이나 경기 위축 요인으로 작용할 수 있다”고 설명했다.',\n",
       " '한편 정부가 임기 내 공급을 약속한 270만가구 중 50만가구에 해당하는 공공주택 사전청약이 이달 말부터 실시된다.은 이날 회의에서 “무주택 서민과 젊은 세대에게 희망을 갖고 내 집 마련을 포기하지 말라고 (지속적으로) 공급할 것”이라며 “이번달 말부터 (공공주택) 사전청약을 받으니 관심이 있는 분들은 이용해주시라”고 당부했다.',\n",
       " '경기일보(),및 수집,']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_filter(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b0500f0-020c-4c16-89d6-5578dc712a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['강해인 기자  기자페이지 윤 대통령, 제1차 국정과제 점검회의 다주택자 세 중과 해제 검토 주택공급, 공공 민간 믹스 이달말 공공주택 50만가구 사전청약 윤석열 대통령은 15일 국정과제 점검회의에서 규제 완화와 다주택자 세 중과 해제 검토,주택공급, 공공 민간 믹스 등 3대개혁 청사진을 제시했다.',\n",
       " '윤 대통령은 이날 오후 2시부터 청와대 영빈관에서 열린 제1차 국정과제 점검회의에서 국민 패널 2명으로부터  내 집 마련 과  부동산 경기 활성화  방안과 관련한 질문을 받고 부동산 시장 연착륙을 위한 규제 완화 필요성을 강조했다.',\n",
       " '전 정부에서 강화된 부동산 규제를 일제히 풀 경우 부작용을 우려해 그간 속도 조절을 해왔지만, 고금리 등으로 상황이 급변한 시장 대응을 위해 다시 규제 완화 고삐를 죄겠다고 했다.이같은 발언은 규제완화를 통한 부동산 시장을 활성화시키겠다는 것으로 풀이된다.',\n",
       " '불합리한 복합 규제 때문 규제 완화  속도 윤 대통령은  제가 정부를 맡기 전까지 공급 측면과 수요 측면의 불합리한 복합 규제 때문에 집값이 너무 천정부지로 솟고 거래물량이 위축됐다 며  시장 정상화가 중요하다고 해서 많은 규제를 풀고 정상화하려고 했는데, 고금리 상황 때문에 다시 부동산 가격이 하락하는 추세를 보였다 고 진단했다.',\n",
       " '그러면서  전 정부의  잘못된 정책으로 인한 현상이라고 해도, 그것을 일시에 제거하다 보면 시장에 혼란이 일어나서 결국 국민에게 불편을 줄 수 있어서 시장 정상화의 속도를 조율해야 한다고 생각했다 며 수요 규제를 조금 더 빠른 속도로 풀어나가서 시장이 안정 찾는데 최선의 노력할 것이라고 강조했다.',\n",
       " '이 같은 발언은 최근 고금리와 집값 하락 우려로 거래가 메마르면서 곳곳에서  경고음 이 터지는 데 따른 것으로 풀이된다.',\n",
       " '내놓은 매물이 나가지 않아 이사를 못가는 상황이 발생하거나,  영끌  매입자들의 금리 부담 급증, 집값 하락세에 따른 보유세 부담 호소, 빠른 월세화 현상 등이다.',\n",
       " '다주택자 세 중과 해제 검토 주택공급, 공공 민간 믹스 종합부동산세의 경우 국회에서 관련법 개정 논의가 이어지고 있다.',\n",
       " '여야는 앞서 종부세 중과세율 % 을 적용하는 조정대상지역 2주택 이상 소유자에 대해 중과를 폐지하고 일반세율 % 을 적용하는 방안 등을 담은 잠정 합의안을 도출한 상태다.',\n",
       " '종부세법 개정안은 예산안이 처리되는대로 국회 본회의에 오를 것으로 보인다.',\n",
       " '서울과 경기 4곳만 남은 규제지역의 추가 해제 여부 역시 시장의 관심사다.',\n",
       " '윤 대통령은 이날 주택공급계획과 관련해  민간과 공공임대를 잘 믹스해서 섞어서  공급하려고 한다 고 말했다.',\n",
       " '이어  공공임대주택 많이 지어서 공급하다 보면 중앙정부나 지방정부가 상당한 재정부담을 안게 된다 며  납세자에게도 굉장히 큰 부담이 되고 전반적으로 우리 경제에 부담 요인이나 경기 위축 요인으로 작용할 수 있다 고 설명했다.',\n",
       " '한편 정부가 임기 내 공급을 약속한 270만가구 중 50만가구에 해당하는 공공주택 사전청약이 이달 말부터 실시된다.',\n",
       " '원희룡 국토부 장관은 이날 회의에서  무주택 서민과 젊은 세대에게 희망을 갖고 내 집 마련을 포기하지 말라고  지속적으로  공급할 것 이라며  이번달 말부터  공공주택  사전청약을 받으니 관심이 있는 분들은 이용해주시라 고 당부했다.',\n",
       " '강해인기자  경기일보 , 무단전재 및 수집, 재배포금지']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_filter(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bf6279-fea5-4c80-8a69-e8e18aee4264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
