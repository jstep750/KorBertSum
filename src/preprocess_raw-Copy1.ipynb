{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb87b15-49c0-4b1b-861d-a17f66beed76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/mxnet/numpy/utils.py:36: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.\n",
      "  bool = onp.bool\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 388\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[0;32m--> 388\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgluonnlp\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnlp\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkobert\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tokenizer\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkobert\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download \u001b[38;5;28;01mas\u001b[39;00m _download\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/gluonnlp/__init__.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"NLP toolkit.\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmxnet\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m loss\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/mxnet/__init__.py:31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m library\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m contrib\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ndarray\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ndarray \u001b[38;5;28;01mas\u001b[39;00m nd\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/mxnet/contrib/__init__.py:30\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograd\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensorboard\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m text\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m onnx\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m io\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/mxnet/contrib/text/__init__.py:23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m vocab\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m embedding\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/mxnet/contrib/text/embedding.py:39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_np_array\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m numpy \u001b[38;5;28;01mas\u001b[39;00m _mx_np\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m numpy_extension \u001b[38;5;28;01mas\u001b[39;00m _mx_npx\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mregister\u001b[39m(embedding_cls):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/mxnet/numpy/__init__.py:28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _register\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_op\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunction_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstride_tricks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# pylint: disable=wildcard-import\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/mxnet/numpy/utils.py:36\u001b[0m\n\u001b[1;32m     34\u001b[0m int64 \u001b[38;5;241m=\u001b[39m onp\u001b[38;5;241m.\u001b[39mint64\n\u001b[1;32m     35\u001b[0m bool_ \u001b[38;5;241m=\u001b[39m onp\u001b[38;5;241m.\u001b[39mbool_\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43monp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m\n\u001b[1;32m     38\u001b[0m pi \u001b[38;5;241m=\u001b[39m onp\u001b[38;5;241m.\u001b[39mpi\n\u001b[1;32m     39\u001b[0m inf \u001b[38;5;241m=\u001b[39m onp\u001b[38;5;241m.\u001b[39minf\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/numpy/__init__.py:284\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_expired\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _expired\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Main training workflow\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import signal\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertConfig\n",
    "\n",
    "\n",
    "import distributed\n",
    "from models import data_loader, model_builder\n",
    "from models.data_loader import load_dataset\n",
    "from models.model_builder import Summarizer\n",
    "from tensorboardX import SummaryWriter\n",
    "from models.reporter import ReportMgr\n",
    "from models.stats import Statistics\n",
    "from others.logging import logger\n",
    "# from models.trainer import build_trainer\n",
    "# build_trainer의 dependency package pyrouge.utils가 import되지 않아 직접 셀에 삽입\n",
    "from others.logging import logger, init_logger\n",
    "import easydict\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    \"encoder\":'classifier',\n",
    "    \"mode\":'summary',\n",
    "    \"bert_data_path\":'../bert_sample/korean',\n",
    "    \"model_path\":'../models/bert_classifier',\n",
    "    \"bert_model\":'../001_bert_morp_pytorch',\n",
    "    \"result_path\":'../results/korean',\n",
    "    \"temp_dir\":'.',\n",
    "    \"bert_config_path\":'../001_bert_morp_pytorch/bert_config.json',\n",
    "    \"batch_size\":1000,\n",
    "    \"use_interval\":True,\n",
    "    \"hidden_size\":128,\n",
    "    \"ff_size\":512,\n",
    "    \"heads\":4,\n",
    "    \"inter_layers\":2,\n",
    "    \"rnn_size\":512,\n",
    "    \"param_init\":0,\n",
    "    \"param_init_glorot\":True,\n",
    "    \"dropout\":0.1,\n",
    "    \"optim\":'adam',\n",
    "    \"lr\":2e-3,\n",
    "    \"report_every\":1,\n",
    "    \"save_checkpoint_steps\":5,\n",
    "    \"block_trigram\":True,\n",
    "    \"recall_eval\":False,\n",
    "    \n",
    "    \"accum_count\":1,\n",
    "    \"world_size\":1,\n",
    "    \"visible_gpus\":'0',\n",
    "    \"gpu_ranks\":'0',\n",
    "    \"log_file\":'../logs/bert_classifier',\n",
    "    \"test_from\":'../models/bert_classifier2/model_step_35000.pt'\n",
    "})\n",
    "\n",
    "\n",
    "def build_trainer(args, device_id, model,\n",
    "                  optim):\n",
    "    \"\"\"\n",
    "    Simplify `Trainer` creation based on user `opt`s*\n",
    "    Args:\n",
    "        opt (:obj:`Namespace`): user options (usually from argument parsing)\n",
    "        model (:obj:`onmt.models.NMTModel`): the model to train\n",
    "        fields (dict): dict of fields\n",
    "        optim (:obj:`onmt.utils.Optimizer`): optimizer used during training\n",
    "        data_type (str): string describing the type of data\n",
    "            e.g. \"text\", \"img\", \"audio\"\n",
    "        model_saver(:obj:`onmt.models.ModelSaverBase`): the utility object\n",
    "            used to save the model\n",
    "    \"\"\"\n",
    "    device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
    "\n",
    "\n",
    "    grad_accum_count = args.accum_count\n",
    "    n_gpu = args.world_size\n",
    "\n",
    "    if device_id >= 0:\n",
    "        gpu_rank = int(args.gpu_ranks[device_id])\n",
    "    else:\n",
    "        gpu_rank = 0\n",
    "        n_gpu = 0\n",
    "\n",
    "    #print('gpu_rank %d' % gpu_rank)\n",
    "\n",
    "    tensorboard_log_dir = args.model_path\n",
    "\n",
    "    writer = SummaryWriter(tensorboard_log_dir, comment=\"Unmt\")\n",
    "\n",
    "    report_manager = ReportMgr(args.report_every, start_time=-1, tensorboard_writer=writer)\n",
    "\n",
    "    trainer = Trainer(args, model, optim, grad_accum_count, n_gpu, gpu_rank, report_manager)\n",
    "\n",
    "    # print(tr)\n",
    "    if (model):\n",
    "        n_params = _tally_parameters(model)\n",
    "        #logger.info('* number of parameters: %d' % n_params)\n",
    "\n",
    "    return trainer\n",
    "\n",
    "class Trainer(object):\n",
    "    \"\"\"\n",
    "    Class that controls the training process.\n",
    "\n",
    "    Args:\n",
    "            model(:py:class:`onmt.models.model.NMTModel`): translation model\n",
    "                to train\n",
    "            train_loss(:obj:`onmt.utils.loss.LossComputeBase`):\n",
    "               training loss computation\n",
    "            valid_loss(:obj:`onmt.utils.loss.LossComputeBase`):\n",
    "               training loss computation\n",
    "            optim(:obj:`onmt.utils.optimizers.Optimizer`):\n",
    "               the optimizer responsible for update\n",
    "            trunc_size(int): length of truncated back propagation through time\n",
    "            shard_size(int): compute loss in shards of this size for efficiency\n",
    "            data_type(string): type of the source input: [text|img|audio]\n",
    "            norm_method(string): normalization methods: [sents|tokens]\n",
    "            grad_accum_count(int): accumulate gradients this many times.\n",
    "            report_manager(:obj:`onmt.utils.ReportMgrBase`):\n",
    "                the object that creates reports, or None\n",
    "            model_saver(:obj:`onmt.models.ModelSaverBase`): the saver is\n",
    "                used to save a checkpoint.\n",
    "                Thus nothing will be saved if this parameter is None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,  args, model,  optim,\n",
    "                  grad_accum_count=1, n_gpu=1, gpu_rank=1,\n",
    "                  report_manager=None):\n",
    "        # Basic attributes.\n",
    "        self.args = args\n",
    "        self.save_checkpoint_steps = args.save_checkpoint_steps\n",
    "        self.model = model\n",
    "        self.optim = optim\n",
    "        self.grad_accum_count = grad_accum_count\n",
    "        self.n_gpu = n_gpu\n",
    "        self.gpu_rank = gpu_rank\n",
    "        self.report_manager = report_manager\n",
    "\n",
    "        self.loss = torch.nn.BCELoss(reduction='none')\n",
    "        assert grad_accum_count > 0\n",
    "        # Set model in training mode.\n",
    "        if (model):\n",
    "            self.model.train()\n",
    "\n",
    "    def summary(self, test_iter, step, cal_lead=False, cal_oracle=False):\n",
    "        \"\"\" Validate model.\n",
    "            valid_iter: validate data iterator\n",
    "        Returns:\n",
    "            :obj:`nmt.Statistics`: validation loss statistics\n",
    "        \"\"\"\n",
    "        # Set model in validating mode.\n",
    "        def _get_ngrams(n, text):\n",
    "            ngram_set = set()\n",
    "            text_length = len(text)\n",
    "            max_index_ngram_start = text_length - n\n",
    "            for i in range(max_index_ngram_start + 1):\n",
    "                ngram_set.add(tuple(text[i:i + n]))\n",
    "            return ngram_set\n",
    "\n",
    "        def _block_tri(c, p):\n",
    "            tri_c = _get_ngrams(3, c.split())\n",
    "            for s in p:\n",
    "                tri_s = _get_ngrams(3, s.split())\n",
    "                if len(tri_c.intersection(tri_s))>0:\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        if (not cal_lead and not cal_oracle):\n",
    "            self.model.eval()\n",
    "        stats = Statistics()\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_iter:\n",
    "                src = batch.src\n",
    "                labels = batch.labels\n",
    "                segs = batch.segs\n",
    "                clss = batch.clss\n",
    "                mask = batch.mask\n",
    "                mask_cls = batch.mask_cls\n",
    "\n",
    "\n",
    "                gold = []\n",
    "                pred = []\n",
    "\n",
    "                if (cal_lead):\n",
    "                    selected_ids = [list(range(batch.clss.size(1)))] * batch.batch_size\n",
    "                elif (cal_oracle):\n",
    "                    selected_ids = [[j for j in range(batch.clss.size(1)) if labels[i][j] == 1] for i in\n",
    "                                    range(batch.batch_size)]\n",
    "                else:\n",
    "                    sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
    "\n",
    "                    # loss = self.loss(sent_scores, labels.float())\n",
    "                    # loss = (loss * mask.float()).sum()\n",
    "                    # batch_stats = Statistics(float(loss.cpu().data.numpy()), len(labels))\n",
    "                    # stats.update(batch_stats)\n",
    "\n",
    "                    sent_scores = sent_scores + mask.float()\n",
    "                    sent_scores = sent_scores.cpu().data.numpy()\n",
    "                    selected_ids = np.argsort(-sent_scores, 1)\n",
    "                # selected_ids = np.sort(selected_ids,1)\n",
    "                \n",
    "\n",
    "        return selected_ids\n",
    "\n",
    "\n",
    "    def _gradient_accumulation(self, true_batchs, normalization, total_stats,\n",
    "                               report_stats):\n",
    "        if self.grad_accum_count > 1:\n",
    "            self.model.zero_grad()\n",
    "\n",
    "        for batch in true_batchs:\n",
    "            if self.grad_accum_count == 1:\n",
    "                self.model.zero_grad()\n",
    "\n",
    "            src = batch.src\n",
    "            labels = batch.labels\n",
    "            segs = batch.segs\n",
    "            clss = batch.clss\n",
    "            mask = batch.mask\n",
    "            mask_cls = batch.mask_cls\n",
    "\n",
    "            sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
    "\n",
    "            loss = self.loss(sent_scores, labels.float())\n",
    "            loss = (loss*mask.float()).sum()\n",
    "            (loss/loss.numel()).backward()\n",
    "            # loss.div(float(normalization)).backward()\n",
    "\n",
    "            batch_stats = Statistics(float(loss.cpu().data.numpy()), normalization)\n",
    "\n",
    "\n",
    "            total_stats.update(batch_stats)\n",
    "            report_stats.update(batch_stats)\n",
    "\n",
    "            # 4. Update the parameters and statistics.\n",
    "            if self.grad_accum_count == 1:\n",
    "                # Multi GPU gradient gather\n",
    "                if self.n_gpu > 1:\n",
    "                    grads = [p.grad.data for p in self.model.parameters()\n",
    "                             if p.requires_grad\n",
    "                             and p.grad is not None]\n",
    "                    distributed.all_reduce_and_rescale_tensors(\n",
    "                        grads, float(1))\n",
    "                self.optim.step()\n",
    "\n",
    "        # in case of multi step gradient accumulation,\n",
    "        # update only after accum batches\n",
    "        if self.grad_accum_count > 1:\n",
    "            if self.n_gpu > 1:\n",
    "                grads = [p.grad.data for p in self.model.parameters()\n",
    "                         if p.requires_grad\n",
    "                         and p.grad is not None]\n",
    "                distributed.all_reduce_and_rescale_tensors(\n",
    "                    grads, float(1))\n",
    "            self.optim.step()\n",
    "\n",
    "    def _save(self, step):\n",
    "        real_model = self.model\n",
    "        # real_generator = (self.generator.module\n",
    "        #                   if isinstance(self.generator, torch.nn.DataParallel)\n",
    "        #                   else self.generator)\n",
    "\n",
    "        model_state_dict = real_model.state_dict()\n",
    "        # generator_state_dict = real_generator.state_dict()\n",
    "        checkpoint = {\n",
    "            'model': model_state_dict,\n",
    "            # 'generator': generator_state_dict,\n",
    "            'opt': self.args,\n",
    "            'optim': self.optim,\n",
    "        }\n",
    "        checkpoint_path = os.path.join(self.args.model_path, 'model_step_%d.pt' % step)\n",
    "        #logger.info(\"Saving checkpoint %s\" % checkpoint_path)\n",
    "        # checkpoint_path = '%s_step_%d.pt' % (FLAGS.model_path, step)\n",
    "        if (not os.path.exists(checkpoint_path)):\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            return checkpoint, checkpoint_path\n",
    "\n",
    "    def _start_report_manager(self, start_time=None):\n",
    "        \"\"\"\n",
    "        Simple function to start report manager (if any)\n",
    "        \"\"\"\n",
    "        if self.report_manager is not None:\n",
    "            if start_time is None:\n",
    "                self.report_manager.start()\n",
    "            else:\n",
    "                self.report_manager.start_time = start_time\n",
    "\n",
    "    def _maybe_gather_stats(self, stat):\n",
    "        \"\"\"\n",
    "        Gather statistics in multi-processes cases\n",
    "\n",
    "        Args:\n",
    "            stat(:obj:onmt.utils.Statistics): a Statistics object to gather\n",
    "                or None (it returns None in this case)\n",
    "\n",
    "        Returns:\n",
    "            stat: the updated (or unchanged) stat object\n",
    "        \"\"\"\n",
    "        if stat is not None and self.n_gpu > 1:\n",
    "            return Statistics.all_gather_stats(stat)\n",
    "        return stat\n",
    "\n",
    "    def _maybe_report_training(self, step, num_steps, learning_rate,\n",
    "                               report_stats):\n",
    "        \"\"\"\n",
    "        Simple function to report training stats (if report_manager is set)\n",
    "        see `onmt.utils.ReportManagerBase.report_training` for doc\n",
    "        \"\"\"\n",
    "        if self.report_manager is not None:\n",
    "            return self.report_manager.report_training(\n",
    "                step, num_steps, learning_rate, report_stats,\n",
    "                multigpu=self.n_gpu > 1)\n",
    "\n",
    "    def _report_step(self, learning_rate, step, train_stats=None,\n",
    "                     valid_stats=None):\n",
    "        \"\"\"\n",
    "        Simple function to report stats (if report_manager is set)\n",
    "        see `onmt.utils.ReportManagerBase.report_step` for doc\n",
    "        \"\"\"\n",
    "        if self.report_manager is not None:\n",
    "            return self.report_manager.report_step(\n",
    "                learning_rate, step, train_stats=train_stats,\n",
    "                valid_stats=valid_stats)\n",
    "\n",
    "    def _maybe_save(self, step):\n",
    "        \"\"\"\n",
    "        Save the model if a model saver is set\n",
    "        \"\"\"\n",
    "        if self.model_saver is not None:\n",
    "            self.model_saver.maybe_save(step)\n",
    "\n",
    "            \n",
    "def summary(args, b_list, device_id, pt, step, model):\n",
    "    device_id = 0\n",
    "    device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
    "    if (pt != ''):\n",
    "        test_from = pt\n",
    "    else:\n",
    "        test_from = args.test_from\n",
    "    \n",
    "    opt = vars(checkpoint['opt'])\n",
    "    for k in opt.keys():\n",
    "        if (k in model_flags):\n",
    "            setattr(args, k, opt[k])\n",
    "    #print(args)\n",
    "\n",
    "    #model.load_cp(checkpoint)\n",
    "    #model.eval()\n",
    "\n",
    "    test_iter =data_loader.Dataloader(args, _lazy_dataset_loader(b_list),\n",
    "                                  args.batch_size, device,\n",
    "                                  shuffle=False, is_test=True)\n",
    "    trainer = build_trainer(args, device_id, model, None)\n",
    "    result = trainer.summary(test_iter,step)\n",
    "    return result\n",
    "\n",
    "def _tally_parameters(model):\n",
    "    n_params = sum([p.nelement() for p in model.parameters()])\n",
    "    return n_params\n",
    "\n",
    "args.gpu_ranks = [int(i) for i in args.gpu_ranks.split(',')]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.visible_gpus\n",
    "\n",
    "init_logger(args.log_file)\n",
    "device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
    "device_id = 0 if device == \"cuda\" else -1\n",
    "model_flags = ['hidden_size', 'ff_size', 'heads', 'inter_layers','encoder','ff_actv', 'use_interval','rnn_size']\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import urllib3\n",
    "from glob import glob\n",
    "import collections\n",
    "import six\n",
    "import gc\n",
    "import gluonnlp as nlp\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.utils import download as _download\n",
    "\n",
    "def do_lang ( openapi_key, text ) :\n",
    "    openApiURL = \"http://aiopen.etri.re.kr:8000/WiseNLU\"\n",
    "    requestJson = {  \n",
    "        \"argument\": {\n",
    "            \"text\": text,\n",
    "            \"analysis_code\": \"morp\"\n",
    "        }\n",
    "    }\n",
    "    http = urllib3.PoolManager()\n",
    "    response = http.request(\n",
    "        \"POST\",\n",
    "        openApiURL,\n",
    "        headers={\"Content-Type\": \"application/json; charset=UTF-8\", \"Authorization\" :  openapi_key},\n",
    "        body=json.dumps(requestJson)\n",
    "    )\n",
    "\n",
    "    json_data = json.loads(response.data.decode('utf-8'))\n",
    "    json_result = json_data[\"result\"]\n",
    "    \n",
    "    if json_result == -1:\n",
    "        json_reason = json_data[\"reason\"]\n",
    "        if \"Invalid Access Key\" in json_reason:\n",
    "            logger.info(json_reason)\n",
    "            logger.info(\"Please check the openapi access key.\")\n",
    "            sys.exit()\n",
    "        return \"openapi error - \" + json_reason\n",
    "    else:\n",
    "        json_data = json.loads(response.data.decode('utf-8'))\n",
    "    \n",
    "        json_return_obj = json_data[\"return_object\"]\n",
    "        \n",
    "        return_result = \"\"\n",
    "        json_sentence = json_return_obj[\"sentence\"]\n",
    "        for json_morp in json_sentence:\n",
    "            for morp in json_morp[\"morp\"]:\n",
    "                return_result = return_result+str(morp[\"lemma\"])+\"/\"+str(morp[\"type\"])+\" \"\n",
    "\n",
    "        return return_result\n",
    "\n",
    "def get_kobert_vocab(cachedir=\"./tmp/\"):\n",
    "    # Add BOS,EOS vocab\n",
    "    tokenizer = {\n",
    "        \"url\": \"s3://skt-lsl-nlp-model/KoBERT/tokenizers/kobert_news_wiki_ko_cased-1087f8699e.spiece\",\n",
    "        #\"fname\": \"/opt/ml/SumAI/bertsum/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\",\n",
    "        \"chksum\": \"ae5711deb3\",\n",
    "    }\n",
    "    \n",
    "    vocab_info = tokenizer\n",
    "    vocab_file = _download(\n",
    "        #vocab_info[\"url\"], vocab_info[\"fname\"], vocab_info[\"chksum\"], cachedir=cachedir\n",
    "        vocab_info[\"url\"], vocab_info[\"chksum\"], cachedir=cachedir\n",
    "    )\n",
    "\n",
    "    vocab_b_obj = nlp.vocab.BERTVocab.from_sentencepiece(\n",
    "        vocab_file[0], padding_token=\"[PAD]\", bos_token=\"[BOS]\", eos_token=\"[EOS]\"\n",
    "    )\n",
    "    return vocab_b_obj\n",
    "\n",
    "    \n",
    "class BertData():\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        #self.tokenizer = Tokenizer(vocab_file_path)\n",
    "        self.sep_vid = self.tokenizer.vocab['[SEP]']\n",
    "        self.cls_vid = self.tokenizer.vocab['[CLS]']\n",
    "        self.pad_vid = self.tokenizer.vocab['[PAD]']\n",
    "\n",
    "    def preprocess(self, src):\n",
    "\n",
    "        if (len(src) == 0):\n",
    "            return None\n",
    "\n",
    "        original_src_txt = [''.join(s) for s in src]\n",
    "\n",
    "\n",
    "        idxs = [i for i, s in enumerate(src) if (len(s) > 0)]\n",
    "\n",
    "        src = [src[i][:20000] for i in idxs]\n",
    "        src = src[:10000]\n",
    "\n",
    "        if (len(src) < 3):\n",
    "            return None\n",
    "\n",
    "        src_txt = [''.join(sent) for sent in src]\n",
    "        text = ' [SEP] [CLS] '.join(src_txt)\n",
    "        src_subtokens = text.split(' ')\n",
    "        src_subtokens = src_subtokens[:510]\n",
    "        src_subtokens = ['[CLS]'] + src_subtokens + ['[SEP]']\n",
    "\n",
    "        src_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(src_subtokens)\n",
    "        _segs = [-1] + [i for i, t in enumerate(src_subtoken_idxs) if t == self.sep_vid]\n",
    "        segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]\n",
    "        segments_ids = []\n",
    "        for i, s in enumerate(segs):\n",
    "            if (i % 2 == 0):\n",
    "                segments_ids += s * [0]\n",
    "            else:\n",
    "                segments_ids += s * [1]\n",
    "        cls_ids = [i for i, t in enumerate(src_subtoken_idxs) if t == self.cls_vid]\n",
    "        labels = None\n",
    "        tgt_txt = None\n",
    "        src_txt = [original_src_txt[i] for i in idxs]\n",
    "        return src_subtoken_idxs, labels, segments_ids, cls_ids, src_txt, tgt_txt\n",
    "    \n",
    "def convert_to_unicode(text):\n",
    "    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
    "    if six.PY3:\n",
    "        if isinstance(text, str):\n",
    "            return text\n",
    "        elif isinstance(text, bytes):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    elif six.PY2:\n",
    "        if isinstance(text, str):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        elif isinstance(text, unicode):\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    else:\n",
    "        raise ValueError(\"Not running on Python2 or Python 3?\")\n",
    "        \n",
    "class Tokenizer(object):\n",
    "    \n",
    "    def __init__(self, vocab_file_path):\n",
    "        self.vocab_file_path = vocab_file_path\n",
    "        \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "        vocab = collections.OrderedDict()\n",
    "        index = 0\n",
    "        with open(self.vocab_file_path, \"r\", encoding='utf-8') as reader:\n",
    "\n",
    "            while True:\n",
    "                token = convert_to_unicode(reader.readline())\n",
    "                if not token:\n",
    "                    break\n",
    "\n",
    "          ### joonho.lim @ 2019-03-15\n",
    "                if token.find('n_iters=') == 0 or token.find('max_length=') == 0 :\n",
    "\n",
    "                    continue\n",
    "                token = token.split('\\t')[0].strip('_')\n",
    "\n",
    "                token = token.strip()\n",
    "                vocab[token] = index\n",
    "                index += 1\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        \"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                ids.append(self.vocab[token])\n",
    "            except:\n",
    "                ids.append(1)\n",
    "        if len(ids) > 10000:\n",
    "            raise ValueError(\n",
    "                \"Token indices sequence length is longer than the specified maximum \"\n",
    "                \" sequence length for this BERT model ({} > {}). Running this\"\n",
    "                \" sequence through BERT will result in indexing errors\".format(len(ids), 10000)\n",
    "            )\n",
    "        return ids\n",
    "\n",
    "\n",
    "def _lazy_dataset_loader(pt_file):\n",
    "    dataset = pt_file    \n",
    "    yield dataset\n",
    "    \n",
    "def News_to_input(text, openapi_key, tokenizer):\n",
    "    newstemp = do_lang(openapi_key, text)\n",
    "    news = newstemp.split(' ./SF ')[:-1]\n",
    "    \n",
    "    bertdata = BertData(tokenizer)\n",
    "    sent_labels = [0] * len(news)\n",
    "    tmp = bertdata.preprocess(news)\n",
    "    if(not tmp): return None\n",
    "    #print(tmp)\n",
    "    b_data_dict = {\"src\":tmp[0],\n",
    "               \"tgt\": [0],\n",
    "               \"labels\":[0,0,0],\n",
    "               \"src_sent_labels\":sent_labels,\n",
    "               \"segs\":tmp[2],\n",
    "               \"clss\":tmp[3],\n",
    "               \"src_txt\":tmp[4],\n",
    "               \"tgt_txt\":'hehe'}\n",
    "    b_list = []\n",
    "    b_list.append(b_data_dict) \n",
    "    return b_list\n",
    "\n",
    "openapi_key = '9318dc23-24ac-4b59-a99e-a29ec170bf02'\n",
    "\n",
    "\n",
    "import re\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "def clean_byline(text):\n",
    "    # byline\n",
    "    pattern_email = re.compile(r'[-_0-9a-z]+@[-_0-9a-z]+(?:\\.[0-9a-z]+)+', flags=re.IGNORECASE)\n",
    "    pattern_url = re.compile(r'(?:https?:\\/\\/)?[-_0-9a-z]+[^~][^%](?:\\.[-_0-9a-z]+)+[^%]', flags=re.IGNORECASE)\n",
    "    pattern_others = re.compile(r'(\\[)*(^\\])*(◼|기자|뉴스|사진|자료|자료사진|출처|특파원|교수|작가|대표|논설|고문|주필|부문장|팀장|장관|원장|연구원|이사장|위원|실장|차장|부장|에세이|화백|사설|소장|단장|과장|기획자|경제|한겨례|일보|미디어|데일리|한겨례|타임즈|위키트리|큐레이터|저작권|평론가|©|©|ⓒ|\\@|\\/|=|▶|무단|전재|재배포|금지|댓글|좋아요|공유하기|글씨 크게 보기|글씨 작게 보기|고화질|표준화질|자동 재생|키보드 컨트롤 안내|동영상 시작)(^\\])*?(\\])*( 제공)*')\n",
    "    pattern_tag = re.compile(r'\\<[^>][^가-힣 ]*?>')\n",
    "    result = pattern_email.sub('', text)\n",
    "    result = pattern_url.sub('', result)\n",
    "    result = pattern_others.sub('', result)\n",
    "    result = pattern_tag.sub('', result)\n",
    "\n",
    "    # 본문 시작 전 꺽쇠로 쌓인 바이라인 제거\n",
    "    pattern_bracket = re.compile(r'^((?:\\[.+\\])|(?:【.+】)|(?:<.+>)|(?:◆.+◆)\\s)')\n",
    "    result = pattern_bracket.sub('', result).strip()\n",
    "    return result\n",
    "\n",
    "\n",
    "def text_filter(title, text): # str -> 전처리 -> 문장 배열\n",
    "    text = clean_byline(text)\n",
    "    result = text.strip()\n",
    "    sentences = sent_tokenize(result) \n",
    "    return_sentences = []\n",
    "    for sent in sentences:\n",
    "        for s in sent.split('\\n'):\n",
    "            s = s.strip()\n",
    "            if(len(s)>12 and '다' in s[-3:]): \n",
    "                if(s[-1] != '.'):\n",
    "                    s += '.'\n",
    "                return_sentences.append(s+' ')\n",
    "                \n",
    "    s1 = set(title.split())\n",
    "    s2 = set(return_sentences[0].split())\n",
    "    actual_jaccard = float(len(s1.intersection(s2)))/float(len(s1.union(s2)))\n",
    "    if(actual_jaccard > 0.5):\n",
    "        return_sentences = return_sentences[1:]\n",
    "        \n",
    "    return_sentences = return_sentences[:30]\n",
    "    \n",
    "    return return_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227cee75-1430-4fb8-ab38-89ca8b2ea8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "openapi_key = '9318dc23-24ac-4b59-a99e-a29ec170bf02'\n",
    "chat_history = \"뉴스 \"\n",
    "user_input = '''\n",
    "일본 정부가 문재인 대통령이 도쿄올림픽을 계기로 일본을 방문하는 방향으로 한일 양국이 조율하고 있다는 15일 요미우리신문의 보도를 부인했다.\n",
    "정부 대변인인 가토 가쓰노부(加藤勝信) 관방장관은 이날 오전 정례 기자회견에서 관련 질문에 “말씀하신 보도와 같은 사실이 없는 것으로 안다”고 밝혔다.\n",
    "앞서 요미우리는 한국 측이 도쿄올림픽을 계기로 한 문 대통령의 방일을 타진했고, 일본 측은 수용하는 방향이라고 이날 보도했다.\n",
    "한국 측은 문 대통령의 방일 때 스가 요시히데(菅義偉) 총리와 처음으로 정상회담을 하겠다는 생각이라고 요미우리는 전했다.\n",
    "가토 장관은 한일 정상회담에 대한 일본 정부의 자세에 대해 “그런 사실이 없기 때문에 가정의 질문에 대해 답하는 것을 삼가겠다”고 말했다.\n",
    "그는 한국 측의 독도방어훈련에 ‘어떤 대항 조치를 생각하고 있느냐’는 질문에는 “한국 해군의 훈련에 대해 정부로서는 강한 관심을 가지고 주시하는 상황이어서 지금 시점에선 논평을 삼가겠다”고 말을 아꼈다.\n",
    "가토 장관은 “다케시마(竹島·일본이 주장하는 독도의 명칭)는 역사적 사실에 비춰봐도, 국제법상으로도 명백한 일본 고유의 영토”라며 독도 영유권 주장을 되풀이했다.\n",
    "그러면서 “다케시마 문제에 대해서는 계속 우리나라의 영토, 영해, 영공을 단호히 지키겠다는 결의로 냉정하고 의연하게 대응해갈 생각”이라고 밝혔다.\n",
    "'''\n",
    "vocab = get_kobert_vocab()\n",
    "tokenizer = nlp.data.BERTSPTokenizer(get_tokenizer(), vocab, lower=False)\n",
    "\n",
    "\n",
    "bot_input_ids = News_to_input(chat_history + user_input, openapi_key, tokenizer)\n",
    "print('------------------------------------------------------------')\n",
    "\n",
    "\n",
    "#logger.info('Loading checkpoint from %s' % test_from)\n",
    "checkpoint = torch.load('../models/bert_classifier/model_step_1000.pt', map_location=lambda storage, loc: storage)\n",
    "config = BertConfig.from_json_file(args.bert_config_path)\n",
    "model = Summarizer(args, device, load_pretrained_bert=False, bert_config = config)\n",
    "model.load_cp(checkpoint)\n",
    "model.eval()  \n",
    "\n",
    "chat_history_ids = summary(args, bot_input_ids, 0, '', None, model)\n",
    "print(chat_history_ids)\n",
    "print('------------------------------------------------------------')\n",
    "print(chat_history_ids[0])\n",
    "pred_lst = list(chat_history_ids[0][:3])\n",
    "print(pred_lst)\n",
    "print('------------------------------------------------------------')\n",
    "final_text = ''\n",
    "for i,a in enumerate(user_input.split('. ')):\n",
    "    if i in pred_lst:\n",
    "        print(i, a)\n",
    "        final_text = final_text+a+'. '\n",
    "chat_history = user_input + '<token>' +final_text\n",
    "print('------------------------------------------------------------')\n",
    "print(final_text)\n",
    "print('------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "39abaf4d-64a5-40eb-8784-e94d995941a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-23 13:29:46,810 INFO] loading archive file ../001_bert_morp_pytorch\n",
      "[2023-01-23 13:29:46,812 INFO] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30349\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /opt/ml/KorBertSum/src/./tmp/kobert_news_wiki_ko_cased-1087f8699e.spiece\n",
      "using cached model. /opt/ml/KorBertSum/src/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "checkpoint = torch.load('../models/bert_classifier2/model_step_35000.pt', map_location=lambda storage, loc: storage)\n",
    "config = BertConfig.from_json_file(args.bert_config_path)\n",
    "model = Summarizer(args, device, load_pretrained_bert=False, bert_config = config)\n",
    "model.load_cp(checkpoint)\n",
    "model.eval()  \n",
    "vocab = get_kobert_vocab()\n",
    "tokenizer = nlp.data.BERTSPTokenizer(get_tokenizer(), vocab, lower=False)\n",
    "\n",
    "def get_top_sentences(user_input, model, tokenizer):\n",
    "    bot_input_ids = News_to_input(user_input, openapi_key, tokenizer)\n",
    "    \n",
    "    chat_history_ids = summary(args, bot_input_ids, 0, '', None, model)\n",
    "    pred_lst = list(chat_history_ids[0])\n",
    "    final_text = []\n",
    "    for p in pred_lst:\n",
    "        if(p < len(user_input.split('. ')) and len(user_input.split('. ')[p]) > 10):\n",
    "            final_text.append((p, user_input.split('. ')[p]+'. '))\n",
    "    return final_text\n",
    "\n",
    "\n",
    "def add_topk_to_df(df, model, tokenizer):\n",
    "    \n",
    "    start = time.time()\n",
    "    topk = []\n",
    "    for i,context2 in enumerate(tqdm(df['context2'])):\n",
    "        context2 = eval(context2)\n",
    "        context = []\n",
    "        for s in context2:\n",
    "            if(len(s) > 600):\n",
    "                s = s.split('.')[0]+' '\n",
    "            if(len(s) < 600):\n",
    "                context.append(s)\n",
    "        \n",
    "        context = context[:30]\n",
    "        top = None\n",
    "        if(len(context) > 4):\n",
    "            top = get_top_sentences(' '.join(context), model, tokenizer)\n",
    "        if(top):\n",
    "            topk.append(top)\n",
    "        else:\n",
    "            topk.append('Hello world')\n",
    "            \n",
    "    df['topk'] = topk\n",
    "    df = df.drop(df[df['topk'] == 'Hello world'].index)\n",
    "    print(f\"{time.time()-start:.4f} sec\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "87764c16-096a-4d91-ab95-c1209759385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_context_to_df(df):\n",
    "    contexts = []\n",
    "    for i, cont in enumerate(tqdm(df['raw'])):\n",
    "        if(cont):\n",
    "            contexts.append(text_filter(cont))\n",
    "        else:\n",
    "            contexts.append('delete this')\n",
    "    df['context2'] = contexts\n",
    "    df = df.drop(df[df['context2'] == 'delete this'].index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c07ac327-3b2e-4d0f-90bd-bed639400eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 국내 지방자치단체들이 테슬라의 생산기지인 '기가팩토리' 유치전에 본격적으로 뛰어들었다.일론 머스크 테슬라 최고경영자(CEO)가 지난달 23일 윤석열 대통령과 영상면담에서 \"한국은 아시아권 최우선 투자 후보지 중 하나\"라고 밝히면서 테슬라의 아시아 2공장 유력 후보지로 한국이 부상했기 때문이다. \n",
      "1 정부도 산업통상자원부와 대한무역투자진흥공사(KOTRA)를 중심으로 전담 팀을 구성해 테슬라의 국내 투자를 유치할 계획인 만큼 지자체들 움직임도 바빠졌다. \n",
      "2 1일 각 지자체에 따르면 현재 경북 포항시, 경기 고양시, 강원도 등이 테슬라의 기가팩토리 유치에 가장 적극적이다. \n",
      "3 이들 지자체는 산업부에 유치 의향을 전달하는 등 투자 강점을 알리는 데 집중하고 있다.가장 먼저 움직인 곳이 경북 포항시다. \n",
      "4 이철우 경북도지사와 이강덕 포항시장은 머스크와 윤 대통령의 영상면담 직후부터 기가팩토리 포항 유치를 위한 전략 마련에 돌입했다. \n",
      "5 이를 위해 포항시는 테슬라 유치팀을 구성하고 지난달 30일 산업부를 찾아 투자 강점 등을 담은 유치 의향서를 전달했다. \n",
      "6 포항시는 포스코의 철판 공급망과 함께 배터리 리사이클링 규제자유특구 등 2차전지 생태계 구축 등을 강점으로 내세우고 있다. \n",
      "7 또 포스텍의 연구 기반 시설과 영일만 배후산단 등 물류 기반 시설이 우수하다는 점도 강조하고 있다. \n",
      "8 포항시 관계자는 \"테슬라 기가팩토리가 유치되면 1973년 포항제철소 1기 준공 이후 지역  발전의 최대 전환점이 될 것\"이라며 \"포항의 뛰어난 입지 여건을 기반으로 유치에 총력을 기울이겠다\"고 말했다.경기 고양시도 정부와 경기도에 고양자유구역과 연계한 테슬라 기가팩토리 유치를 제안할 계획이다. \n",
      "9 고양시는 최근 경기도 자유구역 후보지로 선정된 점을 강점으로 내세우고 있다. \n",
      "10 자유구역으로 최종 지정될 경우 해외 투자자본 유치를 위한 세제 감면이나 규제 완화 등의 혜택이 부여된다. \n",
      "11 테슬라처럼 기술력을 갖춘 해외 기업이 입주할 만한 장점이 있는 셈이다. \n",
      "12 이동환 고양시장은 \"자유구역으로 지정되고 적절한 세제 지원 등 인센티브가 제공된다면 테슬라 유치가 전혀 불가능한 얘기는 아니다\"며 \"정부·기업·대학·연구소와 긴밀히 협력해 나가겠다\"고 밝혔다.강원도도 테슬라 공장 유치에 도전장을 던졌다. \n",
      "13 김진태 강원도지사는 지난달 28일 회견에서 \"최근 테슬라코리아에 강원도를 최우선으로 고려해 달라는 의견을 전달했다\"며 \"테슬라가 공장 건립 후보지로 한국을 고려한다면 강원도가 그곳이 되기 위해 본격적으로 나서겠다\"고 밝혔다.산업부는 투자 유치를 희망하는 지자체들과 충분한 협의를 통해 테슬라 유치에 노력하겠다는 입장이다. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['국내 지방자치단체들이 테슬라의 생산기지인 \\'기가팩토리\\' 유치전에 본격적으로 뛰어들었다.일론 머스크 테슬라 최고경영자(CEO)가 지난달 23일 윤석열 대통령과 영상면담에서 \"한국은 아시아권 최우선 투자 후보지 중 하나\"라고 밝히면서 테슬라의 아시아 2공장 유력 후보지로 한국이 부상했기 때문이다. ',\n",
       " '정부도 산업통상자원부와 대한무역투자진흥공사(KOTRA)를 중심으로 전담 팀을 구성해 테슬라의 국내 투자를 유치할 계획인 만큼 지자체들 움직임도 바빠졌다. ',\n",
       " '1일 각 지자체에 따르면 현재 경북 포항시, 경기 고양시, 강원도 등이 테슬라의 기가팩토리 유치에 가장 적극적이다. ',\n",
       " '이들 지자체는 산업부에 유치 의향을 전달하는 등 투자 강점을 알리는 데 집중하고 있다.가장 먼저 움직인 곳이 경북 포항시다. ',\n",
       " '이철우 경북도지사와 이강덕 포항시장은 머스크와 윤 대통령의 영상면담 직후부터 기가팩토리 포항 유치를 위한 전략 마련에 돌입했다. ',\n",
       " '이를 위해 포항시는 테슬라 유치팀을 구성하고 지난달 30일 산업부를 찾아 투자 강점 등을 담은 유치 의향서를 전달했다. ',\n",
       " '포항시는 포스코의 철판 공급망과 함께 배터리 리사이클링 규제자유특구 등 2차전지 생태계 구축 등을 강점으로 내세우고 있다. ',\n",
       " '또 포스텍의 연구 기반 시설과 영일만 배후산단 등 물류 기반 시설이 우수하다는 점도 강조하고 있다. ',\n",
       " '포항시 관계자는 \"테슬라 기가팩토리가 유치되면 1973년 포항제철소 1기 준공 이후 지역  발전의 최대 전환점이 될 것\"이라며 \"포항의 뛰어난 입지 여건을 기반으로 유치에 총력을 기울이겠다\"고 말했다.경기 고양시도 정부와 경기도에 고양자유구역과 연계한 테슬라 기가팩토리 유치를 제안할 계획이다. ',\n",
       " '고양시는 최근 경기도 자유구역 후보지로 선정된 점을 강점으로 내세우고 있다. ',\n",
       " '자유구역으로 최종 지정될 경우 해외 투자자본 유치를 위한 세제 감면이나 규제 완화 등의 혜택이 부여된다. ',\n",
       " '테슬라처럼 기술력을 갖춘 해외 기업이 입주할 만한 장점이 있는 셈이다. ',\n",
       " '이동환 고양시장은 \"자유구역으로 지정되고 적절한 세제 지원 등 인센티브가 제공된다면 테슬라 유치가 전혀 불가능한 얘기는 아니다\"며 \"정부·기업·대학·연구소와 긴밀히 협력해 나가겠다\"고 밝혔다.강원도도 테슬라 공장 유치에 도전장을 던졌다. ',\n",
       " '김진태 강원도지사는 지난달 28일 회견에서 \"최근 테슬라코리아에 강원도를 최우선으로 고려해 달라는 의견을 전달했다\"며 \"테슬라가 공장 건립 후보지로 한국을 고려한다면 강원도가 그곳이 되기 위해 본격적으로 나서겠다\"고 밝혔다.산업부는 투자 유치를 희망하는 지자체들과 충분한 협의를 통해 테슬라 유치에 노력하겠다는 입장이다. ']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = df.raw[56]\n",
    "for i,v in enumerate(text_filter(raw)):\n",
    "    print(i,v)\n",
    "    \n",
    "context2 = df.context2[56]\n",
    "context2 = eval(context2)\n",
    "context = []\n",
    "for s in context2:\n",
    "    if(len(s) > 600):\n",
    "        s = s.split('.')[0]+' '\n",
    "    if(len(s) < 600):\n",
    "        context.append(s)\n",
    "\n",
    "context = context[:30]\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3a7d5a56-d62c-439a-91eb-db51ca5e05d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'국내 지방자치단체들이 테슬라의 생산기지인 \\'기가팩토리\\' 유치전에 본격적으로 뛰어들었다.일론 머스크 테슬라 최고경영자(CEO)가 지난달 23일 윤석열 대통령과 영상면담에서 \"한국은 아시아권 최우선 투자 후보지 중 하나\"라고 밝히면서 테슬라의 아시아 2공장 유력 후보지로 한국이 부상했기 때문이다.  정부도 산업통상자원부와 대한무역투자진흥공사(KOTRA)를 중심으로 전담 팀을 구성해 테슬라의 국내 투자를 유치할 계획인 만큼 지자체들 움직임도 바빠졌다.  1일 각 지자체에 따르면 현재 경북 포항시, 경기 고양시, 강원도 등이 테슬라의 기가팩토리 유치에 가장 적극적이다.  이들 지자체는 산업부에 유치 의향을 전달하는 등 투자 강점을 알리는 데 집중하고 있다.가장 먼저 움직인 곳이 경북 포항시다.  이철우 경북도지사와 이강덕 포항시장은 머스크와 윤 대통령의 영상면담 직후부터 기가팩토리 포항 유치를 위한 전략 마련에 돌입했다.  이를 위해 포항시는 테슬라 유치팀을 구성하고 지난달 30일 산업부를 찾아 투자 강점 등을 담은 유치 의향서를 전달했다.  포항시는 포스코의 철판 공급망과 함께 배터리 리사이클링 규제자유특구 등 2차전지 생태계 구축 등을 강점으로 내세우고 있다.  또 포스텍의 연구 기반 시설과 영일만 배후산단 등 물류 기반 시설이 우수하다는 점도 강조하고 있다.  포항시 관계자는 \"테슬라 기가팩토리가 유치되면 1973년 포항제철소 1기 준공 이후 지역  발전의 최대 전환점이 될 것\"이라며 \"포항의 뛰어난 입지 여건을 기반으로 유치에 총력을 기울이겠다\"고 말했다.경기 고양시도 정부와 경기도에 고양자유구역과 연계한 테슬라 기가팩토리 유치를 제안할 계획이다.  고양시는 최근 경기도 자유구역 후보지로 선정된 점을 강점으로 내세우고 있다.  자유구역으로 최종 지정될 경우 해외 투자자본 유치를 위한 세제 감면이나 규제 완화 등의 혜택이 부여된다.  테슬라처럼 기술력을 갖춘 해외 기업이 입주할 만한 장점이 있는 셈이다.  이동환 고양시장은 \"자유구역으로 지정되고 적절한 세제 지원 등 인센티브가 제공된다면 테슬라 유치가 전혀 불가능한 얘기는 아니다\"며 \"정부·기업·대학·연구소와 긴밀히 협력해 나가겠다\"고 밝혔다.강원도도 테슬라 공장 유치에 도전장을 던졌다.  김진태 강원도지사는 지난달 28일 회견에서 \"최근 테슬라코리아에 강원도를 최우선으로 고려해 달라는 의견을 전달했다\"며 \"테슬라가 공장 건립 후보지로 한국을 고려한다면 강원도가 그곳이 되기 위해 본격적으로 나서겠다\"고 밝혔다.산업부는 투자 유치를 희망하는 지자체들과 충분한 협의를 통해 테슬라 유치에 노력하겠다는 입장이다. '"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9fd3e412-91e1-4310-9801-fd5cc2a5c8e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-5d0db2e94f5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_top_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-bb7ff62e62fb>\u001b[0m in \u001b[0;36mget_top_sentences\u001b[0;34m(user_input, model, tokenizer)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mbot_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNews_to_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopenapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mchat_history_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbot_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mpred_lst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat_history_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mfinal_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-bae4d15ccfae>\u001b[0m in \u001b[0;36msummary\u001b[0;34m(args, b_list, device_id, pt, step, model)\u001b[0m\n\u001b[1;32m    362\u001b[0m                                   shuffle=False, is_test=True)\n\u001b[1;32m    363\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-bae4d15ccfae>\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, test_iter, step, cal_lead, cal_oracle)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m                 \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/KorBertSum/src/models/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mdataset_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_dataset_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/KorBertSum/src/models/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m                 \u001b[0;31m# fast-forward if loaded from state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterations_this_epoch\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/KorBertSum/src/models/data_loader.py\u001b[0m in \u001b[0;36mcreate_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;34m\"\"\" Create batches \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mp_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/KorBertSum/src/models/data_loader.py\u001b[0m in \u001b[0;36mbatch_buffer\u001b[0;34m(self, data, batch_size)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbatch_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_so_far\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'src'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "a = get_top_sentences(' '.join(context[1:20]), model, tokenizer)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5592cb7f-bbb2-4761-9c6f-83eea1a9c61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[우성덕   이상헌 '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_others = re.compile(r'(\\[)*(^\\])*(◼|기자|뉴스|사진|자료|자료사진|출처|특파원|교수|작가|대표|논설|고문|주필|부문장|팀장|장관|원장|연구원|이사장|위원|실장|차장|부장|에세이|화백|사설|소장|단장|과장|기획자|경제|한겨례|일보|미디어|데일리|한겨례|타임즈|위키트리|큐레이터|저작권|평론가|©|©|ⓒ|\\@|\\/|=|▶|무단|전재|재배포|금지|댓글|좋아요|공유하기|글씨 크게 보기|글씨 작게 보기|고화질|표준화질|자동 재생|키보드 컨트롤 안내|동영상 시작)(^\\])*?(\\])*( 제공)*')\n",
    "\n",
    "result = '[우성덕 기자 / 이상헌 기자]'\n",
    "result = pattern_others.sub('', result)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bd348bd5-5253-4f00-b99c-b729c71b4587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "윤석열 대통령은 15일 국정과제 점검회의에서 규제 완화와 다주택자 세 중과 해제 검토,주택공급, 공공·민간 믹스 등 3대개혁 청을 제시했다. \n",
      "윤 대통령은 이날 오후 2시부터 청와대 영빈관에서 열린 제1차 국정과제 점검회의에서 국민 패널 2명으로부터 ‘내 집 마련’과 ‘부동산 경기 활성화’ 방안과 관련한 질문을 받고 부동산 시장 연착륙을 위한 규제 완화 필요성을 강조했다. \n",
      "전 정부에서 강화된 부동산 규제를 일제히 풀 경우 부작용을 우려해 그간 속도 조절을 해왔지만, 고금리 등으로 상황이 급변한 시장 대응을 위해 다시 규제 완화 고삐를 죄겠다고 했다.이같은 발언은 규제완화를 통한 부동산 시장을 활성화시키겠다는 것으로 풀이된다. \n",
      "윤 대통령은 “제가 정부를 맡기 전까지 공급 측면과 수요 측면의 불합리한 복합 규제 때문에 집값이 너무 천정부지로 솟고 거래물량이 위축됐다”며 “시장 정상화가 중요하다고 해서 많은 규제를 풀고 정상화하려고 했는데, 고금리 상황 때문에 다시 부동산 가격이 하락하는 추세를 보였다”고 진단했다. \n",
      "그러면서 “(전 정부의) 잘못된 정책으로 인한 현상이라고 해도, 그것을 일시에 제거하다 보면 시장에 혼란이 일어나서 결국 국민에게 불편을 줄 수 있어서 시장 정상화의 속도를 조율해야 한다고 생각했다”며 수요 규제를 조금 더 빠른 속도로 풀어나가서 시장이 안정 찾는데 최선의 노력할 것이라고 강조했다. \n",
      "이 같은 발언은 최근 고금리와 집값 하락 우려로 거래가 메마르면서 곳곳에서 ‘경고음’이 터지는 데 따른 것으로 풀이된다. \n",
      "내놓은 매물이 나가지 않아 이사를 못가는 상황이 발생하거나, ‘영끌’ 매입자들의 금리 부담 급증, 집값 하락세에 따른 보유세 부담 호소, 빠른 월세화 현상 등이다. \n",
      "종합부동산세의 경우 국회에서 관련법 개정 논의가 이어지고 있다. \n",
      "여야는 앞서 종부세 중과세율(1.2~6.0%)을 적용하는 조정대상지역 2주택 이상 소유자에 대해 중과를 폐지하고 일반세율(0.5~2.7%)을 적용하는 방안 등을 담은 잠정 합의안을 도출한 상태다. \n",
      "종부세법 개정안은 예산안이 처리되는대로 국회 본회의에 오를 것으로 보인다. \n",
      "서울과 경기 4곳만 남은 규제지역의 추가 해제 여부 역시 시장의 관심사다. \n",
      "윤 대통령은 이날 주택공급계획과 관련해 “민간과 공공임대를 잘 믹스해서(섞어서) 공급하려고 한다”고 말했다. \n",
      "이어 “공공임대주택 많이 지어서 공급하다 보면 중앙정부나 지방정부가 상당한 재정부담을 안게 된다”며 “납세자에게도 굉장히 큰 부담이 되고 전반적으로 우리 에 부담 요인이나 경기 위축 요인으로 작용할 수 있다”고 설명했다. \n",
      "한편 정부가 임기 내 공급을 약속한 270만가구 중 50만가구에 해당하는 공공주택 사전청약이 이달 말부터 실시된다. \n",
      "원희룡 국토부 은 이날 회의에서 “무주택 서민과 젊은 세대에게 희망을 갖고 내 집 마련을 포기하지 말라고 (지속적으로) 공급할 것”이라며 “이번달 말부터 (공공주택) 사전청약을 받으니 관심이 있는 분들은 이용해주시라”고 당부했다. \n"
     ]
    }
   ],
   "source": [
    "for raw in eval(df.context2[0]):\n",
    "    print(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5415e8c3-9e96-4044-9379-83ad55bdb0b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9f2a2152-b921-4b8f-99ed-33c3c234fcb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./bertopic(mpnet)\n",
      "['bertopic_윤석열_20221201_20221231_crwal_news_context_raw.csv', 'bertopic_부동산_20221201_20221203_crwal_news_context_raw.csv', 'bertopic_삼성전자_20221201_20221215_crwal_news_context_raw.csv', 'bertopic_윤석열_20221201_20221203_crwal_news_context_raw.csv', 'bertopic_부동산_20221201_20221231_crwal_news_context_raw.csv', 'bertopic_삼성전자_20221201_20221231_crwal_news_context_raw.csv', 'bertopic_삼성전자_20221201_20221203_crwal_news_context_raw.csv', 'bertopic_윤석열_20221201_20221215_crwal_news_context_raw.csv', 'bertopic_부동산_20221201_20221215_crwal_news_context_raw.csv']\n",
      "0 88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:00<00:00, 827.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 673.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:00<00:00, 962.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 318/318 [00:00<00:00, 634.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:00<00:00, 932.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 1345.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 667.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 122/122 [00:00<00:00, 801.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/88 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-88a28bc4f09c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mnew_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_topk_to_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'./preprocessed2/{mypath}/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'./preprocessed2/{mypath}/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-830cc39a0772>\u001b[0m in \u001b[0;36madd_topk_to_df\u001b[0;34m(df, model, tokenizer)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mtop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_top_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mtopk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-2c4b76cce2db>\u001b[0m in \u001b[0;36mget_top_sentences\u001b[0;34m(user_input, model, tokenizer)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mbot_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNews_to_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopenapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mchat_history_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbot_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mpred_lst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat_history_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfinal_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-bae4d15ccfae>\u001b[0m in \u001b[0;36msummary\u001b[0;34m(args, b_list, device_id, pt, step, model)\u001b[0m\n\u001b[1;32m    362\u001b[0m                                   shuffle=False, is_test=True)\n\u001b[1;32m    363\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-bae4d15ccfae>\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, test_iter, step, cal_lead, cal_oracle)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m                 \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/KorBertSum/src/models/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mdataset_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_dataset_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/KorBertSum/src/models/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m                 \u001b[0;31m# fast-forward if loaded from state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterations_this_epoch\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/KorBertSum/src/models/data_loader.py\u001b[0m in \u001b[0;36mcreate_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;34m\"\"\" Create batches \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mp_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/KorBertSum/src/models/data_loader.py\u001b[0m in \u001b[0;36mbatch_buffer\u001b[0;34m(self, data, batch_size)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbatch_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_so_far\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'src'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "all_paths = [\n",
    " 'bertopic(mpnet)',\n",
    " 'bertopic(paraphrase)',\n",
    " 'kmeans_12',\n",
    " 'hdbscan',\n",
    " 'kmeans_8',\n",
    " 'bertopic(SR-BERT)',\n",
    " 'kmeans_4']\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "import pandas as pd\n",
    "\n",
    "i = 0\n",
    "mypath = f'./{all_paths[i]}'\n",
    "print(mypath)\n",
    "onlyfiles = [f for f in listdir(mypath+'/raw') if isfile(join(mypath+'/raw', f)) and f.split('.')[1] == 'csv']\n",
    "print(onlyfiles)\n",
    "############### raw 전처리 ################\n",
    "for i,fname in enumerate(onlyfiles):\n",
    "    file = fname.split('_crwal')[0]\n",
    "    filepath = f'{mypath}/raw/{onlyfiles[i]}'\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(i, len(df))\n",
    "    new_df = add_context_to_df(df)\n",
    "    if not os.path.exists(f'./preprocessed/{mypath}/'):\n",
    "        os.makedirs(f'./preprocessed/{mypath}/')\n",
    "    new_df.to_csv(f'./preprocessed/{mypath}/{file}_pre.csv')\n",
    "\n",
    "############### 추출요약 ################\n",
    "for i,fname in enumerate(onlyfiles):\n",
    "    file = fname.split('_crwal')[0]\n",
    "    filepath = f'./preprocessed/{mypath}/{file}_pre.csv'\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(i, len(df))\n",
    "    new_df = add_topk_to_df(df, model, tokenizer)\n",
    "    if not os.path.exists(f'./preprocessed2/{mypath}/'):\n",
    "        os.makedirs(f'./preprocessed2/{mypath}/')\n",
    "    new_df.to_csv(f'./preprocessed2/{mypath}/{file}_pre2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "801e85bb-57df-4b16-bafc-8618b253b200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./bertopic(paraphrase)\n",
      "['bertopic_윤석열_20221201_20221231_crwal_news_context_raw.csv', 'bertopic_부동산_20221201_20221203_crwal_news_context_raw.csv', 'bertopic_삼성전자_20221201_20221215_crwal_news_context_raw.csv', 'bertopic_윤석열_20221201_20221203_crwal_news_context_raw.csv', 'bertopic_부동산_20221201_20221231_crwal_news_context_raw.csv', 'bertopic_삼성전자_20221201_20221231_crwal_news_context_raw.csv', 'bertopic_삼성전자_20221201_20221203_crwal_news_context_raw.csv', 'bertopic_윤석열_20221201_20221215_crwal_news_context_raw.csv', 'bertopic_부동산_20221201_20221215_crwal_news_context_raw.csv']\n",
      "0 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 111/111 [00:00<00:00, 798.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 651.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:00<00:00, 961.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 306/306 [00:00<00:00, 615.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 949.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 1347.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:00<00:00, 693.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [00:00<00:00, 714.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/111 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-a1ca8c3b80ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mnew_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_topk_to_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'./preprocessed2/{mypath}/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'./preprocessed2/{mypath}/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-830cc39a0772>\u001b[0m in \u001b[0;36madd_topk_to_df\u001b[0;34m(df, model, tokenizer)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mtop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_top_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mtopk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-2c4b76cce2db>\u001b[0m in \u001b[0;36mget_top_sentences\u001b[0;34m(user_input, model, tokenizer)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mbot_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNews_to_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopenapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mchat_history_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbot_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mpred_lst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat_history_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfinal_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-bae4d15ccfae>\u001b[0m in \u001b[0;36msummary\u001b[0;34m(args, b_list, device_id, pt, step, model)\u001b[0m\n\u001b[1;32m    362\u001b[0m                                   shuffle=False, is_test=True)\n\u001b[1;32m    363\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-bae4d15ccfae>\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, test_iter, step, cal_lead, cal_oracle)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m                 \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/KorBertSum/src/models/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mdataset_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_dataset_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/KorBertSum/src/models/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m                 \u001b[0;31m# fast-forward if loaded from state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterations_this_epoch\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/KorBertSum/src/models/data_loader.py\u001b[0m in \u001b[0;36mcreate_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;34m\"\"\" Create batches \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mp_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/KorBertSum/src/models/data_loader.py\u001b[0m in \u001b[0;36mbatch_buffer\u001b[0;34m(self, data, batch_size)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbatch_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_so_far\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'src'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "all_paths = [\n",
    " 'bertopic(mpnet)',\n",
    " 'bertopic(paraphrase)',\n",
    " 'kmeans_12',\n",
    " 'hdbscan',\n",
    " 'kmeans_8',\n",
    " 'bertopic(SR-BERT)',\n",
    " 'kmeans_4']\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "import pandas as pd\n",
    "\n",
    "for i in range(len(all_paths)):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    mypath = f'./{all_paths[i]}'\n",
    "    print(mypath)\n",
    "    onlyfiles = [f for f in listdir(mypath+'/raw') if isfile(join(mypath+'/raw', f)) and f.split('.')[1] == 'csv']\n",
    "    print(onlyfiles)\n",
    "    ############### raw 전처리 ################\n",
    "    for i,fname in enumerate(onlyfiles):\n",
    "        file = fname.split('_crwal')[0]\n",
    "        filepath = f'{mypath}/raw/{onlyfiles[i]}'\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(i, len(df))\n",
    "        new_df = add_context_to_df(df)\n",
    "        if not os.path.exists(f'./preprocessed/{mypath}/'):\n",
    "            os.makedirs(f'./preprocessed/{mypath}/')\n",
    "        new_df.to_csv(f'./preprocessed/{mypath}/{file}_pre.csv')\n",
    "\n",
    "    ############### 추출요약 ################\n",
    "    for i,fname in enumerate(onlyfiles):\n",
    "        file = fname.split('_crwal')[0]\n",
    "        filepath = f'./preprocessed/{mypath}/{file}_pre.csv'\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(i, len(df))\n",
    "        new_df = add_topk_to_df(df, model, tokenizer)\n",
    "        if not os.path.exists(f'./preprocessed2/{mypath}/'):\n",
    "            os.makedirs(f'./preprocessed2/{mypath}/')\n",
    "        new_df.to_csv(f'./preprocessed2/{mypath}/{file}_pre2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "3a2e22a6-f890-4c4c-872f-863978736c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = 'bertopic_부동산_20221201_20221231_pre.csv'\n",
    "df = pd.read_csv(f'./preprocessed/bertopic(mpnet)/{z}')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150a508a-3bcc-4ed8-ae8f-da8c3ca57f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = f'./preprocessed/bertopic(mpnet)/{file}_pre.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "94ae59f3-6aeb-4050-9b79-a34bc284f892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2,\n",
       "  ' 한종희 삼성전자 디바이스경험(DX)(부회장)은 다음 달 초 미국 라스베이거스에서 열리는 세계 최대 가전·IT 전시회인 ‘CES 2023’에서 ‘맞춤형 경험으로 여는 초(超)연결 시대’(Bringing Calm to Our Connected World)를 제안할 예정이라고 15일 밝혔다  올해 초 열린 ‘CES 2022’에서는 ‘미래를 위한 동행’을 주제로 기조연설을 했다  삼성전자는 CES 2023에서 초연결 시대에 기반한 진화된 스마트싱스 경험을 선보이며, 지속가능한 미래를 위한 여정을 공유할 계획이다  한종희 삼성전자 부회장이 지난 1월 4일 미국 라스베이거스 베네시안 팔라조에서 ‘미래를 위한 동행(Together for tomorrow)’을 주제로 CES 2022 기조 연설을 하고 있다. '),\n",
       " (3,\n",
       "  ' 지난 5일(현지시간) 미국 네바다주 라스베이거스컨벤션센터에서 CES 2022 관람객과 취재진이 전시장 입장을 기다리고 있다. '),\n",
       " (0,\n",
       "  '한종희 삼성전자 부회장이 지난 1월 4일 미국 라스베이거스 베네시안 팔라조에서 ‘미래를 위한 동행(Together for tomorrow)’을 주제로 CES 2022 기조 연설을 하고 있다. '),\n",
       " (1,\n",
       "  ' 지난 5일(현지시간) 미국 네바다주 라스베이거스컨벤션센터에서 CES 2022 관람객과 취재진이 전시장 입장을 기다리고 있다. ')]"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''\n",
    "['한종희 삼성전자 부회장이 지난 1월 4일 미국 라스베이거스 베네시안 팔라조에서 ‘미래를 위한 동행(Together for tomorrow)’을 주제로 CES 2022 기조 연설을 하고 있다. ', '지난 5일(현지시간) 미국 네바다주 라스베이거스컨벤션센터에서 CES 2022 관람객과 취재진이 전시장 입장을 기다리고 있다. ', '한종희 삼성전자 디바이스경험(DX)(부회장)은 다음 달 초 미국 라스베이거스에서 열리는 세계 최대 가전·IT 전시회인 ‘CES 2023’에서 ‘맞춤형 경험으로 여는 초(超)연결 시대’(Bringing Calm to Our Connected World)를 제안할 예정이라고 15일 밝혔다 ', '올해 초 열린 ‘CES 2022’에서는 ‘미래를 위한 동행’을 주제로 기조연설을 했다 ', '삼성전자는 CES 2023에서 초연결 시대에 기반한 진화된 스마트싱스 경험을 선보이며, 지속가능한 미래를 위한 여정을 공유할 계획이다 ','한종희 삼성전자 부회장이 지난 1월 4일 미국 라스베이거스 베네시안 팔라조에서 ‘미래를 위한 동행(Together for tomorrow)’을 주제로 CES 2022 기조 연설을 하고 있다. ', '지난 5일(현지시간) 미국 네바다주 라스베이거스컨벤션센터에서 CES 2022 관람객과 취재진이 전시장 입장을 기다리고 있다. ', '한종희 삼성전자 디바이스경험(DX)(부회장)은 다음 달 초 미국 라스베이거스에서 열리는 세계 최대 가전·IT 전시회인 ‘CES 2023’에서 ‘맞춤형 경험으로 여는 초(超)연결 시대’(Bringing Calm to Our Connected World)를 제안할 예정이라고 15일 밝혔다 ', '올해 초 열린 ‘CES 2022’에서는 ‘미래를 위한 동행’을 주제로 기조연설을 했다 ', '삼성전자는 CES 2023에서 초연결 시대에 기반한 진화된 스마트싱스 경험을 선보이며, 지속가능한 미래를 위한 여정을 공유할 계획이다 ']\n",
    "'''\n",
    "context = eval(text)\n",
    "top = get_top_sentences(' '.join(context), model, tokenizer)\n",
    "top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "66464900-a9ea-4efa-b90a-befc4793f69f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4,\n",
       "  '한 부회장은 다음달 4일 오후 2시 미국 서부시간 라스베이거스 만 달레이 베이 볼룸에서 열리는 삼성전자 프레스 컨퍼런스에 대표 연사로 나선다. '),\n",
       " (9,\n",
       "  '한 부회장은 또 연결은 보다 쉬워지고, 개개인의 맞춤 경험은 AI로 더욱 정교해지며, 기 기간 연결은 보다 안전해질 것 이라며   삼성은 스마트싱스 가시화 를 위해 업계의 다양한 파트너들과 긴밀하게 협력하며, 개인의 취향과 라이프스타일에 최적화된 맞춤형 연결 경험을 보다 안전하고 풍부하게 제공할 수 있는 새로운 길 을 열어 보일 것 이라고 말했다. '),\n",
       " (8,\n",
       "  '삼성전자는 CES 2023에서 초연결 시대에 기반 한 진화된 스마트 싱스 경험을 선보이며, 지속가능한 미래를 위한 여정을 공유할 계획이다. '),\n",
       " (3,\n",
       "  '한 부회장은 이 날 삼성전자 뉴스룸에 올린 CES 2023 초연결 시대를 위한 혁신 제목의 기 고문을 통해 삼성전자는 이 번 CES 2023에서 캄테크 Calm Technology 철학을 바탕으로 한층 강화된 보안과 사물의 초연결 생태계에서 누리는 새롭고 확장된 스마트 싱스 SmartThin gs 경험을 선보인 다 고 밝혔다. '),\n",
       " (2,\n",
       "  '라스 베이거스 연합뉴스 한종희 삼성전자 디바이스 경험 DX 부문장 부회장 은 다음 달 초 미국 라스베이거스에서 열리는 세계 최대 가전 IT 전시회인 CES 2023 에서 맞춤형 경험으로 여는 초 연결 시대 Bringing Calm to Our Con nected World 를 제안할 예정이라고 15일 밝혔다. '),\n",
       " (5, '올해 초 열린 CES 2022 에서는 미래를 위한 동행 을 주제로 기조연설을 했다. '),\n",
       " (6,\n",
       "  '한 부회장은 기 고문에서 팬데믹과 글로벌 경제의 불확실성이 더해져 지금까지 겪어보지 못했던  새로운 패러다임을 맞이하고 있다 라면서 나와 내 가족이 살아가는 환경 과 경험 의 중요성이 한 층 더 커졌고, 미래 세대가 살아갈 지속가능한 미래에 대한 고민도 깊어졌다 고 현 상황을 진단했다. '),\n",
       " (0,\n",
       "  '1월 라스베이거스 서 CES2023 개막 한종희 부회장, 미래를 위한 동행 기조연설 한종희 삼성전자 부회장이 지난 1월 4일 미국 라스베이거스 베네시안 팔라조에서 미래를 위한 동행 Together for tom orrow 을 주제로 CES 2022 기 조 연설을 하고 있다. '),\n",
       " (7,\n",
       "  '그는 이어 삼성전자는 소비자 가전과 정보통신기술 ICT 분야의 글로벌 리더로서 혁신기술과 제품을 통해 밸류체인 가치사슬 전반에 걸쳐 친환경 생태계를 구축 하고, 소비자 개개인에게 더 가치 있고 풍부한 경험을 제공해야 할 책임이 있다 고 덧붙였다. '),\n",
       " (1,\n",
       "  '삼성전자 제공 2년 만에 돌아온 세계 최대 가전 전시회 지난 5일 현지시간   미국 네바다 주 라스베이거스 컨벤션센터에서 CES 2022 관람객과 취재진이 전시장 입장을 기다리고 있다. ')]"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context2 = eval(df.context[76])\n",
    "context2 = context2[:25]\n",
    "context = []\n",
    "for s in context2:\n",
    "    if(len(s) > 300):\n",
    "        s = s.split('.')[0]+' '\n",
    "    if(len(s) > 300):\n",
    "        s = s[:300]\n",
    "    context.append(s)\n",
    "get_top_sentences(' '.join(context), model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "618acbca-2637-4f13-80c5-5cdf6d53de49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:44<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.6000 sec\n",
      "3 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0035 sec\n",
      "4 318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 318/318 [02:09<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129.7272 sec\n",
      "5 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [01:39<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.9719 sec\n",
      "6 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:05<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0490 sec\n",
      "7 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:21<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.3915 sec\n",
      "8 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 122/122 [00:51<00:00,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.1139 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "import pandas as pd\n",
    "\n",
    "i = 0\n",
    "mypath = f'./{all_paths[i]}'\n",
    "onlyfiles = [f for f in listdir(mypath+'/raw') if isfile(join(mypath+'/raw', f)) and f.split('.')[1] == 'csv']\n",
    "\n",
    "############### 추출요약 ################\n",
    "for i,fname in enumerate(onlyfiles):\n",
    "    if i > 1:\n",
    "        file = fname.split('_crwal')[0]\n",
    "        filepath = f'./preprocessed/{mypath}/{file}_pre.csv'\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(i, len(df))\n",
    "        new_df = add_topk_to_df(df, model, tokenizer)\n",
    "        if not os.path.exists(f'./preprocessed2/{mypath}/'):\n",
    "            os.makedirs(f'./preprocessed2/{mypath}/')\n",
    "        new_df.to_csv(f'./preprocessed2/{mypath}/{file}_pre2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53bcf1d7-bb0e-40a0-95c3-b08b978c370b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./bertopic(paraphrase)\n",
      "['bertopic_윤석열_20221201_20221231_crwal_news_context_raw.csv', 'bertopic_부동산_20221201_20221203_crwal_news_context_raw.csv', 'bertopic_삼성전자_20221201_20221215_crwal_news_context_raw.csv', 'bertopic_윤석열_20221201_20221203_crwal_news_context_raw.csv', 'bertopic_부동산_20221201_20221231_crwal_news_context_raw.csv', 'bertopic_삼성전자_20221201_20221231_crwal_news_context_raw.csv', 'bertopic_삼성전자_20221201_20221203_crwal_news_context_raw.csv', 'bertopic_윤석열_20221201_20221215_crwal_news_context_raw.csv', 'bertopic_부동산_20221201_20221215_crwal_news_context_raw.csv']\n",
      "0 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/111 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'text_filter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-cb52362ee9f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mnew_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_context_to_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'./preprocessed/{mypath}/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'./preprocessed/{mypath}/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-4dca5d75e9e1>\u001b[0m in \u001b[0;36madd_context_to_df\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcont\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'raw'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0mcontexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mcontexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'delete this'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_filter' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "\n",
    "for i in range(len(all_paths)):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    mypath = f'./{all_paths[i]}'\n",
    "    print(mypath)\n",
    "    onlyfiles = [f for f in listdir(mypath+'/raw') if isfile(join(mypath+'/raw', f)) and f.split('.')[1] == 'csv']\n",
    "    print(onlyfiles)\n",
    "    ############### raw 전처리 ################\n",
    "    for i,fname in enumerate(onlyfiles):\n",
    "        file = fname.split('_crwal')[0]\n",
    "        filepath = f'{mypath}/raw/{onlyfiles[i]}'\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(i, len(df))\n",
    "        new_df = add_context_to_df(df)\n",
    "        if not os.path.exists(f'./preprocessed/{mypath}/'):\n",
    "            os.makedirs(f'./preprocessed/{mypath}/')\n",
    "        new_df.to_csv(f'./preprocessed/{mypath}/{file}_pre.csv')\n",
    "\n",
    "    ############### 추출요약 ################\n",
    "    for i,fname in enumerate(onlyfiles):\n",
    "        file = fname.split('_crwal')[0]\n",
    "        filepath = f'./preprocessed/{mypath}/{file}_pre.csv'\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(i, len(df))\n",
    "        new_df = add_topk_to_df(df, model, tokenizer)\n",
    "        if not os.path.exists(f'./preprocessed2/{mypath}/'):\n",
    "            os.makedirs(f'./preprocessed2/{mypath}/')\n",
    "        new_df.to_csv(f'./preprocessed2/{mypath}/{file}_pre2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfba0711-4823-41a5-b4bc-4876aee48644",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,file in enumerate(onlyfiles):\n",
    "    filepath = f'./preprocessed/{mypath}/{file}_pre.csv'\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(i, len(df))\n",
    "    new_df = add_topk_to_df(df, model, tokenizer)\n",
    "    if not os.path.exists(f'./preprocessed2/{mypath}/'):\n",
    "        os.makedirs(f'./preprocessed2/{mypath}/')\n",
    "    new_df.to_csv(f'./preprocessed2/{mypath}/{file}_pre2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f08e33-2c23-41c3-a086-a09e791ab07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,file in enumerate(onlyfiles):\n",
    "    filepath = f'{mypath}/raw/{onlyfiles[i]}'\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(i, len(df))\n",
    "    new_df = add_context_to_df(df)\n",
    "    if not os.path.exists(f'./preprocessed/{mypath}/'):\n",
    "        os.makedirs(f'./preprocessed/{mypath}/')\n",
    "    new_df.to_csv(f'./preprocessed/{mypath}/{file}_pre.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "f4fc1fb2-099b-422a-80dd-2fff171e37cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['어쩌구저쩌구 (2%~3&)제목 오늘부터 마스크 해제 책 이름 <안녕하세요> 저는 신혜진입니다. ']"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"신혜진 기자 어쩌구저쩌구 (2%~3&) 뉴스 제목 [차민수 기자] <tag>오늘</tag>부터 <p>마스크 해제</p> 책 이름 <안녕하세요> 저는 신혜진입니다 [연합뉴스] 제공\"\n",
    "#text = \"여야는 앞서 종부세 중과세율(1.2~6.0%)을 적용하는 조정대상지역 2주택 이상 소유자에 대해 중과를 폐지하고 일반세율(0.5~2.7%)을 적용하는 방안 등을 담은 잠정 합의안을 도출한 상태다. 종부세법 개정안은 예산안이 처리되는대로 국회 본회의에 오를 것으로 보인다. \"\n",
    "#text = '''서대구 역세권 사업 ‘공공주도로 개발’ 뉴스9(대구) 입력 2022.12.01 (21:47) 수정 2022.12.01 (22:05)\\n\\n댓글\\n\\n좋아요\\n\\n공유하기 글씨 크게보기\\n\\n가\\n\\n글씨 작게보기\\n\\n고화질 표준화질 자동재생 키보드 컨트롤 안내 동영상영역 시작 동영상 시작 동영상영역 끝 동영상 고정 취소 이전기사 이전기사 다음기사 다음기사\\n\\n고화질 표준화질 자동재생 키보드 컨트롤 안내 동영상영역 시작 동영상 시작 동영상영역 끝 동영상설명 동영상 고정 취소\\n\\n[앵커]\\n\\n\\n\\n대구시가 서대구 역세권 개발 방식을 민·관 공동추진에서 공공주도로 바꾸기로 했습니다.'''\n",
    "\n",
    "text_filter(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "3630e556-e8f6-4258-9577-c58bb68a2b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[차민수 기자]', '[연합뉴스]']"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pattern_others2 = re.compile(r'\\<[^가-힣][^>]*?\\>')\n",
    "#pattern_others2 = re.compile(r'\\[.[^]]*?\\]')\n",
    "pattern_others = re.compile(r'\\[[^\\]]*[기자|사진|자료|자료사진|출처|특파원|교수|작가|대표|논설|고문|주필|부문장|팀장|장관|원장|연구원|이사장|위원|실장|차장|부장|에세이|화백|사설|소장|단장|과장|기획자|큐레이터|저작권|평론가|©|©|ⓒ|\\@|\\/|=|▶|무단|전재|재배포|금지][^\\]]*?\\]')\n",
    "\n",
    "re.findall(pattern_others, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "33df40e3-6b1f-420f-9c58-93e5120fc812",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "\n",
    "path = 'hdbscan'\n",
    "onlyfiles = [f for f in listdir(path+'/raw') if isfile(join(path+'/raw', f)) and f.split('.')[1] == 'csv']\n",
    "\n",
    "\n",
    "filepath = f'{path}/raw/{onlyfiles[i]}'\n",
    "df = pd.read_csv(filepath)\n",
    "raw = df.raw[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "20351d6f-8048-4949-81da-7e6e31af5962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bertopic_윤석열_20221201_20221231_crwal_news_context'"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onlyfiles[0].split('_raw')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "e54d88d1-7200-494d-972a-89b8aa992c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'외지인 아파트 매입 두달새 5.8%P·1년새 17%P 하락\\n\\n서울거주자 춘천아파트 매입 한 달간 7건 사상 최저\\n\\n지방 부동산 활성화 위한 취득세 중과 해제 시급해져\\n\\n◇1일부터 시행되는 LTV·주택담보대출 완화 내용<제공=연합뉴스>\\n\\n수도권을 포함한 전국적인 부동산 규제지역 해제 이후 강원도내 아파트에 대한 외지인 투자가 급감한 것으로 나타났다.\\n\\n한국부동산원에 따르면 10월 기준 도내 아파트 매매거래 884건 중 외지인이 아파트를 매입한 거래는 245건, 27.7%로 조사됐다. 규제지역 해제 직전인 지난 8월에는 이 비율이 33.5%였다. 외지인 투자가 활발했던 지난해 10월 도내 아파트 매매거래 2,742건 중 외지인 매입 비율이 45.4%(1,246건)였던 점을 감안하면 1년 새 17.7%포인트나 하락한 셈이다.\\n\\n외지인들이 지난 8월까지 비규제지역인 강원도의 아파트를 활발하게 매입했지만 정부의 수도권 규제지역 해제가 본격화된 9월부터 그 수치가 뚝 떨어진 것.\\n\\n지역별로는 수도권과 가까운 춘천과 원주에서 외지인 매입 감소 폭이 컸다. 춘천지역 아파트를 서울 거주자가 산 경우는 지난해 10월 93건에서 올해 10월 7건으로 92.5% 급감했다. 춘천에서 월 단위로 서울 거주자 매입이 10건 이하를 기록한 것은 통계 작성이 시작된 2006년 1월 이후 사상 처음이다. 원주의 경우 같은 기간 서울 거주자 매입은 132건에서 10건으로 92.4%나 줄었다. 지난해 외지인 매입 비율이 전국 1위를 기록할 정도로 높았던 만큼 하락도 가팔랐다.\\n\\n토지 거래에서도 외지인 이탈이 두드러졌다. 지난 10월 강원지역 순수토지 거래(필지 수 기준) 중 외지인은 2,030건을 매입해 전년동월(3,532건) 대비 42.5% 감소했다. 같은 기간 도내 거주자가 매입한 건수는 15.4% 감소에 그쳤다.\\n\\n규제지역은 지난 9월부터 차례로 해제되는 추세로 최근에는 서울, 경기 일부를 제외한 모든 지역을 규제대상에서 해제했다. 더욱이 1일부터 15억원 초과 아파트에도 주택담보대출이 허용되는 등 추가 완화 방안이 본격화되면서 도내 외지인 매입 규모는 더욱 줄어들 전망이다.\\n\\n이처럼 외지인들의 도내 아파트와 토지 매입이 줄어들 것으로 예상되자 지방 부동산의 취득세 중과 해제 등을 요구하는 목소리가 높아지고 있다.\\n\\n강문식 한국공인중개사협회 춘천시지회장은 “수도권 규제지역 해제로 외지 투자자들이 지방으로 내려올 이유가 사라졌다”면서 “지역 부동산 활성화를 위해선 취득세 중과 해제가 시급한 만큼 지자체가 나서서 정부를 설득할 필요가 있다”고 말했다.'"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "5b571695-3415-4645-9f0a-d9395eaf78e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['수도권을 포함한 전국적인 부동산 규제지역 해제 이후 강원도내 아파트에 대한 외지인 투자가 급감한 것으로 나타났다. ',\n",
       " '한국부동산원에 따르면 10월 기준 도내 아파트 매매거래 884건 중 외지인이 아파트를 매입한 거래는 245건, 27.7%로 조사됐다. ',\n",
       " '규제지역 해제 직전인 지난 8월에는 이 비율이 33.5%였다. ',\n",
       " '외지인 투자가 활발했던 지난해 10월 도내 아파트 매매거래 2,742건 중 외지인 매입 비율이 45.4%(1,246건)였던 점을 감안하면 1년 새 17.7%포인트나 하락한 셈이다. ',\n",
       " '지역별로는 수도권과 가까운 춘천과 원주에서 외지인 매입 감소 폭이 컸다. ',\n",
       " '춘천지역 아파트를 서울 거주자가 산 경우는 지난해 10월 93건에서 올해 10월 7건으로 92.5% 급감했다. ',\n",
       " '춘천에서 월 단위로 서울 거주자 매입이 10건 이하를 기록한 것은 통계 작성이 시작된 2006년 1월 이후 사상 처음이다. ',\n",
       " '원주의 경우 같은 기간 서울 거주자 매입은 132건에서 10건으로 92.4%나 줄었다. ',\n",
       " '지난해 외지인 매입 비율이 전국 1위를 기록할 정도로 높았던 만큼 하락도 가팔랐다. ',\n",
       " '토지 거래에서도 외지인 이탈이 두드러졌다. ',\n",
       " '지난 10월 강원지역 순수토지 거래(필지 수 기준) 중 외지인은 2,030건을 매입해 전년동월(3,532건) 대비 42.5% 감소했다. ',\n",
       " '같은 기간 도내 거주자가 매입한 건수는 15.4% 감소에 그쳤다. ',\n",
       " '규제지역은 지난 9월부터 차례로 해제되는 추세로 최근에는 서울, 경기 일부를 제외한 모든 지역을 규제대상에서 해제했다. ',\n",
       " '더욱이 1일부터 15억원 초과 아파트에도 주택담보대출이 허용되는 등 추가 완화 방안이 본격화되면서 도내 외지인 매입 규모는 더욱 줄어들 전망이다. ',\n",
       " '이처럼 외지인들의 도내 아파트와 토지 매입이 줄어들 것으로 예상되자 지방 부동산의 취득세 중과 해제 등을 요구하는 목소리가 높아지고 있다. ',\n",
       " '강문식 한국공인중개사협회 춘천시지회장은 “수도권 규제지역 해제로 외지 투자자들이 지방으로 내려올 이유가 사라졌다”면서 “지역 부동산 활성화를 위해선 취득세 중과 해제가 시급한 만큼 지자체가 나서서 정부를 설득할 필요가 있다”고 말했다. ']"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_filter(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e77592f7-566a-41c5-82e9-4ba6a1b5f910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['페이지\\n\\n윤 대통령, 제1차 국정과제 점검회의 다주택자 세 중과 해제 검토 주택공급, 공공·민간 믹스…이달말 공공주택 50만가구 사전청약\\n\\n윤석열 대통령은 15일 국정과제 점검회의에서 규제 완화와 다주택자 세 중과 해제 검토,주택공급, 공공·민간 믹스 등 3대개혁 청사진을 제시했다.',\n",
       " '윤 대통령은 이날 오후 2시부터 청와대 영빈관에서 열린 제1차 국정과제 점검회의에서 국민 패널 2명으로부터 ‘내 집 마련’과 ‘부동산 경기 활성화’ 방안과 관련한 질문을 받고 부동산 시장 연착륙을 위한 규제 완화 필요성을 강조했다.',\n",
       " '전 정부에서 강화된 부동산 규제를 일제히 풀 경우 부작용을 우려해 그간 속도 조절을 해왔지만, 고금리 등으로 상황이 급변한 시장 대응을 위해 다시 규제 완화 고삐를 죄겠다고 했다.이같은 발언은 규제완화를 통한 부동산 시장을 활성화시키겠다는 것으로 풀이된다.',\n",
       " '불합리한 복합 규제 때문…규제 완화 ‘속도’\\n\\n윤 대통령은 “제가 정부를 맡기 전까지 공급 측면과 수요 측면의 불합리한 복합 규제 때문에 집값이 너무 천정부지로 솟고 거래물량이 위축됐다”며 “시장 정상화가 중요하다고 해서 많은 규제를 풀고 정상화하려고 했는데, 고금리 상황 때문에 다시 부동산 가격이 하락하는 추세를 보였다”고 진단했다.',\n",
       " '그러면서 “(전 정부의) 잘못된 정책으로 인한 현상이라고 해도, 그것을 일시에 제거하다 보면 시장에 혼란이 일어나서 결국 국민에게 불편을 줄 수 있어서 시장 정상화의 속도를 조율해야 한다고 생각했다”며 수요 규제를 조금 더 빠른 속도로 풀어나가서 시장이 안정 찾는데 최선의 노력할 것이라고 강조했다.',\n",
       " '이 같은 발언은 최근 고금리와 집값 하락 우려로 거래가 메마르면서 곳곳에서 ‘경고음’이 터지는 데 따른 것으로 풀이된다.',\n",
       " '내놓은 매물이 나가지 않아 이사를 못가는 상황이 발생하거나, ‘영끌’ 매입자들의 금리 부담 급증, 집값 하락세에 따른 보유세 부담 호소, 빠른 월세화 현상 등이다.',\n",
       " '다주택자 세 중과 해제 검토…주택공급, 공공·민간 믹스\\n\\n종합부동산세의 경우 국회에서 관련법 개정 논의가 이어지고 있다.',\n",
       " '여야는 앞서 종부세 중과세율(~%)을 적용하는 조정대상지역 2주택 이상 소유자에 대해 중과를 폐지하고 일반세율(~%)을 적용하는 방안 등을 담은 잠정 합의안을 도출한 상태다.',\n",
       " '종부세법 개정안은 예산안이 처리되는대로 국회 본회의에 오를 것으로 보인다.',\n",
       " '서울과 경기 4곳만 남은 규제지역의 추가 해제 여부 역시 시장의 관심사다.',\n",
       " '윤 대통령은 이날 주택공급계획과 관련해 “민간과 공공임대를 잘 믹스해서(섞어서) 공급하려고 한다”고 말했다.',\n",
       " '이어 “공공임대주택 많이 지어서 공급하다 보면 중앙정부나 지방정부가 상당한 재정부담을 안게 된다”며 “납세자에게도 굉장히 큰 부담이 되고 전반적으로 우리 경제에 부담 요인이나 경기 위축 요인으로 작용할 수 있다”고 설명했다.',\n",
       " '한편 정부가 임기 내 공급을 약속한 270만가구 중 50만가구에 해당하는 공공주택 사전청약이 이달 말부터 실시된다.은 이날 회의에서 “무주택 서민과 젊은 세대에게 희망을 갖고 내 집 마련을 포기하지 말라고 (지속적으로) 공급할 것”이라며 “이번달 말부터 (공공주택) 사전청약을 받으니 관심이 있는 분들은 이용해주시라”고 당부했다.',\n",
       " '경기일보(),및 수집,']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_filter(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b0500f0-020c-4c16-89d6-5578dc712a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['강해인 기자  기자페이지 윤 대통령, 제1차 국정과제 점검회의 다주택자 세 중과 해제 검토 주택공급, 공공 민간 믹스 이달말 공공주택 50만가구 사전청약 윤석열 대통령은 15일 국정과제 점검회의에서 규제 완화와 다주택자 세 중과 해제 검토,주택공급, 공공 민간 믹스 등 3대개혁 청사진을 제시했다.',\n",
       " '윤 대통령은 이날 오후 2시부터 청와대 영빈관에서 열린 제1차 국정과제 점검회의에서 국민 패널 2명으로부터  내 집 마련 과  부동산 경기 활성화  방안과 관련한 질문을 받고 부동산 시장 연착륙을 위한 규제 완화 필요성을 강조했다.',\n",
       " '전 정부에서 강화된 부동산 규제를 일제히 풀 경우 부작용을 우려해 그간 속도 조절을 해왔지만, 고금리 등으로 상황이 급변한 시장 대응을 위해 다시 규제 완화 고삐를 죄겠다고 했다.이같은 발언은 규제완화를 통한 부동산 시장을 활성화시키겠다는 것으로 풀이된다.',\n",
       " '불합리한 복합 규제 때문 규제 완화  속도 윤 대통령은  제가 정부를 맡기 전까지 공급 측면과 수요 측면의 불합리한 복합 규제 때문에 집값이 너무 천정부지로 솟고 거래물량이 위축됐다 며  시장 정상화가 중요하다고 해서 많은 규제를 풀고 정상화하려고 했는데, 고금리 상황 때문에 다시 부동산 가격이 하락하는 추세를 보였다 고 진단했다.',\n",
       " '그러면서  전 정부의  잘못된 정책으로 인한 현상이라고 해도, 그것을 일시에 제거하다 보면 시장에 혼란이 일어나서 결국 국민에게 불편을 줄 수 있어서 시장 정상화의 속도를 조율해야 한다고 생각했다 며 수요 규제를 조금 더 빠른 속도로 풀어나가서 시장이 안정 찾는데 최선의 노력할 것이라고 강조했다.',\n",
       " '이 같은 발언은 최근 고금리와 집값 하락 우려로 거래가 메마르면서 곳곳에서  경고음 이 터지는 데 따른 것으로 풀이된다.',\n",
       " '내놓은 매물이 나가지 않아 이사를 못가는 상황이 발생하거나,  영끌  매입자들의 금리 부담 급증, 집값 하락세에 따른 보유세 부담 호소, 빠른 월세화 현상 등이다.',\n",
       " '다주택자 세 중과 해제 검토 주택공급, 공공 민간 믹스 종합부동산세의 경우 국회에서 관련법 개정 논의가 이어지고 있다.',\n",
       " '여야는 앞서 종부세 중과세율 % 을 적용하는 조정대상지역 2주택 이상 소유자에 대해 중과를 폐지하고 일반세율 % 을 적용하는 방안 등을 담은 잠정 합의안을 도출한 상태다.',\n",
       " '종부세법 개정안은 예산안이 처리되는대로 국회 본회의에 오를 것으로 보인다.',\n",
       " '서울과 경기 4곳만 남은 규제지역의 추가 해제 여부 역시 시장의 관심사다.',\n",
       " '윤 대통령은 이날 주택공급계획과 관련해  민간과 공공임대를 잘 믹스해서 섞어서  공급하려고 한다 고 말했다.',\n",
       " '이어  공공임대주택 많이 지어서 공급하다 보면 중앙정부나 지방정부가 상당한 재정부담을 안게 된다 며  납세자에게도 굉장히 큰 부담이 되고 전반적으로 우리 경제에 부담 요인이나 경기 위축 요인으로 작용할 수 있다 고 설명했다.',\n",
       " '한편 정부가 임기 내 공급을 약속한 270만가구 중 50만가구에 해당하는 공공주택 사전청약이 이달 말부터 실시된다.',\n",
       " '원희룡 국토부 장관은 이날 회의에서  무주택 서민과 젊은 세대에게 희망을 갖고 내 집 마련을 포기하지 말라고  지속적으로  공급할 것 이라며  이번달 말부터  공공주택  사전청약을 받으니 관심이 있는 분들은 이용해주시라 고 당부했다.',\n",
       " '강해인기자  경기일보 , 무단전재 및 수집, 재배포금지']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_filter(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bf6279-fea5-4c80-8a69-e8e18aee4264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
